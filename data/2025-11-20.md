<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 22]
- [cs.LG](#cs.LG) [Total: 33]
- [cs.AI](#cs.AI) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective](https://arxiv.org/abs/2511.14772)
*Zhuoyi Yang,Xu Guo,Tong Zhang,Huijuan Xu,Boyang Li*

Main category: cs.CL

TL;DR: 本文综述了通过增加推理时计算来提升预训练大语言模型预测准确性的技术，特别关注问题分解方式和子问题组织结构，统一了多种方法并分析其优缺点。


<details>
  <summary>Details</summary>
Motivation: 提高预训练大语言模型在推理时的预测准确性，通过分配额外计算资源来优化模型性能。

Method: 对测试时扩展方法进行分类，重点分析问题分解为子问题的方式以及子问题的拓扑组织结构（顺序、并行或树状结构），将Chain-of-Thought、Branch-Solve-Merge和Tree-of-Thought等方法统一在同一框架下。

Result: 建立了统一的分类框架，能够系统性地分析和比较不同测试时扩展方法的特性。

Conclusion: 综合分析了现有技术的优缺点，并指出了未来研究的潜在方向。

Abstract: With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research

</details>


### [2] [Temporal Predictors of Outcome in Reasoning Language Models](https://arxiv.org/abs/2511.14773)
*Joey David*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型在推理过程中很早就内部确定了最终答案，即使需要更长的输出来得出明确结论。线性分类器分析显示，仅通过前几个推理token就能高度预测最终正确性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在思维链推理过程中何时内部确定最终结果，理解模型内部自我评估机制的出现时机。

Method: 通过在推理过程的前t个token后的隐藏状态上训练线性分类器，分析模型内部表示对最终正确性的预测能力。

Result: 发现最终正确性在仅几个推理token后就能高度预测，对于更难的问题，预测准确率下降揭示了选择偏差：困难问题在长思维链中占比过高。

Conclusion: 推理模型的内部自我评估能力在推理早期就出现，这对可解释性和推理时控制具有重要意义。

Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.

</details>


### [3] [LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs](https://arxiv.org/abs/2511.14774)
*Pei-Fu Guo,Yun-Da Tsai,Chun-Chia Hsu,Kai-Xin Chen,Ya-An Tsai,Kai-Wei Chang,Nanyun Peng,Mi-Yen Yeh,Shou-De Lin*

Main category: cs.CL

TL;DR: LiveCLKTBench是一个自动生成管道，专门用于隔离和测量大型语言模型的跨语言知识迁移，通过时间敏感的知识实体来评估真实的跨语言迁移能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的跨语言知识迁移具有挑战性，因为目标语言中的正确答案可能来自真实的迁移，也可能来自预训练期间的先前接触。需要一种方法来隔离和测量真正的跨语言知识迁移。

Method: 开发了LiveCLKTBench自动生成管道，从现实领域识别自包含、时间敏感的知识实体，基于时间发生进行过滤，并验证模型的知识。使用这些有效实体的文档生成事实性问题，翻译成多种语言来评估跨语言迁移。

Result: 在五种语言上评估多个LLM，发现跨语言迁移受语言距离强烈影响，且在不同语言方向上通常不对称。虽然更大的模型能改善迁移，但收益随规模增加而减少，且在不同领域间变化。

Conclusion: 这些发现为多语言迁移提供了新的见解，并证明了LiveCLKTBench作为未来研究的可靠基准的价值。

Abstract: Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.

</details>


### [4] [COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation](https://arxiv.org/abs/2511.14776)
*Snigdha Pandya,Rohan Nagale,Kenji Sahay,Anna Lin,Shikhar Shiromani,Kevin Zhu,Dev Sunishchal*

Main category: cs.CL

TL;DR: COMPASS是一个轻量级、可解释的控制框架，通过模型反馈循环在解码过程中动态调节注意力头，以减少LLM的事实性错误，同时提供对模型行为的科学理解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常生成流畅但事实错误的陈述，这是由于它们在上下文知识和参数知识之间分配注意力的方式存在问题。理解和引导这种内部行为对于可信部署和模型机制的科学可解释性至关重要。

Method: 引入COMPASS框架，通过上下文依赖分数(CRS)量化上下文依赖，作为在线探针来监测注意力头如何将生成过程基于证据。使用PID控制器动态调节注意力头，无需重新训练或多轮解码。

Result: 在多个基准测试(HotpotQA、XSum、HaluEval、RAGTruth)中，COMPASS持续减少了上下文幻觉率(绝对减少2.8%到5.8%)，同时揭示了不同注意力头对证据对齐的贡献。

Conclusion: 反馈驱动的可解释性为科学理解LLM行为提供了一条途径，COMPASS框架能够在不重新训练的情况下有效减少事实性错误，同时提供对模型内部机制的洞察。

Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.

</details>


### [5] [The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech](https://arxiv.org/abs/2511.14779)
*Julio Cesar Galdino,Sidney Evaldo Leal,Leticia Gabriella De Souza,Rodrigo de Freitas Lima,Antonio Nelson Fornari Mendes Moreira,Arnaldo Candido Junior,Miguel Oliveira,Edresson Casanova,Sandra M. Aluísio*

Main category: cs.CL

TL;DR: 本文评估了手动和自动韵律分割标注对巴西葡萄牙语语音合成质量的影响，发现韵律分割训练能产生更清晰和自然的语音，手动分割引入的变异性有助于更自然的韵律。


<details>
  <summary>Details</summary>
Motivation: 自发语音合成面临捕捉对话自然流程的挑战，包括话轮转换、停顿和不流畅性。现有系统主要通过隐式建模韵律特征取得进展，但明确韵律分割数据集及其对自发语音合成的影响尚未充分探索。

Method: 使用非自回归模型FastSpeech 2，在巴西葡萄牙语数据集上评估手动和自动韵律分割标注的效果，分析其对语音合成质量的影响。

Result: 实验结果显示，使用韵律分割训练产生的语音在可懂度和声学自然度方面略有提升。自动分割产生更规则的片段，而手动分割引入更大变异性，有助于更自然的韵律。两种方法都能重现预期的核心重音模式，但韵律模型与自然前核心轮廓更接近。

Conclusion: 韵律分割标注对提升语音合成质量有积极影响，手动分割的变异性有助于更自然的韵律表现。所有数据集、源代码和训练模型已公开，支持可重复性和未来研究。

Abstract: Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.

</details>


### [6] [Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings](https://arxiv.org/abs/2511.14868)
*Xueying Ding,Xingyue Huang,Mingxuan Ju,Liam Collins,Yozen Liu,Leman Akoglu,Neil Shah,Tong Zhao*

Main category: cs.CL

TL;DR: HTP通过分层令牌前置和均值池化解决大语言模型在长文档嵌入中的信息压缩问题，显著提升检索和嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的因果注意力机制限制了从后向前的信息流动，导致长文档表示质量下降。现有方法通过前置单个摘要令牌会过度压缩信息，在长文档上表现不佳。

Method: 提出分层令牌前置(HTP)：将输入分块并为后续块前置块级摘要令牌，创建多条后向信息流路径；用均值池化替代最后令牌池化来解决读取级过度压缩问题。

Result: 在11个检索数据集和30个通用嵌入基准测试中取得一致性能提升，特别是在长上下文设置中表现突出。

Conclusion: HTP作为一种简单且架构无关的方法，能够提升零样本和微调模型的性能，为优质长文档嵌入提供了可扩展的解决方案。

Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.

</details>


### [7] [Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation](https://arxiv.org/abs/2511.15005)
*Moses Kiprono*

Main category: cs.CL

TL;DR: 本文提出了一个数学基础框架来理解、测量和减轻大语言模型的幻觉问题，结合概率建模、信息论、三角信号分析和贝叶斯不确定性估计，提出了改进的不确定性度量和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然功能强大，但仍容易产生看似合理但事实错误或无依据的幻觉输出，这影响了模型的可靠性和安全性。

Method: 使用概率建模、信息论、三角信号分析和贝叶斯不确定性估计来分析自回归误差传播，提出语义和相位感知的不确定性度量，开发对比解码、检索增强接地、事实对齐和弃权等缓解策略。

Result: 建立了一个统一的理论框架，将校准、检索和对齐等最新进展联系起来，支持更安全可靠的大语言模型。

Conclusion: 该数学框架为理解和解决大语言模型幻觉问题提供了系统的方法，有助于开发更可靠的语言模型系统。

Abstract: Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.

</details>


### [8] [Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs](https://arxiv.org/abs/2511.15163)
*Yang Wu,Rujing Yao,Tong Zhang,Yufei Shi,Zhuoren Jiang,Zhushan Li,Xiaozhong Liu*

Main category: cs.CL

TL;DR: TASA是一个学生感知的数学辅导框架，通过整合学生画像、记忆和遗忘动态来实现个性化学习，在LLM辅导系统中表现出优越的学习效果和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅导系统未能捕捉学生知识随熟练度、概念差距和遗忘模式的动态演变，特别是在需要精细校准的数学辅导中。

Method: TASA维护结构化学生画像记录熟练度档案和事件记忆，结合连续遗忘曲线和知识追踪，动态更新学生掌握状态并生成情境适当、难度校准的问题和解释。

Result: 实证结果表明，TASA相比代表性基线方法实现了更优越的学习成果和更自适应的辅导行为。

Conclusion: 在基于LLM的辅导系统中建模时间遗忘和学习者画像具有重要意义。

Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.

</details>


### [9] [HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples](https://arxiv.org/abs/2511.15183)
*Rishikant Chigrupaatii,Ponnada Sai Tulasi Kanishka,Lalit Chandra Routhu,Martin Patel Sama Supratheek Reddy,Divyam Gupta,Dasari Srikar,Krishna Teja Kuchimanchi,Rajiv Misra,Rohun Tripathi*

Main category: cs.CL

TL;DR: 提出了一个评估多语言视觉语言模型在印度语言中性能的可扩展框架，并创建了HinTel-AlignBench基准测试，发现模型在印度语言中的表现相比英语平均下降8.3分（印地语）和5.5分（泰卢固语）。


<details>
  <summary>Details</summary>
Motivation: 解决当前多语言VLM评估的四个主要局限：依赖未经验证的自动翻译、任务/领域覆盖范围窄、样本量有限、缺乏文化和本地来源的问答数据。

Method: 开发了一个半自动化的数据集创建框架，结合回译、过滤和人工验证；创建了最全面的印地语和泰卢固语视觉语言基准，包括适应英语数据集和本地新颖的印度数据集。

Result: 在5个任务中的4个任务中，所有模型在印度语言中的表现相比英语都有下降，印地语平均下降8.3分，泰卢固语平均下降5.5分。

Conclusion: 揭示了多语言多模态理解的具体改进领域，并强调了为低资源语言开发公平AI的重要性。

Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.

</details>


### [10] [Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story](https://arxiv.org/abs/2511.15210)
*Vladislav Pedashenko,Laida Kushnareva,Yana Khassan Nibal,Eduard Tulchinskii,Kristian Kuznetsov,Vladislav Zharchinskii,Yury Maximov,Irina Piontkovskaya*

Main category: cs.CL

TL;DR: 本文首次全面研究了文本内在维度(ID)与可解释文本属性之间的关系，发现ID与基于熵的指标互补，呈现明显的体裁分层，并通过稀疏自编码器识别了影响ID的因果特征。


<details>
  <summary>Details</summary>
Motivation: 内在维度是现代LLM分析中的重要工具，但其文本决定因素尚未得到充分探索。本文旨在通过可解释的文本属性来理解ID的成因。

Method: 采用交叉编码器分析、语言特征分析和稀疏自编码器(SAEs)方法，结合转向实验验证因果效应。

Result: 发现ID与熵指标不相关，呈现明显的体裁分层：科学文本ID最低(~8)，百科全书内容中等(~9)，创意/观点写作最高(~10.5)。通过SAEs识别出科学信号降低ID，人性化信号增加ID。

Conclusion: 对于当代模型，科学写作相对"简单"，而小说、观点和情感表达增加了表示自由度。研究为ID的正确使用和基于ID结果的合理解释提供了实践指导。

Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.

</details>


### [11] [OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition](https://arxiv.org/abs/2511.15211)
*Xinli Tao,Xin Dong,Xuezhong Zhou*

Main category: cs.CL

TL;DR: OEMA是一个零样本临床命名实体识别框架，通过多智能体协作解决传统方法对标注数据依赖和提示工程问题，在MTSamples和VAERS数据集上达到最先进的精确匹配性能。


<details>
  <summary>Details</summary>
Motivation: 解决监督式临床NER模型对昂贵标注数据的依赖，以及零样本NER在示例选择粒度和提示集成方面的挑战。

Method: 提出OEMA框架，包含三个组件：自标注器生成示例、鉴别器通过SNOMED CT过滤示例、预测器使用实体描述进行准确推理。

Result: 在MTSamples和VAERS数据集上实现最先进的精确匹配性能；在相关匹配下，与监督式BioClinicalBERT相当并超越CRF。

Conclusion: OEMA通过本体引导推理和多智能体协作解决了零样本NER的关键挑战，实现了接近监督学习的性能，在临床NLP应用中具有前景。

Abstract: Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.

</details>


### [12] [Context Cascade Compression: Exploring the Upper Limits of Text Compression](https://arxiv.org/abs/2511.15244)
*Fanfan Liu,Haibo Qiu*

Main category: cs.CL

TL;DR: 提出C3上下文级联压缩方法，通过大小两个LLM级联处理长文本压缩和解码任务，在20倍压缩比下达到98%解码准确率，远超DeepSeek-OCR的60%准确率。


<details>
  <summary>Details</summary>
Motivation: 百万级token的长上下文任务给大语言模型带来计算和内存挑战，需要探索文本压缩的上限。

Method: 级联两个不同大小的LLM：小LLM作为第一阶段将长文本压缩为少量潜在token（如32或64长度），大LLM作为第二阶段在压缩上下文中执行解码任务。

Result: 20倍压缩比下达到98%解码准确率（DeepSeek-OCR约60%），40倍压缩比下仍保持约93%准确率。

Conclusion: C3压缩在上下文压缩领域表现出优越性能和可行性，为光学字符压缩、OCR等领域提供了压缩比上限参考。

Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression

</details>


### [13] [IndicGEC: Powerful Models, or a Measurement Mirage?](https://arxiv.org/abs/2511.15260)
*Sowmya Vajjala*

Main category: cs.CL

TL;DR: TeamNRC参与BHASHA-Task 1语法错误纠正共享任务，使用零/少样本提示语言模型方法，在泰卢固语和印地语中分别获得第4和第2名，并将实验扩展到泰米尔语、马拉雅拉姆语和孟加拉语。


<details>
  <summary>Details</summary>
Motivation: 探索小语言模型在印度语言语法错误纠正任务中的潜力，并关注数据集质量和评估指标的问题。

Method: 使用不同规模的语言模型（4B到大型专有模型）进行零/少样本提示，应用于5种印度语言的语法错误纠正任务。

Result: 在泰卢固语和印地语中分别获得GLEU分数83.78和84.31，排名第4和第2；将方法扩展到其他三种语言。

Conclusion: 小语言模型在印度语言语法错误纠正任务中具有潜力，但需要关注数据集质量和适合印度语言脚本的评估指标。

Abstract: In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.

</details>


### [14] [MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews](https://arxiv.org/abs/2511.15291)
*Randa Zarnoufi*

Main category: cs.CL

TL;DR: 本文使用SetFit框架对阿拉伯方言酒店评论进行情感分析，在数据稀缺情况下取得73%的F1分数，排名第12位。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言情感分析面临语言多样性和标注数据稀缺的挑战，特别是在酒店评论等专业领域。

Method: 采用SetFit（句子变换器微调）框架，这是一种数据高效的少样本学习技术。

Result: 在官方评估集上获得73%的F1分数，在26个参与者中排名第12位。

Conclusion: 少样本学习在处理阿拉伯方言文本方面具有潜力，能够有效应对专业领域中的数据稀缺问题。

Abstract: Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.

</details>


### [15] [Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models](https://arxiv.org/abs/2511.15304)
*Piercosma Bisconti,Matteo Prandi,Federico Pierucci,Francesco Giarrusso,Marcantonio Bracale,Marcello Galisai,Vincenzo Suriani,Olga Sorokoletova,Federico Sartore,Daniele Nardi*

Main category: cs.CL

TL;DR: 研究发现诗歌形式的对抗性提示可以作为一种通用的单轮越狱技术，在25个前沿专有和开源大语言模型中有效绕过安全机制，攻击成功率显著高于普通文本基线。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型安全机制的局限性，特别是风格变异是否能绕过当代安全训练方法，揭示对齐方法的基本缺陷。

Method: 将1200个MLCommons有害提示通过标准化元提示转换为诗歌形式，使用开放权重评判模型和人工验证子集进行评估，手动解决分歧。

Result: 诗歌攻击在手工制作诗歌中平均越狱成功率达62%，元提示转换约为43%，显著优于非诗歌基线，最高可达基线的18倍。

Conclusion: 仅凭风格变异就能规避当代安全机制，表明当前对齐方法和评估协议存在根本性限制，诗歌形式揭示了跨模型系列和安全训练方法的系统性漏洞。

Abstract: We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.

</details>


### [16] [HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning](https://arxiv.org/abs/2511.15355)
*Alexis Correa-Guillén,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: HEAD-QA v2是西班牙语/英语医疗保健多选题推理数据集的扩展更新版本，包含超过12,000个问题，来自十年西班牙专业考试，用于评估开源LLM在医疗推理方面的表现。


<details>
  <summary>Details</summary>
Motivation: 响应对高质量数据集的需求，以捕捉医疗保健推理的语言和概念复杂性。

Method: 扩展数据集至超过12,000个问题，使用提示、RAG和基于概率的答案选择对多个开源LLM进行基准测试，并提供多语言版本。

Result: 性能主要由模型规模和内在推理能力驱动，复杂推理策略获得的收益有限。

Conclusion: HEAD-QA v2成为推进生物医学推理和模型改进研究的可靠资源。

Abstract: We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.

</details>


### [17] [NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework](https://arxiv.org/abs/2511.15408)
*Shanlin Zhou,Xinpeng Wang,Jianxun Lian,Zhenghao Liu,Laks V. S. Lakshmanan,Xiaoyuan Yi,Yongtao Hao*

Main category: cs.CL

TL;DR: 论文提出了NAMEGEn框架，通过多智能体优化解决中文起名这一短文本创意生成任务，能够同时满足个性化需求和提供美学解释。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在创意自然语言生成中面临两个主要挑战：多目标灵活性不足（难以同时满足个性化、细粒度、多元化的用户需求）和解释复杂性（需要理解隐含意义来增强用户感知）。这些挑战限制了短文本生成中的创意性和洞察力。

Method: 提出NAMEGEn多智能体优化框架，通过目标提取、名字生成和评估的迭代交替过程来满足多样化需求并生成准确解释。构建了包含17,000+首古诗的语料库来增强美学效果，并引入了CBNames基准测试。

Result: 大量实验表明，NAMEGEn能够有效生成满足多样化、个性化需求的创意名字，并提供有意义的解释，在无需任何训练的情况下优于六种基于不同LLM骨干的基线方法。

Conclusion: NAMEGEn框架成功解决了创意短文本生成中的多目标灵活性和解释复杂性挑战，为中文起名等需要同时满足显式约束和美学解释的任务提供了有效解决方案。

Abstract: Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.

</details>


### [18] [Building Robust and Scalable Multilingual ASR for Indian Languages](https://arxiv.org/abs/2511.15418)
*Arjun Gangwar,Kaousheik Jayakumar,S. Umesh*

Main category: cs.CL

TL;DR: SPRING Lab开发了用于ASRU MADASR 2.0挑战的多语言ASR系统，采用多解码器架构和音素通用标签集作为中间表示，在Track 2中在3种语言上超越基线性能，并获得了最高的语言和方言识别准确率。


<details>
  <summary>Details</summary>
Motivation: 改进ASR系统在8种语言33种方言中的语言和方言识别能力，参与Track 1和Track 2的从零开始构建多语言系统的挑战。

Method: 使用多解码器架构，以音素通用标签集作为中间表示，并开发了多种方法将在音素空间获得的性能增益转换回对应的字形表示。

Result: 在CLS空间性能优于基线，在Track 2中3种语言的WER/CER超越基线，在所有参赛团队中获得最高的语言ID和方言ID准确率。

Conclusion: 提出的多解码器架构和音素CLS中间表示方法有效提升了多语言ASR系统的性能，特别是在语言和方言识别方面表现优异。

Abstract: This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).

</details>


### [19] [LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering](https://arxiv.org/abs/2511.15424)
*Yuanjie Zhu,Liangwei Yang,Ke Xu,Weizhi Zhang,Zihe Song,Jindong Wang,Philip S. Yu*

Main category: cs.CL

TL;DR: LLM-MemCluster是一个新颖的LLM原生聚类框架，通过动态内存和双提示策略实现无需调优的端到端文本聚类，在多个基准数据集上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在文本聚类中存在两个根本限制：缺乏状态记忆进行迭代优化，以及难以管理聚类粒度。现有方法依赖复杂的外部模块流水线，无法实现真正的端到端方法。

Method: 提出LLM-MemCluster框架，将聚类重新概念化为完全LLM原生的任务。利用动态内存实现状态感知，采用双提示策略让模型推理并确定聚类数量。

Result: 在多个基准数据集上的评估显示，这个无需调优的框架显著且持续地优于强基线方法。

Conclusion: LLM-MemCluster为基于LLM的文本聚类提供了一个有效、可解释且真正端到端的范式。

Abstract: Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.

</details>


### [20] [Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis](https://arxiv.org/abs/2511.15512)
*Yves Pauli,Jan-Bernard Marsman,Finn Rabe,Victoria Edkins,Roya Hüppi,Silvia Ciampelli,Akhil Ratan Misra,Nils Lang,Wolfram Hinzen,Iris Sommer,Philipp Homan*

Main category: cs.CL

TL;DR: 本文提出了语言处理数据结构（LPDS）和pelican nlp工具包，旨在解决语言数据处理中的标准化和可重复性问题，提供端到端的语言数据处理流程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型等AI语言处理技术的发展带来了语言数据分析方法的演进，但同时也暴露了语言数据组织和共享缺乏标准化、处理方法不可重复等挑战。

Method: 1. 提出LPDS数据结构，受神经科学BIDS标准启发，提供文件夹结构和文件命名规范；2. 开发pelican nlp模块化Python包，支持从数据清洗到复杂语言和声学特征提取的完整处理流程，可通过单一配置文件执行。

Result: LPDS和pelican nlp共同提供了端到端的语言数据处理管道，能够生成预处理后的语言数据或标准化的语言和声学特征提取结果。

Conclusion: LPDS和pelican nlp为语言数据处理提供了标准化的解决方案，确保方法透明度并增强可重复性。

Abstract: The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.

</details>


### [21] [Multimodal Evaluation of Russian-language Architectures](https://arxiv.org/abs/2511.15552)
*Artem Chervyakov,Ulyana Isaeva,Anton Emelyanov,Artem Safin,Maria Tikhonova,Alexander Kharitonov,Yulia Lyakh,Petr Surovtsev,Denis Shevelev Vildan Saburov,Vasily Konovalov,Elisei Rykov,Ivan Sviridov,Amina Miftakhova,Ilseyar Alimova,Alexander Panchenko,Alexander Kapitanov,Alena Fenogenova*

Main category: cs.CL

TL;DR: Mera Multi是一个针对俄语的多模态评估框架，包含18个新构建的评估任务，涵盖文本、图像、音频和视频模态，专注于俄语文化和语言特性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对俄语的多模态基准测试，而多模态大语言模型的智能、局限性和风险尚未得到充分理解，需要专门的评估框架。

Method: 创建基于指令的评估框架，包含18个全新构建的数据集，采用统一提示和度量标准，并包含防止基准泄漏的方法论（水印和私有集许可）。

Result: 为闭源和开源模型提供了基线结果，展示了该框架在评估俄语多模态模型方面的有效性。

Conclusion: Mera Multi不仅填补了俄语多模态基准的空白，其方法论还可复制用于构建其他斯拉夫语系语言的类似基准测试。

Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.

</details>


### [22] [HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://arxiv.org/abs/2511.15574)
*Qihao Yang,Xuelin Wang,Jiale Chen,Xuelian Dong,Yuxin Hao,Tianyong Hao*

Main category: cs.CL

TL;DR: HSKBenchmark是首个用于中文二语习得的分阶段建模和写作评估基准，涵盖HSK 3-6级，包含676万词符的真实教材、1.6万条合成指令样本、30个测试主题和基于语言学的评估系统。


<details>
  <summary>Details</summary>
Motivation: 语言习得对揭示人类语言智能本质至关重要，但控制人类学习者语言输入的实验在伦理和实践上不可行，这给语言习得建模的可验证性和可扩展性带来挑战，特别是在中文二语习得领域。

Method: 提出了课程调优框架，从初级到高级水平训练模型；构建了评估系统，检查基于水平的语法覆盖、写作错误、词汇和句法复杂性以及整体评分；构建了HSKAgent，在1万篇学习者作文上微调。

Result: 实验结果表明HSKBenchmark不仅能有效建模中文二语习得，还能作为LLMs动态写作评估的可靠基准。微调后的LLMs写作表现与高级人类学习者相当，并展现出类似人类的习得特征。

Conclusion: HSKBenchmark、HSKAgent和检查点作为基础工具和资源，有潜力为未来语言习得建模和LLMs可解释性研究铺平道路。

Abstract: Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States](https://arxiv.org/abs/2511.14808)
*Mikael von Strauss*

Main category: cs.LG

TL;DR: 该论文研究了仅解码器Transformer中离散提示到最后一词隐藏状态的映射的注入性。理论分析表明，在温和条件下，该映射在参数空间中是泛型单射的，且这种单射性在训练过程中持续存在。实证研究通过几何诊断指标验证了预训练模型在完整精度和8位量化下的单射性，而4位量化会引入少量碰撞。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer表示的单射性对于理解模型如何区分不同输入以及表示空间的结构至关重要。先前工作证明了离散提示到隐藏状态映射的泛型单射性，但缺乏对参数空间中单射性结构和训练过程中单射性持久性的深入分析。

Method: 理论分析：定义碰撞判别式和单射层，证明单射性在参数空间中的二分性质；研究在优化器和初始化条件下单射性沿训练轨迹的持久性；分析对称群对单射性的影响。实证研究：定义分离边界和共Lipschitz常数作为几何诊断指标，在预训练模型（LLaMA-3、Qwen）和从头训练的GPT-2上评估这些指标。

Result: 理论结果：证明了单射性在参数空间中的开放稠密性质，以及在温和条件下单射性沿训练轨迹的持久性。实证结果：在完整精度和8位量化下未观察到碰撞，4位量化引入少量碰撞并显著降低共Lipschitz估计；小规模GPT-2的归一化指标在训练过程中保持稳定。

Conclusion: Transformer表示在连续参数理想化下是泛型且持续单射的，其实际可逆性可以通过简单的几何诊断指标来探测。量化精度对单射性有显著影响，4位量化会损害表示的区分能力。

Abstract: Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.

</details>


### [24] [DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models](https://arxiv.org/abs/2511.14813)
*Yifan Li,Qin Li,Min Zhang,Min Zhang,Peixin Wang*

Main category: cs.LG

TL;DR: 本文提出了推导关系(DR)和推导能力(DC)的概念来评估LLMs的推理能力，开发了DEVAL评估框架，并提出了推导提示(DP)方法显著提升LLMs的推导能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在数据上的推理能力是一个重要但尚未充分研究的问题。相比人类能够根据输入变化推导相应输出修改的推理模式，LLMs的这种能力尚未得到全面描述和评估。

Method: 正式定义推导关系(DR)和推导能力(DC)，提出系统构建的DEVAL评估框架，在7个主流任务中评估5个流行LLMs和1个大型推理模型，并提出推导提示(DP)方法。

Result: 主流LLMs如GPT-4o和Claude3.5表现出中等的DR识别能力，但在问题解决场景中应用DR时存在显著下降。推导提示(DP)方法使所有测试LLMs的DC平均提升15.2%，优于常用提示工程技术。

Conclusion: LLMs在推导能力方面仍有改进空间，推导提示方法能有效提升其推导能力，为改进LLMs推理能力提供了新思路。

Abstract: Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.

</details>


### [25] [Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence](https://arxiv.org/abs/2511.14823)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 本文提出动态嵌套层次结构作为机器学习模型的新范式，通过允许模型在训练或推理过程中自主调整优化层级数量、嵌套结构和更新频率，解决现有模型在非平稳环境中的适应性问题，实现真正的终身学习。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型（包括大语言模型）在静态任务中表现出色，但在非平稳环境中表现不佳，因为其刚性架构阻碍了持续适应和终身学习能力。现有模型存在逆行性遗忘问题，无法有效适应分布变化。

Method: 基于嵌套学习范式，提出动态嵌套层次结构，使模型能够自主调整优化层级数量、嵌套结构和更新频率。该方法受神经可塑性启发，无需预定义约束即可实现自我进化，通过动态压缩上下文流来适应分布变化。

Result: 通过严格的数学公式、收敛性理论证明、表达能力边界分析以及不同机制下的次线性遗憾证明，结合在语言建模、持续学习和长上下文推理中的实证演示，动态嵌套层次结构展现出卓越性能。

Conclusion: 动态嵌套层次结构为实现自适应、通用目的智能奠定了基础性进展，是人工智能和机器学习发展的下一个进化步骤。

Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.

</details>


### [26] [Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization](https://arxiv.org/abs/2511.14846)
*Yifeng Ding,Hung Le,Songyang Han,Kangrui Ruan,Zhenghui Jin,Varun Kumar,Zijian Wang,Anoop Deoras*

Main category: cs.LG

TL;DR: 提出GTPO算法解决多轮工具集成推理中现有RL方法奖励信号不足的问题，通过轮级奖励分配、基于回报的优势估计和自监督奖励塑造来提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在多轮工具集成推理任务中面临粗粒度的轨迹级奖励问题，导致训练停滞，需要更精细的奖励信号来支持复杂的多轮交互。

Method: GTPO算法包含三个关键创新：轮级奖励分配提供细粒度反馈、基于回报的优势估计使用归一化折扣回报作为优势、自监督奖励塑造利用生成代码的自监督信号来丰富稀疏的二元结果奖励。

Result: 综合评估显示GTPO在多样化推理基准上平均比GRPO提升3.0%，证明了其在推进复杂数学推理方面的有效性。

Conclusion: GTPO通过精细的奖励机制设计有效解决了多轮工具集成推理中的训练挑战，为复杂数学推理提供了更有效的强化学习解决方案。

Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.

</details>


### [27] [FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications](https://arxiv.org/abs/2511.14865)
*Dwipam Katariya,Snehita Varma,Akshat Shreemali,Benjamin Wu,Kalanand Mishra,Pranab Mohanty*

Main category: cs.LG

TL;DR: FinTRec是一个基于Transformer的金融推荐框架，解决了金融服务中长序列交互和多产品协调的挑战，相比传统树模型在性能和效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 金融服务中的推荐系统面临独特挑战：长范围用户交互产生的时间异质性上下文，以及需要协调多个相关产品来支持不同广告投放和个性化推送，同时平衡竞争性业务目标。

Method: 提出FinTRec框架，采用基于Transformer的架构，通过统一架构支持产品适配，实现跨产品信号共享，降低训练成本和技术债务。

Result: 通过历史模拟和实时A/B测试，FinTRec持续优于生产级树模型基线，在所有产品上提升了离线性能，同时减少了训练成本。

Conclusion: FinTRec证明了Transformer架构在金融服务推荐系统中的可行性和有效性，是首个全面解决技术和业务考量的统一序列推荐模型研究。

Abstract: Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.

</details>


### [28] [It's LIT! Reliability-Optimized LLMs with Inspectable Tools](https://arxiv.org/abs/2511.14903)
*Ruixin Zhang,Jon Donnelly,Zhicheng Guo,Ghazal Khalighinejad,Haiyang Huang,Alina Jade Barnett,Cynthia Rudin*

Main category: cs.LG

TL;DR: LIT框架通过强制LLMs使用外部可靠工具来解决LLM推理过程不透明的问题，提升解决方案的可靠性和可调试性。


<details>
  <summary>Details</summary>
Motivation: LLMs的推理过程不透明，在高风险领域难以信任，且可能选择不可靠的解决方案，即使有更好的选项可用。

Method: 基于现有LLMs的工具调用能力构建LIT框架，让LLMs选择最可靠且易于调试的解决方案路径，可能涉及多个顺序工具调用。

Result: LLMs在使用LIT框架后能够实现更可靠和明智的问题解决，同时保持任务性能。

Conclusion: LIT框架有效提升了LLMs在问题解决中的可靠性和可调试性，为高风险领域的应用提供了更可信的解决方案。

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.

</details>


### [29] [Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis](https://arxiv.org/abs/2511.14922)
*Pranay Kumar Peddi,Dhrubajyoti Ghosh*

Main category: cs.LG

TL;DR: 本文提出了Causal-GCN框架，通过整合do-calculus的后门调整来识别对阿尔茨海默病进展具有稳定因果影响的大脑区域，解决了传统图学习方法中混淆变量的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的深度图学习方法在阿尔茨海默病分类中往往混淆了人口统计学和遗传因素与疾病特异性特征，需要开发能够识别稳定因果关系的模型。

Method: 提出Causal-GCN框架，将结构连接体表示为图结构，通过主成分分析总结年龄、性别和APOE4基因型等混淆变量，并整合do-calculus的后门调整进行因果干预分析。

Result: 在ADNI队列的484名受试者中，Causal-GCN实现了与基线GNN相当的性能，同时提供了可解释的因果效应排名，突出了与已知AD神经病理学一致的后部、扣带回和岛叶枢纽区域。

Conclusion: Causal-GCN框架能够有效识别对阿尔茨海默病进展具有因果影响的大脑区域，为理解疾病机制提供了可解释的因果洞察。

Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.

</details>


### [30] [How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding](https://arxiv.org/abs/2511.14936)
*Mathieu Dufour,Andrew Duncan*

Main category: cs.LG

TL;DR: 本文首次系统比较了四种用于医院出院总结自动诊断编码的隐私保护训练管道，发现在中度隐私预算下，基于DP训练教师模型的知识蒸馏方法在隐私-效用权衡上表现最佳，能恢复63%的非私有性能。


<details>
  <summary>Details</summary>
Motivation: 临床文本训练的大型语言模型存在泄露敏感患者信息的风险，但差分隐私方法通常会严重降低诊断准确性。目前尚不清楚哪种隐私保护策略在临床语言任务中效果最好。

Method: 使用相同的10亿参数模型和匹配的隐私预算，系统比较四种训练管道：直接DP-SGD、DP合成数据训练、知识蒸馏等，用于预测ICD-9编码。

Result: 在中等隐私预算（ε=4,6）下，从DP训练教师模型的知识蒸馏方法优于直接DP-SGD和DP合成数据训练，能恢复高达63%的非私有性能，同时保持强大的经验隐私保护（成员推断AUC≈0.5）。

Conclusion: 不同架构在隐私-效用权衡上存在巨大差异，知识蒸馏被确定为保护隐私的临床NLP最实用的途径。

Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.

</details>


### [31] [Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference](https://arxiv.org/abs/2511.14961)
*Artur A. Oliveira,Mateus Espadoto,Roberto M. Cesar,Roberto Hirata*

Main category: cs.LG

TL;DR: Graph Memory (GM) 是一个结构化的非参数框架，通过区域级原型的关系记忆增强基于嵌入的推理，将嵌入空间总结为带可靠性指标的原型节点，并通过几何和上下文关系边连接。


<details>
  <summary>Details</summary>
Motivation: 传统的训练实例独立处理方式无法有效利用嵌入空间的结构信息，GM旨在通过构建关系记忆来统一实例检索、原型推理和图标签传播，提供更可靠的推理和解释。

Method: GM将嵌入空间总结为原型节点，标注可靠性指标，并通过编码几何和上下文关系的边连接这些节点，形成一个支持归纳推理的图结构模型。

Result: 在合成和真实数据集（包括乳腺组织病理学IDC）上的实验表明，GM在准确率上与kNN和标签传播方法相当，但具有更好的校准性和更平滑的决策边界，且所需样本数量少一个数量级。

Conclusion: GM通过显式建模可靠性和关系结构，为非参数学习中局部证据与全局一致性之间提供了原则性的桥梁，实现了高效推理和可信解释的统一。

Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.

</details>


### [32] [Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment](https://arxiv.org/abs/2511.15032)
*Jeffrey Jiang,Kevin Hong,Emily Kuczynski,Gregory Pottie*

Main category: cs.LG

TL;DR: 本文开发了一个动态时间序列环境来模拟课堂设置，结合强化学习智能辅导系统，通过探测性干预来平衡学生状态估计的准确性和教学干扰成本。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统可以利用过去学生的信息进行个性化教学，但每个新学生都是独特的，且学习过程只能部分观察，因此需要开发能够处理这些挑战的模拟环境和方法。

Method: 设计动态时间序列环境模拟课堂，包括辅导、讲座和考试等师生干预；开发结合个体状态学习和群体信息的强化学习ITS，使用探测性干预来减少学生估计难度；比较标准RL算法与基于规则的启发式方法。

Result: RL算法和启发式方法提供不同解决方案但效果相似；随着隐藏信息增加问题难度加大，探测性干预能显著提升效果；两种策略对改变学生群体分布都灵活，但RL策略在帮助困难班级方面表现不佳；在非探测策略下，测验和期中考试结构比仅期末考试结构表现更好。

Conclusion: 探测性干预能有效平衡信息获取与教学干扰，强化学习和启发式方法在智能辅导系统中各有优势，课程结构设计对教学效果有重要影响，需要根据具体情况选择合适策略。

Abstract: While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.

</details>


### [33] [Interpretable temporal fusion network of multi- and multi-class arrhythmia classification](https://arxiv.org/abs/2511.15062)
*Yun Kwan Kim*

Main category: cs.LG

TL;DR: 提出了一种结合局部和全局特征提取与注意力机制的心律失常检测框架，能够处理不同长度的心律失常，在MITDB和AFDB数据库上取得了优于基准模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有临床决策支持系统在处理不同长度的心律失常时面临挑战，特别是心律失常发作时间变化的情况未被充分考虑。

Method: 采用局部和全局特征提取结合注意力机制的框架，实现约束输入长度下的心律失常检测和分类。

Result: 在MITDB数据库上获得96.45%的持续时间F1分数、82.05%的发作F1分数和96.31%的Dice分数；在AFDB数据库上获得97.57%、98.31%和97.45%的相应分数，均优于基准模型。

Conclusion: 该方法能有效捕捉局部和全局信息及动态特征，无显著信息损失，可更准确地检测心律失常并精确定位发生时间，有助于制定更精确的治疗方案。

Abstract: Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.

</details>


### [34] [Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer](https://arxiv.org/abs/2511.15067)
*Zisong Wang,Xuanyu Wang,Hang Chen,Haizhou Wang,Yuxin Chen,Yihang Xu,Yunhe Yuan,Lihuan Luo,Xitong Ling,Xiaoping Liu*

Main category: cs.LG

TL;DR: 开发了基于病理全切片图像的TDAM-CRC多实例学习模型，用于结直肠癌预后预测，并通过多组学分析揭示分子机制，识别出MRPL37作为关键预后生物标志物。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌具有高度异质性，传统的TNM分期系统不足以满足个性化医疗需求，需要开发更准确的预后预测工具。

Method: 使用TCGA发现队列（n=581）训练TDAM-CRC模型，在独立外部队列（n=1031）验证，整合多组学数据提高模型可解释性并识别预后生物标志物。

Result: TDAM-CRC在两个队列中均实现稳健的风险分层，预测性能显著优于传统临床分期系统和多个先进模型。多组学分析显示高风险亚型与代谢重编程和免疫抑制肿瘤微环境相关，识别出MRPL37作为关键枢纽基因。

Conclusion: TDAM-CRC为结直肠癌提供了改进的风险分层工具，揭示了新的分子靶点，并促进个性化临床决策。

Abstract: Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.

</details>


### [35] [Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature](https://arxiv.org/abs/2511.15136)
*Andrew Amos,Joanne Lee,Tarun Sen Gupta,Bunmi S. Malau-Aduli*

Main category: cs.LG

TL;DR: 提出了一种新的稀疏矩阵乘法算法，使得能够对整个Medline数据集应用自组织映射，从而更完整地映射现有医学知识。


<details>
  <summary>Details</summary>
Motivation: 现有算法由于内存和处理需求呈指数增长，只能处理Medline数据库的小子集，限制了医学知识的完整映射。

Method: 设计了一种新颖的稀疏矩阵乘法算法，并将其应用于整个Medline数据集的自组织映射。

Result: 成功实现了对整个Medline数据集的自组织映射，提供了更完整的医学知识图谱。

Conclusion: 该算法不仅实现了对大规模医学数据的完整映射，还提高了随时间更新自组织映射的可行性。

Abstract: Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.

</details>


### [36] [From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs](https://arxiv.org/abs/2511.15137)
*Xiaoxuan Wang,Bo Liu,Song Jiang,Jingzhou Liu,Jingyuan Qi,Xia Chen,Baosheng He*

Main category: cs.LG

TL;DR: 提出GRPO-Verif算法，通过统一损失函数联合优化解决方案生成和自我验证能力，使用可调超参数控制验证信号权重。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过强化学习显著提升了推理能力，但仍难以一致地验证自身的推理轨迹，需要研究如何增强其自我验证能力并进一步提升推理性能。

Method: GRPO-Verif算法，在统一损失函数中联合优化解决方案生成和自我验证，通过可调超参数控制验证信号的权重。

Result: 实验结果表明该方法在保持推理性能的同时增强了自我验证能力。

Conclusion: GRPO-Verif算法成功增强了语言模型的自我验证能力，同时保持了推理性能的相当水平。

Abstract: The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.

</details>


### [37] [Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems](https://arxiv.org/abs/2511.15138)
*Hyo-Jeong Jang,Hye-Bin Shin,Kang Yin*

Main category: cs.LG

TL;DR: 本文提出了一种不确定性感知的主动学习框架，通过联合利用模型不确定性和跨模态一致性来增强对标签噪声的鲁棒性，用于EEG情感识别任务。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受伪影和个体差异影响，情感标签通常来自主观且不一致的报告，使得稳健的情感解码特别困难。需要一种能够处理标签噪声的数据高效方法。

Method: 使用不确定性感知主动学习框架，通过跨模态对齐评估不确定性来源（认知模糊或传感器噪声），将EEG和面部特征嵌入共享潜在空间，选择性查询噪声样本获取反馈。

Result: 在ASCERTAIN数据集上的实验验证了该方法的效率和鲁棒性，显示出作为脑机接口系统中数据高效且噪声容忍的EEG情感解码方法的潜力。

Conclusion: 该方法通过结合模型不确定性和跨模态一致性，有效提升了EEG情感识别对标签噪声的鲁棒性，为脑机接口系统提供了数据高效且噪声容忍的解决方案。

Abstract: Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.

</details>


### [38] [Complex variational autoencoders admit Kähler structure](https://arxiv.org/abs/2511.15172)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 本文研究了复数变分自编码器中的Kähler几何结构，提出了基于复数高斯混合的Kähler势导数方法，有效计算Fisher信息度量，并在解码器几何中实现潜在空间正则化。


<details>
  <summary>Details</summary>
Motivation: 探索复数变分自编码器的几何结构，特别是Kähler几何在复数潜在空间中的表现，以改进潜在表示的质量和减少语义异常值。

Method: 采用复数变分自编码器，在复数潜在空间下推导Fisher信息度量，提出Kähler势导数方法替代自动微分，实现高效计算和潜在空间正则化。

Result: 提出的方法能够生成更平滑的表示，减少语义异常值，同时通过加权复数体积元素进行采样，提高了计算效率。

Conclusion: 复数变分自编码器确实展现出Kähler几何结构，通过几何正则化可以改善潜在表示质量，提出的Kähler势导数方法为大规模计算提供了高效替代方案。

Abstract: It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.

</details>


### [39] [FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model](https://arxiv.org/abs/2511.15174)
*Yi Xu,Zhigang Chen,Rui Wang,Yangfan Li,Fengxiao Tang,Ming Zhao,Jiaqi Liu*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的少样本故障时间序列生成框架，通过正负差异适配器利用预训练的正常数据分布来建模正常与故障域之间的差异，并引入多样性损失防止模式崩溃，在真实性和多样性方面显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 工业设备监控中故障诊断至关重要，但故障数据稀缺严重阻碍了数据驱动方法的发展。现有时间序列生成模型在少样本场景下难以捕捉故障分布，生成样本缺乏真实性和多样性。

Method: 基于扩散模型的少样本故障时间序列生成框架，采用正负差异适配器利用预训练正常数据分布建模正常与故障域差异，并引入多样性损失通过样本间差异正则化鼓励生成多样化故障样本。

Result: 实验结果表明，该模型在真实性和多样性方面显著优于传统方法，在关键基准测试中达到了最先进的性能。

Conclusion: 提出的基于扩散模型的少样本故障时间序列生成框架有效解决了故障数据稀缺问题，能够生成真实且多样化的故障样本，为工业设备故障诊断提供了有力支持。

Abstract: In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.

</details>


### [40] [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)
*Yuxuan Gu,Weimin Bai,Yifei Wang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: MARVAL是一个基于蒸馏的框架，将掩码自回归扩散模型的扩散链压缩为单个生成步骤，实现30倍加速并保持样本质量，同时使强化学习后训练变得实用。


<details>
  <summary>Details</summary>
Motivation: 解决掩码自回归扩散模型推理速度慢的问题，其分层推理机制（外部AR解掩码循环和内部扩散去噪链）不仅影响生成效率，还阻碍了强化学习后训练的实际应用。

Method: 提出基于分数的变分目标，将掩码自回归扩散模型蒸馏为单步生成；开发MARVAL-RL高效强化学习框架，实现可验证奖励的RL后训练。

Result: 在ImageNet 256*256上，MARVAL-Huge达到FID 2.00，相比MAR-diffusion加速30倍以上；MARVAL-RL在ImageNet数据集上持续提升CLIP和图像奖励分数。

Conclusion: MARVAL展示了掩码自回归扩散模型蒸馏和强化学习的首个实用路径，实现了快速采样和更好的偏好对齐。

Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.

</details>


### [41] [Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones](https://arxiv.org/abs/2511.15208)
*Ranfei Chen,Ming Chen,Kaifei Wang*

Main category: cs.LG

TL;DR: 本文提出了ATPO方法，通过分析扩散大语言模型推理轨迹中的不确定性模式，动态选择关键步骤进行梯度更新，显著提升了推理准确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的强化学习方法均匀分配策略梯度到所有去噪步骤，隐含假设所有步骤同等重要。本文挑战这一假设，发现推理轨迹中存在结构化的'混淆区域'——不确定性和不稳定的瞬时峰值，这些区域强烈预测最终成功或失败。

Method: 提出自适应轨迹策略优化(ATPO)，使用基于熵的不确定性、置信度边缘不确定性和熵变化率等步骤级指标分析轨迹，通过混合RoEC+CM规则动态重新分配梯度更新到高影响力步骤，而不改变RL目标、奖励或计算预算。

Result: ATPO在多个基准测试中显著提升了推理准确性和训练稳定性，证明利用轨迹动态是推进dLLM强化学习的关键。

Conclusion: 通过识别和针对性优化轨迹中的关键混淆区域，ATPO为扩散大语言模型的强化学习对齐提供了更有效的方法，展示了步骤选择策略在复杂推理任务中的重要性。

Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.

</details>


### [42] [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248)
*Kai Yang,Xin Xu,Yangkun Chen,Weijie Liu,Jiafei Lyu,Zichuan Lin,Deheng Ye,Saiyong Yang*

Main category: cs.LG

TL;DR: 提出EntroPIC方法，通过比例-积分控制动态调整正负样本的损失系数，稳定大语言模型训练中的熵值，确保高效探索和稳定进展。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以在训练过程中维持适当的熵水平，因为正负样本以不同方式影响熵值，导致模型可能陷入次优行为。

Method: EntroPIC方法：通过比例-积分控制自适应调整正负样本的损失系数，动态调节它们对熵的影响，实现熵的稳定控制。

Result: 实验结果表明该方法能成功维持期望的熵水平，为大语言模型提供稳定且最优的强化学习训练。

Conclusion: EntroPIC方法能有效控制大语言模型训练中的熵值，确保稳定的探索过程，防止模型过早收敛到次优解。

Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.

</details>


### [43] [GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning](https://arxiv.org/abs/2511.15256)
*Yanchen Xu,Ziheng Jiao,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出了GRPO-RM方法，将GRPO强化学习技术从大语言模型扩展到表示学习模型，通过预定义输出集和专门设计的奖励函数来优化表示模型的性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在大语言模型微调中表现出色，但能否推广到表示学习模型尚不明确。本文旨在研究GRPO类策略在表示模型后训练中的性能。

Method: 建立预定义输出集替代LLM中的token序列采样，生成输出组用于GRPO的概率驱动优化，并设计专门的奖励函数以适应表示模型的特性。

Result: 在多个真实世界数据集上进行广泛实验，验证了所提方法的有效性。

Conclusion: GRPO-RM成功将GRPO技术扩展到表示学习模型，为表示模型的后训练优化提供了有效解决方案。

Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

</details>


### [44] [SNAP: Low-Latency Test-Time Adaptation with Sparse Updates](https://arxiv.org/abs/2511.15276)
*Hyeongheon Cha,Dong Min Kim,Hye Won Chung,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: SNAP是一个稀疏测试时自适应框架，通过减少自适应频率和数据使用量，在保持精度的同时显著降低计算成本，特别适合资源受限的边缘环境。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法依赖频繁自适应和高计算成本，不适合资源受限的边缘环境，需要一种更高效的解决方案。

Method: 提出SNAP框架，包含两个关键组件：类域代表内存(CnDRM)识别存储代表样本，推理时批量感知内存归一化(IoBMN)动态调整归一化统计量。

Result: SNAP仅使用1%的数据流就能保持竞争力精度，延迟降低高达93.12%，精度下降低于3.3%。

Conclusion: SNAP展示了在边缘设备上实际应用的强大潜力，特别适合延迟敏感的应用场景。

Abstract: Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.

</details>


### [45] [Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs](https://arxiv.org/abs/2511.15300)
*Rayen Dhahri,Steffen Urban*

Main category: cs.LG

TL;DR: Quant-Trim是一种训练阶段方法，通过渐进式伪量化和反向剪枝技术，创建硬件无关的检查点，使模型在不同边缘加速器后端和精度选择下保持一致的准确性。


<details>
  <summary>Details</summary>
Motivation: 边缘加速器在低比特量化时，由于不同厂商编译器的缩放、裁剪和内核支持差异，导致相同浮点检查点在不同后端产生不一致的准确性，迫使开发者调整参数或重构模型。

Method: 结合渐进式伪量化来对齐训练与部署的整数网格，以及反向剪枝来控制异常值驱动的尺度膨胀，同时保持可学习性。该方法与量化方案无关，无需特定厂商的图修改。

Result: 在各种模型和任务中，Quant-Trim缩小了浮点与低比特之间的差距，减少了对编译器启发式/校准的依赖，避免了针对每个后端的重新训练。

Conclusion: Quant-Trim提供了一种有效的训练阶段解决方案，使模型能够在不同边缘加速器后端和精度配置下保持稳定性能，提高了部署的灵活性和效率。

Abstract: Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.

</details>


### [46] [On the Internal Semantics of Time-Series Foundation Models](https://arxiv.org/abs/2511.15324)
*Atharva Pandey,Abhilash Neog,Gautam Jajoo*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列基础模型（TSFMs）中概念的可解释性，分析了不同层编码的概念类型、概念参数的线性可恢复性、表示在模型深度上的演化以及概念组合的处理方式。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在经验上取得了成功，但其内部如何表示基本时间序列概念的机制仍然不清楚。本文旨在系统性地理解这些模型的概念可解释性。

Method: 使用分层分析、线性可恢复性测试和表示相似性度量等方法，系统性地探究了TSFMs的概念表示机制。

Result: 研究发现早期层主要捕捉局部时域模式，深层编码离散度和变化时间信号，频谱和扭曲因子最难线性恢复。在组合设置中，探针性能下降，显示概念间存在干扰。

Conclusion: 虽然原子概念能够可靠地定位，但概念组合仍然是当前TSFMs的一个关键限制，突显了这些模型在表示交互时间现象方面的局限性。

Abstract: Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.

</details>


### [47] [Multi-layer Stack Ensembles for Time Series Forecasting](https://arxiv.org/abs/2511.15350)
*Nathanael Bosch,Oleksandr Shchur,Nick Erickson,Michael Bohlke-Schneider,Caner Türkmen*

Main category: cs.LG

TL;DR: 本文系统评估了33种时间序列预测集成方法，发现堆叠法能持续提升精度，但单一堆叠器在不同任务中表现不一。为此提出多层堆叠框架，在多样化预测场景中均能提供更优精度。


<details>
  <summary>Details</summary>
Motivation: 集成学习在表格任务中效果显著，但在时间序列预测领域应用不足，目前仍以简单线性组合为主流。本文旨在系统探索时间序列预测的集成策略。

Method: 评估33种集成模型（包括现有和新提出的），在50个真实世界数据集上进行测试。提出多层堆叠框架，结合不同堆叠器模型的优势。

Result: 堆叠法能持续提高预测精度，但没有单一堆叠器在所有任务中表现最佳。提出的多层堆叠框架在多样化预测场景中均能提供更优精度。

Conclusion: 基于堆叠的方法有潜力改进时间序列预测的AutoML系统，多层堆叠框架是解决不同堆叠器性能差异的有效方案。

Abstract: Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.

</details>


### [48] [CID: Measuring Feature Importance Through Counterfactual Distributions](https://arxiv.org/abs/2511.15371)
*Eddie Conti,Álvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 本文提出了一种新的局部特征重要性方法CID，通过生成正负反事实样本、使用核密度估计建模分布，并基于分布差异度量来排序特征重要性，该方法在忠实性指标上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习中评估个体特征重要性对于理解模型决策过程至关重要，但现有方法缺乏明确的真实基准进行比较，需要替代的、有理论基础的度量方法。

Method: 提出CID方法：生成正负反事实样本集，使用核密度估计建模其分布，基于分布差异度量对特征进行排序，该度量具有严格的数学框架并满足有效度量的关键性质。

Result: 与成熟的局部特征重要性解释器相比，CID方法不仅提供了对现有方法的补充视角，还在忠实性指标（全面性和充分性）上提高了性能，产生了更忠实的系统解释。

Conclusion: CID方法作为模型分析的有价值工具具有潜力，能够提供更可靠的特征重要性解释。

Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.

</details>


### [49] [Parameter Importance-Driven Continual Learning for Foundation Models](https://arxiv.org/abs/2511.15375)
*Lingxiang Wang,Hainan Zhang,Zhiming Zheng*

Main category: cs.LG

TL;DR: PIECE是一种基于参数重要性估计的持续增强方法，通过仅更新0.1%的核心参数来避免灾难性遗忘，同时有效学习领域知识，无需访问历史数据或增加模型参数。


<details>
  <summary>Details</summary>
Motivation: 领域特定后训练通常会导致灾难性遗忘，使基础模型失去通用推理能力，限制其在动态现实环境中的适应性。在保持通用能力的同时获取下游领域知识是大型语言和多模态模型的核心挑战。

Method: PIECE方法使用两种重要性估计器：基于Fisher信息的PIECE-F和基于二阶归一化的PIECE-S，结合梯度和曲率信息，选择性地仅更新与新任务最相关的0.1%核心参数。

Result: 在三个语言模型和两个多模态模型上的实验表明，PIECE能够保持通用能力，并在各种下游任务中实现最先进的持续学习性能。

Conclusion: PIECE为可扩展、领域自适应基础模型提供了一条实用路径，能够避免灾难性遗忘问题。

Abstract: Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.

</details>


### [50] [Proximal Approximate Inference in State-Space Models](https://arxiv.org/abs/2511.15409)
*Hany Abdulsamad,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.LG

TL;DR: 提出一种基于变分拉格朗日框架的非线性非高斯状态空间模型状态估计算法，通过熵信任域更新和动态约束实现贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 解决非线性非高斯状态空间模型中的状态估计问题，传统方法在处理此类复杂模型时存在局限性。

Method: 采用变分拉格朗日公式，将贝叶斯推断转化为序列熵信任域更新问题；对于高斯-马尔可夫近似使用递归方案，对于一般非线性非高斯模型使用广义统计线性回归和傅里叶-埃尔米特矩匹配。

Result: 开发出一类具有良好计算复杂度的前向-后向算法，算法结构由变分后验的因子分解决定。

Conclusion: 该框架为非线性非高斯状态估计提供了一种有效的变分推断方法，能够处理复杂的动态系统建模问题。

Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.

</details>


### [51] [Towards Understanding Layer Contributions in Tabular In-Context Learning Models](https://arxiv.org/abs/2511.15432)
*Amir Rezaei Balef,Mykhailo Koshil,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 该论文研究了表格上下文学习模型中各层的作用，发现只有部分层共享共同表示语言，存在结构冗余，为模型压缩和可解释性提供了机会。


<details>
  <summary>Details</summary>
Motivation: 尽管表格上下文学习模型与大型语言模型在架构上相似，但个体层在表格预测中的贡献机制尚不清楚，需要深入研究各层潜在空间的演化。

Method: 通过"层作为画家"的视角分析TabPFN和TabICL模型，研究各层潜在空间的演化动态，识别冗余层，并与LLMs中的动态进行比较。

Result: 发现只有部分层子集共享共同表示语言，表明存在结构冗余，这为模型压缩和可解释性改进提供了机会。

Conclusion: 表格ICL模型存在结构冗余，只有特定层子集参与共同表示，这一发现有助于模型优化和可解释性提升。

Abstract: Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.

</details>


### [52] [TSFM in-context learning for time-series classification of bearing-health status](https://arxiv.org/abs/2511.15447)
*Michel Tokic,Slobodan Djukanović,Anja von Beuningen,Cheng Feng*

Main category: cs.LG

TL;DR: 提出了一种使用时序基础模型进行上下文学习的分类方法，无需微调模型即可对训练数据之外的数据进行分类，应用于伺服压力机轴承健康状态评估。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种无需微调预训练模型就能对新数据进行分类的方法，超越定制化窄AI解决方案，向更广泛的AI驱动维护系统发展。

Method: 将示例以目标（类别ID）和协变量（数据矩阵）形式表示在模型提示中，通过上下文学习沿预测轴对未知协变量数据模式进行分类。将频域参考信号转换为伪时序模式，生成对齐的协变量和目标信号。

Result: 该方法在不同操作条件下均表现出有效性，能够预测分类数据与预定义标签对应的概率。

Conclusion: 这标志着从定制窄AI解决方案向更广泛的AI驱动维护系统的重要进展，利用了预训练模型的可扩展性。

Abstract: This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.

</details>


### [53] [FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning](https://arxiv.org/abs/2511.15454)
*Ouiame Marnissi,Hajar EL Hammouti,El Houcine Bergou*

Main category: cs.LG

TL;DR: FairEnergy是一个公平感知的联邦学习能量最小化框架，通过在无线边缘系统中联合优化设备选择、带宽分配和压缩级别，实现了更高的模型精度和显著的能量节省。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在无线边缘系统中面临异构资源、不公平参与和有限通信容量的挑战，需要在保持数据隐私的同时平衡能量效率、公平性和模型精度。

Method: 提出FairEnergy框架，将包含更新幅度和压缩比的贡献分数整合到联合优化中，通过松弛二元选择变量和应用拉格朗日分解来处理全局带宽耦合，然后进行每设备子问题优化。

Result: 在非IID数据上的实验表明，与基线策略相比，FairEnergy实现了更高的准确率，同时将能耗降低了高达79%。

Conclusion: FairEnergy成功解决了联邦学习中的能量效率与公平参与之间的权衡问题，为无线边缘系统提供了有效的解决方案。

Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.

</details>


### [54] [NTK-Guided Implicit Neural Teaching](https://arxiv.org/abs/2511.15487)
*Chen Zhang,Wei Zuo,Bingyang Cheng,Yikun Wang,Wei-Bin Kou,Yik Chung WU,Ngai Wong*

Main category: cs.LG

TL;DR: 提出了NTK引导的隐式神经教学（NINT）方法，通过动态选择最大化全局功能更新的坐标来加速隐式神经表示的训练，将训练时间减少近一半。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）在高分辨率信号拟合时需要优化数百万个坐标，导致计算成本过高，需要更高效的训练方法。

Method: 利用神经正切核（NTK）对示例进行评分，通过NTK增强的损失梯度范数来选择坐标，同时考虑拟合误差和异构杠杆效应（自影响和跨坐标耦合）。

Result: 通过大量实验证明，NINT显著减少训练时间近一半，同时保持或提高表示质量，在基于采样的加速策略中达到最先进水平。

Conclusion: NINT通过NTK引导的坐标选择有效加速隐式神经表示训练，在计算效率和表示质量之间取得良好平衡。

Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.

</details>


### [55] [Sample-Adaptivity Tradeoff in On-Demand Sampling](https://arxiv.org/abs/2511.15507)
*Nika Haghtalab,Omar Montasser,Mingda Qiao*

Main category: cs.LG

TL;DR: 本文研究了按需采样中样本复杂度和轮数复杂度之间的权衡关系，在可实现的MDL中证明了r轮算法的最优样本复杂度约为dk^{Θ(1/r)}/ε，在不可知情况下提出了在Õ(√k)轮内实现近最优样本复杂度Õ((d+k)/ε²)的算法。


<details>
  <summary>Details</summary>
Motivation: 研究多分布学习中样本复杂度和轮数复杂度之间的基本权衡关系，特别关注按需采样设置下的自适应学习算法。

Method: 提出了新的框架OODS来抽象样本自适应权衡，捕获现有MDL算法；建立了OODS设置下轮数复杂度的紧界；上界直接导出了不可知MDL的Õ(√k)轮算法。

Result: 在可实现的MDL中，证明了r轮算法的最优样本复杂度为dk^{Θ(1/r)}/ε；在不可知情况下，提出了在Õ(√k)轮内实现Õ((d+k)/ε²)样本复杂度的算法；建立了OODS框架的紧轮数复杂度界限。

Conclusion: 实现了多分布学习中样本复杂度和轮数复杂度的近乎最优权衡，OODS框架的下界表明要获得亚多项式轮数复杂度需要绕过OODS固有硬度的新技术。

Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [56] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: 本文介绍了FERMAT强化学习环境，用于自动化发现数学理论，并探索了自动评估数学对象有趣性的问题，使用基于LLM的进化算法显著提升了在初等数论和有限域中的发现能力。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中开放式的数学理论发现这一重大挑战，通过构建强化学习环境来建模概念发现和定理证明过程。

Method: 引入FERMAT强化学习环境，使用符号化动作建模概念发现和定理证明；采用基于LLM的进化算法，特别是引入函数抽象技术，来合成非平凡的有趣性度量。

Result: 在初等数论和有限域领域，基于LLM的进化算法相比硬编码基线取得了显著改进，能够更有效地发现数学对象的有趣性。

Conclusion: FERMAT环境为数学理论发现开辟了新的强化学习问题空间，基于LLM的进化算法在自动评估数学对象有趣性方面表现出色，为自动化数学发现提供了有效工具。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [57] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个用于检查和扰动多智能体交互中信念状态的系统级框架，通过记录回放交互、查询智能体信念和理由、注入反事实证据来测试信念结构对新信息的响应。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体科学推理中的信念形成和认知孤岛，提供一种可重现的方法来观察和测试这些动态，这在人类专家中是不可能的。

Method: 使用多智能体医疗案例模拟器，包括具有角色特定先验的大型语言模型智能体（如神经科医生、传染病专家），它们写入共享医疗记录并与调解员交互，在关键诊断时刻设置断点进行信念查询。

Result: 模拟显示智能体信念往往反映现实世界的学科立场，包括过度依赖规范研究和抵制反证据，这些信念可以被追踪和质疑。

Conclusion: Ask WhAI通过使这些动态可见和可测试，为研究多智能体科学推理中的信念形成和认知孤岛提供了一种可重现的方法。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [58] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 本文提出了一种完全自动化的LLM辅助工作流，使用GPT-4o处理文本位置信息，并通过交叉验证三个独立的地理信息库来分配几何形状，为EM-DAT灾害数据库提供可靠的地理编码。


<details>
  <summary>Details</summary>
Motivation: 灾害事件的地理位置数据对于风险评估至关重要，但现有数据库如EM-DAT中的位置信息通常是非结构化的文本形式，存在粒度不一致和拼写错误等问题，难以与空间数据集集成。

Method: 开发了基于GPT-4o的自动化工作流，处理文本位置信息，并交叉验证GADM、OpenStreetMap和Wikidata三个地理信息库来分配几何形状，同时为每个位置分配可靠性评分。

Result: 应用于2000-2024年的EM-DAT数据集，成功地理编码了14,215个灾害事件，覆盖17,948个独特位置，无需人工干预，涵盖所有灾害类型。

Conclusion: 该方法展示了LLM从非结构化文本中提取和结构化地理信息的潜力，为相关分析提供了可扩展且可靠的方法，支持灵活的重映射到首选框架。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [59] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: 本文记录了Project Rachel行动研究项目，创建并追踪了一个名为Rachel So的完整AI学术身份，通过发布AI生成的研究论文来调查学术生态系统对AI作者身份的反应。


<details>
  <summary>Details</summary>
Motivation: 研究学术生态系统如何应对AI作者身份，为关于超人类、超能力AI系统与学术交流未来的必要讨论提供实证行动研究数据。

Method: 采用行动研究方法，创建AI学术身份Rachel So，在2025年3月至10月期间发表10多篇AI生成的研究论文，并追踪其引用情况和同行评审邀请。

Result: Rachel So成功发表了10多篇论文，获得了引用，并收到了同行评审邀请，表明AI作者身份能够在学术生态系统中获得一定程度的认可。

Conclusion: 这项研究揭示了AI作者身份对出版商、研究人员和整个科学系统的潜在影响，为未来AI与学术交流的融合提供了重要实证依据。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [60] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 本文提出了一种概率方法来量化AI系统训练和测试数据集的代表性，通过比较场景套件特征分布与目标操作域(TOD)特征分布，使用不精确贝叶斯方法处理有限数据和先验不确定性，产生区间值代表性估计。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶汽车）的可信性和安全性，关键在于训练和测试数据集的数据相关安全属性，如代表性。本文重点关注代表性，即训练和测试的场景数据反映系统设计安全运行的操作条件（ODD）或预期遇到条件（TOD）的程度。

Method: 提出概率方法量化代表性，比较场景套件特征编码的统计分布与代表TOD的相应特征分布；应用不精确贝叶斯方法处理有限数据和不确定先验，产生区间值、不确定性感知的代表性估计。

Result: 通过数值示例比较场景套件和推断TOD在操作类别（天气、道路类型、时间等）下的分布，在依赖性和先验不确定性条件下，局部（类别间）和全局估计代表性为区间值。

Conclusion: 该方法能够处理TOD真实分布未知且只能从有限数据推断的情况，提供不确定性感知的代表性量化，有助于评估AI系统训练和测试数据的充分性。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [61] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: 本文提出了一种名为MAQ（宏动作量化）的类人强化学习框架，通过将人类示范蒸馏为宏动作，在最大化奖励的同时实现类人行为。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习智能体虽然在许多领域表现出色，但往往产生与人类行为相比不自然的行为，这引发了可解释性和可信赖性的担忧。

Method: 将类人行为建模为轨迹优化问题，采用后退时域控制作为可实现的实现方法，通过向量量化变分自编码器从人类示范中提取宏动作。

Result: 在D4RL Adroit基准测试中，MAQ显著提高了类人程度，增加了轨迹相似度得分，并在人类评估研究中获得了所有RL智能体中最高的类人排名。

Conclusion: MAQ可以轻松集成到各种现成的RL算法中，为学习类人RL智能体开辟了有前景的方向。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [62] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: OpenBioLLM是一个基于开源模型的多智能体框架，用于基因组问答任务，在保持或超越GeneGPT性能的同时显著降低了延迟和成本。


<details>
  <summary>Details</summary>
Motivation: GeneGPT虽然解决了基因组问答中的复杂推理问题，但依赖专有模型存在可扩展性、运营成本、数据隐私和泛化能力等方面的限制。

Method: 首先使用开源模型（Llama 3.1、Qwen2.5等）在单体架构中复现GeneGPT，然后开发了模块化多智能体框架OpenBioLLM，引入工具路由、查询生成和响应验证的智能体专业化。

Result: OpenBioLLM在90%以上的基准任务中匹配或超越了GeneGPT，在Gene-Turing和GeneHop上的平均得分分别为0.849和0.830，同时延迟降低了40-50%。

Conclusion: 开源多智能体系统在基因组问答任务中具有巨大潜力，能够在保持性能的同时显著提升效率和降低成本。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [63] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: ProRAC是一个神经符号框架，利用LLMs解决RAC问题，通过提取动作和问题元素、逐步执行动作推导最终状态，然后评估查询来得出答案。


<details>
  <summary>Details</summary>
Motivation: 为了解决RAC（动作和变化推理）问题，需要开发一个能够有效结合神经和符号推理的框架，以处理复杂的动作执行和状态变化推理任务。

Method: ProRAC框架从问题中提取RAC基本元素（动作和问题），逐步执行每个动作来推导最终状态，然后针对进展后的状态评估查询以获得答案。

Result: 在多个RAC基准测试上的评估结果显示，ProRAC在不同基准、领域、LLM主干和RAC任务类型上都表现出强大的性能。

Conclusion: ProRAC是一个有效的神经符号框架，能够成功解决各种RAC问题，并在多个测试场景中展现出稳健的性能表现。

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [64] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: Rogue One是一个基于LLM的多智能体框架，通过三个专业智能体（科学家、提取器、测试器）的协作，结合外部领域知识和丰富的定性反馈机制，实现知识驱动的自动特征提取，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动特征提取方法存在架构单一、反馈机制简单、缺乏外部知识整合等问题，限制了特征工程的质量和效果。

Method: 采用多智能体框架，包含科学家、提取器和测试器三个专业智能体，结合检索增强生成（RAG）系统整合外部知识，使用"泛滥-修剪"策略平衡特征探索与利用，并引入丰富的定性反馈机制。

Result: 在19个分类和9个回归数据集上的综合测试表明，Rogue One显著优于现有最先进方法，并能发现新颖、可测试的假设（如心肌数据集中的新生物标志物）。

Conclusion: Rogue One框架通过多智能体协作、知识整合和高级反馈机制，不仅提升了特征提取的性能，还增强了特征的可解释性和科学发现能力。

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [65] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench是首个端到端评估大型推理模型安全性的基准，从输入、中间推理到最终输出全面评估安全风险，包含风险分类分级、细粒度输出分析和人类安全对齐验证。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估主要关注输出层面的判断，很少捕捉推理过程中的动态风险，而大型推理模型的链式推理能力可能引入新的安全风险，如有害内容被巧妙注入、逐渐显现或被误导性理由合理化。

Method: 1) 输入特征化：将风险类别和级别纳入输入设计；2) 细粒度输出分析：通过微思想分块机制将长推理轨迹分割为语义连贯单元；3) 人类安全对齐：用专门设计的人类标注验证基于LLM的评估。

Result: 对19个大型推理模型的评估表明，SafeRBench能够进行详细的多维度安全评估，从多个角度提供风险和保护机制的洞察。

Conclusion: SafeRBench为大型推理模型提供了全面的安全评估框架，能够捕捉推理过程中的动态安全风险，填补了现有安全评估的空白。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [66] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK是一个利用不确定性信号检测大语言模型训练数据中是否包含版权内容的新框架，将LLM的过度自信转化为优势，通过不确定性模式区分已见和未见内容。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在训练过程中可能使用了受版权保护的内容，现有成员推理攻击方法由于LLM的过度自信、缺乏真实训练数据和依赖经验阈值而存在局限。

Method: 采用两阶段策略：(1) 将文件分割成小片段以减少对大规模训练数据的依赖；(2) 基于不确定性的无监督聚类消除对经验调整阈值的需求。

Result: 在LLaMA 7b和LLaMA2 7b上分别达到90.1%和91.6%的平均平衡准确率，相比现有最佳方法提升超过90%，在GPT-J 6B上也保持高性能。

Conclusion: 这是首次将不确定性应用于LLM版权检测的工作，为训练数据透明度提供了实用工具。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [67] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 该论文认为，随着AI研究向复杂问题解决发展，单纯依靠效率提升无法实现可持续的推理AI，需要将明确限制嵌入系统优化和治理中。


<details>
  <summary>Details</summary>
Motivation: AI研究正从模式识别转向多步推理，效率提升接近物理极限，而推理AI缺乏需求饱和点，性能随计算投入指数增长，需要探讨可持续性解决方案。

Method: 通过分析AI发展趋势、效率极限和推理AI特性，提出在系统优化和治理中嵌入明确限制的研究和政策方向。

Result: 识别出推理AI缺乏自然饱和点，性能持续随计算投入指数扩展，单纯效率提升无法解决可持续性问题。

Conclusion: 必须将明确限制嵌入推理AI的优化和治理框架中，仅靠效率提升无法实现可持续性，需要新的研究方法和政策干预。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [68] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 本文分析了AI研究中两种对立的智能观：智能现实主义认为智能是单一、普遍的跨系统可度量能力；智能多元主义认为智能是多样化、情境依赖且无法简化为单一度量的能力。这两种隐含观念深刻影响研究方法、现象解释和风险评估。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中的许多分歧源于对智能本质的不同隐含假设，这些假设影响实证证据的解释、研究方法选择和风险评估，但很少被明确讨论。

Method: 通过分析当前AI研究中的辩论，揭示智能现实主义和智能多元主义这两种对立观念如何在不同研究领域中塑造方法论、解释框架和风险评估。

Result: 发现这两种智能观在三个层面产生根本不同的研究路径：方法论上影响模型选择、基准设计和实验验证；解释上导致对相同现象的矛盾解读；风险评估上产生截然不同的威胁认知和解决方案。

Conclusion: 明确这些基本假设有助于更清晰地理解AI研究中的分歧，促进更富有成效的讨论和更全面的风险评估框架。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [69] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 本文研究了智能体如何通过交互学习获得类人推理能力，提出了IPR（交互物理推理器）模型，使用世界模型推演来评估和强化VLM策略，并引入PhysCode物理中心动作编码。在1000+游戏上预训练后，IPR在三个推理层次上表现稳健，整体匹配GPT-5，在好奇心层面超越GPT-5。


<details>
  <summary>Details</summary>
Motivation: 研究智能体是否能像人类一样通过交互学习获得物理和因果推理能力，并随着经验积累持续改进。

Method: 提出IPR模型，结合世界模型推演来评分和强化VLM策略，引入PhysCode物理中心动作编码，在1000+异构游戏上进行预训练。

Result: IPR在生存、好奇心、实用性三个推理层次上表现稳健，整体性能匹配GPT-5，在好奇心层面超越GPT-5。性能随训练游戏和交互步骤增加而提升，并能零样本迁移到未见游戏。

Conclusion: 物理中心的交互学习是实现持续改进物理推理能力的有效路径。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>
