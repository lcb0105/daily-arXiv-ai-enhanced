<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.LG](#cs.LG) [Total: 68]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [IG-Pruning: Input-Guided Block Pruning for Large Language Models](https://arxiv.org/abs/2511.02213)
*Kangyu Qiao,Shaolei Zhang,Yang Feng*

Main category: cs.CL

TL;DR: IG-Pruning是一种新颖的输入感知块级剪枝方法，通过动态选择层掩码来减少大语言模型的计算成本，在推理时实现高效剪枝。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型计算需求增长，高效推理对实际部署变得至关重要。现有深度剪枝方法依赖固定块掩码，在不同任务和输入上表现不佳。

Method: 提出两阶段方法：(1)通过语义聚类和L0优化发现多样化掩码候选；(2)无需大量训练即可实现高效动态剪枝。

Result: 实验结果表明该方法持续优于最先进的静态深度剪枝方法。

Conclusion: IG-Pruning特别适合资源受限的部署场景，为LLMs的高效推理提供了有效解决方案。

Abstract: With the growing computational demands of large language models (LLMs),
efficient inference has become increasingly critical for practical deployment.
Depth pruning has emerged as a promising approach for reducing the
computational costs of large language models by removing transformer layers.
However, existing methods typically rely on fixed block masks, which can lead
to suboptimal performance across different tasks and inputs. In this paper, we
propose IG-Pruning, a novel input-aware block-wise pruning method that
dynamically selects layer masks at inference time. Our approach consists of two
stages: (1) Discovering diverse mask candidates through semantic clustering and
L0 optimization, and (2) Implementing efficient dynamic pruning without the
need for extensive training. Experimental results demonstrate that our method
consistently outperforms state-of-the-art static depth pruning methods, making
it particularly suitable for resource-constrained deployment scenarios.

</details>


### [2] [Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results](https://arxiv.org/abs/2511.02246)
*Jonathan Liu,Haoling Qiu,Jonathan Lasko,Damianos Karakos,Mahsa Yarmohammadi,Mark Dredze*

Main category: cs.CL

TL;DR: 该研究开发了一个基础设施来自动生成查询并评估医疗聊天机器人在涉及人口统计信息时的表现，发现LLM评估者之间的一致性很低，建议使用多个LLM作为评估者以避免得出统计显著但不具普适性的结果。


<details>
  <summary>Details</summary>
Motivation: 医疗聊天机器人在涉及非医疗因素（如人口统计信息）时必须提供一致的建议，但现有LLM普遍存在幻觉、遗漏和偏见问题，需要了解医疗聊天机器人在哪些条件下会表现不佳。

Method: 开发了一个基础设施：1）自动生成查询来探测LLM，通过采样患者人口统计、病史、疾病和写作风格来创建现实问题；2）使用多个LLM-as-a-judge设置和提示来评估答案，包括幻觉和遗漏检测以及治疗类别检测。

Result: LLM注释者表现出低一致性（平均Cohen's Kappa κ=0.118），只有特定的（回答、评估）LLM对在不同写作风格、性别和种族之间产生统计显著差异。

Conclusion: 建议使用多个LLM作为评估者以避免得出统计显著但不具普适性的结果，特别是在缺乏真实数据的情况下，并建议发布LLM间一致性指标以提高透明度。

Abstract: Recent research has shown that hallucinations, omissions, and biases are
prevalent in everyday use-cases of LLMs. However, chatbots used in medical
contexts must provide consistent advice in situations where non-medical factors
are involved, such as when demographic information is present. In order to
understand the conditions under which medical chatbots fail to perform as
expected, we develop an infrastructure that 1) automatically generates queries
to probe LLMs and 2) evaluates answers to these queries using multiple
LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples
the space of patient demographics, histories, disorders, and writing styles to
create realistic questions that we subsequently use to prompt LLMs. In 2), our
evaluation pipeline provides hallucination and omission detection using
LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge
treatment category detectors. As a baseline study, we perform two case studies
on inter-LLM agreement and the impact of varying the answering and evaluation
LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's
Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs
yield statistically significant differences across writing styles, genders, and
races. We recommend that studies using LLM evaluation use multiple LLMs as
evaluators in order to avoid arriving at statistically significant but
non-generalizable results, particularly in the absence of ground-truth data. We
also suggest publishing inter-LLM agreement metrics for transparency. Our code
and dataset are available here:
https://github.com/BBN-E/medic-neurips-2025-demo.

</details>


### [3] [LTD-Bench: Evaluating Large Language Models by Letting Them Draw](https://arxiv.org/abs/2511.02347)
*Liuhao Lin,Ke Li,Zihan Xu,Yuchen Shi,Yulei Qin,Yan Zhang,Xing Sun,Rongrong Ji*

Main category: cs.CL

TL;DR: LTD-Bench是一个突破性的基准测试，通过要求模型通过点阵或可执行代码生成绘图，将LLM评估从抽象分数转变为可直接观察的视觉输出，揭示了大语言模型在空间推理方面的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估范式存在关键盲点——依赖不透明的数值指标，掩盖了空间推理的基本限制，同时无法提供对模型能力的直观理解。这种缺陷在需要物理世界理解的应用中造成了报告性能与实际能力之间的危险脱节。

Method: LTD-Bench采用综合方法论，包含互补的生成任务（测试空间想象力）和识别任务（评估空间感知），跨越三个渐进难度级别，系统地评估语言-空间映射的关键双向关系。

Result: 对最先进模型的广泛实验暴露了令人担忧的能力差距：即使在传统基准测试中取得令人印象深刻结果的LLM，在建立语言和空间概念之间的双向映射方面也表现出严重缺陷——这一基本限制削弱了它们作为真正世界模型的潜力。

Conclusion: LTD-Bench的视觉输出支持强大的诊断分析，为研究模型相似性提供了潜在方法，揭示了当前LLM在空间推理方面的根本局限性。

Abstract: Current evaluation paradigms for large language models (LLMs) represent a
critical blind spot in AI research--relying on opaque numerical metrics that
conceal fundamental limitations in spatial reasoning while providing no
intuitive understanding of model capabilities. This deficiency creates a
dangerous disconnect between reported performance and practical abilities,
particularly for applications requiring physical world understanding. We
introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation
from abstract scores to directly observable visual outputs by requiring models
to generate drawings through dot matrices or executable code. This approach
makes spatial reasoning limitations immediately apparent even to non-experts,
bridging the fundamental gap between statistical performance and intuitive
assessment. LTD-Bench implements a comprehensive methodology with complementary
generation tasks (testing spatial imagination) and recognition tasks (assessing
spatial perception) across three progressively challenging difficulty levels,
methodically evaluating both directions of the critical language-spatial
mapping. Our extensive experiments with state-of-the-art models expose an
alarming capability gap: even LLMs achieving impressive results on traditional
benchmarks demonstrate profound deficiencies in establishing bidirectional
mappings between language and spatial concept--a fundamental limitation that
undermines their potential as genuine world models. Furthermore, LTD-Bench's
visual outputs enable powerful diagnostic analysis, offering a potential
approach to investigate model similarity.

</details>


### [4] [Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation](https://arxiv.org/abs/2511.02358)
*Wongyu Kim,Hochang Lee,Sanghak Lee,Yoonsung Kim,Jaehyun Park*

Main category: cs.CL

TL;DR: M-Solomon是一个多模态嵌入器，通过自适应查询增强来解决传统LLM嵌入器中每个查询都进行增强导致的延迟问题和性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的嵌入器对所有查询都进行增强，导致显著的嵌入延迟，且某些查询的增强反而会损害性能。现有方法也未探索多模态环境。

Method: 首先将训练数据集中的查询分为需要增强和不需要增强两组；然后利用强大的多模态LLM为需要增强的查询生成合适的增强内容；最后通过自适应查询增强，仅在必要时进行增强。

Result: 实验结果显示，M-Solomon不仅大幅超越无增强的基线，也优于总是使用增强的基线，同时提供更快的嵌入延迟。

Conclusion: M-Solomon通过自适应查询增强策略，在多模态环境中实现了更好的检索性能和更低的延迟。

Abstract: Query augmentation makes queries more meaningful by appending further
information to the queries to find relevant documents. Current studies have
proposed Large Language Model (LLM)-based embedders, which learn representation
for embedding and generation for query augmentation in a multi-task manner by
leveraging the generative capabilities of LLM. During inference, these jointly
trained embedders have conducted query augmentation followed by embedding,
showing effective results. However, augmenting every query leads to substantial
embedding latency and query augmentation can be detrimental to performance for
some queries. Also, previous methods have not been explored in multimodal
environments. To tackle these problems, we propose M-Solomon, a universal
multimodal embedder that can adaptively determine when to augment queries. Our
approach first divides the queries of the training datasets into two groups at
the dataset level. One includes queries that require augmentation and the other
includes queries that do not. Then, we introduces a synthesis process that
generates appropriate augmentations for queries that require them by leveraging
a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.
Through this step, M-Solomon can conduct query augmentation only when necessary
by learning to generate synthetic augmentations with the prefix /augment for
queries that demand them and to generate the simple string /embed for others.
Experimental results showed that M-Solomon not only surpassed the baseline
without augmentation by a large margin but also outperformed the baseline that
always used augmentation, providing much faster embedding latency.

</details>


### [5] [AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda](https://arxiv.org/abs/2511.02374)
*Mohd Nauman,Sravan Gvm,Vijay Devane,Shyam Pawar,Viraj Thakur,Kundeshwar Pundalik,Piyush Sawarkar,Rohit Saluja,Maunendra Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: AyurParam-2.9B是一个专门针对阿育吠陀医学领域的双语语言模型，通过专家精心策划的数据集进行微调，在专业医学知识方面超越了同规模开源模型，甚至能与更大模型竞争。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在通用任务上表现出色，但在需要深厚文化、语言和专业知识的高度专业化领域（如阿育吠陀医学）表现不佳，无法准确解释和应用这些传统医学系统的复杂知识。

Method: 从Param-1-2.9B模型微调开发AyurParam-2.9B，使用包含古典文本和临床指导的广泛阿育吠陀数据集，该数据集包含上下文感知、推理和客观风格的双语问答，并采用严格标注协议确保事实准确性和教学清晰度。

Result: 在BhashaBench-Ayur基准测试中，AyurParam不仅超越了其规模类别（1.5-3B参数）的所有开源指令调优模型，还展现出与更大模型竞争甚至更优的性能。

Conclusion: AyurParam的结果强调了在提供可靠、文化一致的AI用于专业医学知识时，真实领域适应和高质量监督的必要性。

Abstract: Current large language models excel at broad, general-purpose tasks, but
consistently underperform when exposed to highly specialized domains that
require deep cultural, linguistic, and subject-matter expertise. In particular,
traditional medical systems such as Ayurveda embody centuries of nuanced
textual and clinical knowledge that mainstream LLMs fail to accurately
interpret or apply. We introduce AyurParam-2.9B, a domain-specialized,
bilingual language model fine-tuned from Param-1-2.9B using an extensive,
expertly curated Ayurveda dataset spanning classical texts and clinical
guidance. AyurParam's dataset incorporates context-aware, reasoning, and
objective-style Q&A in both English and Hindi, with rigorous annotation
protocols for factual precision and instructional clarity. Benchmarked on
BhashaBench-Ayur, AyurParam not only surpasses all open-source
instruction-tuned models in its size class (1.5--3B parameters), but also
demonstrates competitive or superior performance compared to much larger
models. The results from AyurParam highlight the necessity for authentic domain
adaptation and high-quality supervision in delivering reliable, culturally
congruent AI for specialized medical knowledge.

</details>


### [6] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2511.02376)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CL

TL;DR: AutoAdv是一个无需训练的多轮越狱攻击框架，在Llama-3.1-8B上达到95%攻击成功率，比单轮攻击提升24%，揭示了当前安全机制在多轮对话中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全评估主要关注单轮交互，但现实攻击往往通过多轮自适应对话展开，需要研究多轮越狱攻击的有效性和防御需求。

Method: 结合三种自适应机制：模式管理器从成功攻击中学习增强提示，温度管理器基于失败模式动态调整采样参数，两阶段重写策略先伪装有害请求再迭代优化。

Result: 在商业和开源模型（GPT-4o-mini、Qwen3-235B、Mistral-7B）上广泛评估，多轮攻击始终优于单轮方法，显示当前安全机制存在持续漏洞。

Conclusion: 针对单轮交互优化的对齐策略无法在扩展对话中保持鲁棒性，迫切需要开发多轮感知的防御机制。

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where
adversarial prompts elicit harmful outputs, yet most evaluations focus on
single-turn interactions while real-world attacks unfold through adaptive
multi-turn conversations. We present AutoAdv, a training-free framework for
automated multi-turn jailbreaking that achieves up to 95% attack success rate
on Llama-3.1-8B within six turns a 24 percent improvement over single turn
baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern
manager that learns from successful attacks to enhance future prompts, a
temperature manager that dynamically adjusts sampling parameters based on
failure modes, and a two-phase rewriting strategy that disguises harmful
requests then iteratively refines them. Extensive evaluation across commercial
and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent
vulnerabilities in current safety mechanisms, with multi-turn attacks
consistently outperforming single-turn approaches. These findings demonstrate
that alignment strategies optimized for single-turn interactions fail to
maintain robustness across extended conversations, highlighting an urgent need
for multi-turn-aware defenses.

</details>


### [7] [Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance](https://arxiv.org/abs/2511.02451)
*Kentaro Ueda,François Portet,Hirohiko Suwa,Keiichi Yasumoto*

Main category: cs.CL

TL;DR: 该论文研究了在金融等专业领域中，通过合并领域特定的持续预训练专家模型来构建多技能大语言模型的方法，填补了CPT模型合并的研究空白。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在专业领域表现不佳，需要融合领域知识、数学推理和多语言处理等多样化技能。直接进行多技能训练成本高昂且不稳定，因此探索通过合并现有专家模型来构建多技能模型。

Method: 创建金融、数学和日语领域的专家模型，提出三阶段评估框架（知识恢复、互补性和涌现性），并评估三种合并方法（Task Arithmetic、TIES和DARE-TIES）在包含18个任务、8个数据集的综合金融基准上的表现。

Result: 合并专家与其基础模型可以恢复CPT期间丢失的通用知识；合并多个专家能提升性能并产生跨领域涌现技能；Task Arithmetic表现强劲但对超参数敏感，TIES更稳健；模型相似性与合并成功相关，但涌现技能依赖更复杂因素。

Conclusion: 这是首次对CPT模型合并进行的基础性分析，建立了原则性框架，为从现有资产构建多技能大语言模型提供了明确指导。

Abstract: While LLMs excel at general tasks, they struggle in specialized domains like
finance, requiring diverse skills in domain knowledge, mathematical reasoning,
and multilingual processing. Merging domain-specific Continual Pre-training
(CPT) "experts" offers a practical alternative to costly and unstable
multi-skill training. However, unlike established Supervised Fine-Tuning (SFT)
model-based merging, CPT model merging remains largely unexplored. We address
this gap by creating financial LLMs from experts in finance, math, and
Japanese. We propose a three-stage evaluation focusing on knowledge recovery,
complementarity, and emergence, and assess three merging methods (Task
Arithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated
from 18 tasks across 8 established datasets. Results show that merging an
expert with its base model recovers general knowledge lost during CPT, while
merging experts improves performance and can yield emergent cross-domain
skills. Among the methods, Task Arithmetic performs strongly but is
hyperparameter-sensitive, whereas TIES is more robust. Our findings also
suggest that while model similarity correlates with merging success, emergent
skills depend on more complex factors. This work presents the first
foundational analysis of CPT model merging, establishing a principled framework
and providing clear guidance for building multi-skill LLMs from existing
assets.

</details>


### [8] [Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas](https://arxiv.org/abs/2511.02458)
*Giulia Iadisernia,Carolina Camassa*

Main category: cs.CL

TL;DR: 评估基于角色的提示是否能提升大语言模型在宏观经济预测任务中的表现，使用2368个经济学相关角色对GPT-4o进行测试，发现角色提示对预测准确性没有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究角色提示方法是否能够提高大语言模型在复杂宏观经济预测任务中的性能表现。

Method: 使用PersonaHub语料库中的2368个经济学相关角色，让GPT-4o模拟ECB专业预测者调查，覆盖50个季度（2013-2025），比较角色提示预测与人类专家预测的差异。

Result: GPT-4o与人类预测者达到非常相似的准确度水平，差异在统计上显著但实际影响不大；角色描述对预测准确性没有可测量的优势。

Conclusion: GPT-4o在提供相关背景数据的情况下，能够在样本外宏观经济事件上保持有竞争力的预测准确性，而多样化的提示产生的预测结果与人类专家小组相比表现出显著的同质性。

Abstract: We evaluate whether persona-based prompting improves Large Language Model
(LLM) performance on macroeconomic forecasting tasks. Using 2,368
economics-related personas from the PersonaHub corpus, we prompt GPT-4o to
replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds
(2013-2025). We compare the persona-prompted forecasts against the human
experts panel, across four target variables (HICP, core HICP, GDP growth,
unemployment) and four forecast horizons. We also compare the results against
100 baseline forecasts without persona descriptions to isolate its effect. We
report two main findings. Firstly, GPT-4o and human forecasters achieve
remarkably similar accuracy levels, with differences that are statistically
significant yet practically modest. Our out-of-sample evaluation on 2024-2025
data demonstrates that GPT-4o can maintain competitive forecasting performance
on unseen events, though with notable differences compared to the in-sample
period. Secondly, our ablation experiment reveals no measurable forecasting
advantage from persona descriptions, suggesting these prompt components can be
omitted to reduce computational costs without sacrificing accuracy. Our results
provide evidence that GPT-4o can achieve competitive forecasting accuracy even
on out-of-sample macroeconomic events, if provided with relevant context data,
while revealing that diverse prompts produce remarkably homogeneous forecasts
compared to human panels.

</details>


### [9] [The Analysis of Lexical Errors in Machine Translation from English into Romanian](https://arxiv.org/abs/2511.02587)
*Angela Stamatie*

Main category: cs.CL

TL;DR: 该研究分析谷歌翻译在将WHO、Gavi组织和药品说明书等新冠疫情相关英文文本翻译成罗马尼亚语时产生的词汇错误，旨在改进机器翻译的词汇选择质量。


<details>
  <summary>Details</summary>
Motivation: 改进谷歌翻译的准确性，通过分析新冠疫情相关官方信息的翻译错误来提升机器翻译的整体质量。

Method: 对230篇从英语翻译成罗马尼亚语的文本进行全面的词汇错误分析，重点关注官方信息和药品说明书等专业内容。

Result: 研究发现机器翻译在专业术语和特定语境下的词汇选择存在显著问题，识别了具体的错误类型和模式。

Conclusion: 通过系统分析词汇错误，可以为改进机器翻译系统提供具体方向，特别是在专业领域翻译的词汇准确性方面。

Abstract: The research explores error analysis in the performance of translating by
Machine Translation from English into Romanian, and it focuses on lexical
errors found in texts which include official information, provided by the World
Health Organization (WHO), the Gavi Organization, by the patient information
leaflet (the information about the active ingredients of the vaccines or the
medication, the indications, the dosage instructions, the storage instructions,
the side effects and warning, etc.). All of these texts are related to Covid-19
and have been translated by Google Translate, a multilingual Machine
Translation that was created by Google. In the last decades, Google has
actively worked to develop a more accurate and fluent automatic translation
system. This research, specifically focused on improving Google Translate, aims
to enhance the overall quality of Machine Translation by achieving better
lexical selection and by reducing errors. The investigation involves a
comprehensive analysis of 230 texts that have been translated from English into
Romanian.

</details>


### [10] [Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour](https://arxiv.org/abs/2511.02599)
*Max Norris,Kobi Gal,Sahan Bulathwela*

Main category: cs.CL

TL;DR: NTKT将知识追踪重新定义为使用预训练大语言模型的下一词预测任务，通过将学生历史和问题内容表示为文本序列，显著提升了预测性能和在冷启动场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型通常忽略问题文本这一重要的教学洞察来源，限制了预测性能。

Method: 提出NTKT方法，将知识追踪重构为使用预训练大语言模型的下一词预测任务，将学生历史和问题内容表示为文本序列。

Result: 实验显著优于最先进的神经知识追踪模型，在冷启动问题和用户场景下泛化能力更强。

Conclusion: 研究强调了问题内容在知识追踪中的重要性，并展示了利用预训练大语言模型表示更有效建模学生学习的优势。

Abstract: Modelling student knowledge is a key challenge when leveraging AI in
education, with major implications for personalised learning. The Knowledge
Tracing (KT) task aims to predict how students will respond to educational
questions in learning environments, based on their prior interactions. Existing
KT models typically use response correctness along with metadata like skill
tags and timestamps, often overlooking the question text, which is an important
source of pedagogical insight. This omission poses a lost opportunity while
limiting predictive performance. We propose Next Token Knowledge Tracing
(NTKT), a novel approach that reframes KT as a next-token prediction task using
pretrained Large Language Models (LLMs). NTKT represents both student histories
and question content as sequences of text, allowing LLMs to learn patterns in
both behaviour and language. Our series of experiments significantly improves
performance over state-of-the-art neural KT models and generalises much better
to cold-start questions and users. These findings highlight the importance of
question content in KT and demonstrate the benefits of leveraging pretrained
representations of LLMs to model student learning more effectively.

</details>


### [11] [CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency](https://arxiv.org/abs/2511.02603)
*Ehsan Aghazadeh,Ahmad Ghasemi,Hedyeh Beyhaghi,Hossein Pishro-Nik*

Main category: cs.CL

TL;DR: CGES是一种贝叶斯框架，通过置信度信号自适应停止采样，在保持准确性的同时显著减少模型调用次数。


<details>
  <summary>Details</summary>
Motivation: 解决自一致性策略需要固定调用次数且在正确答案罕见时可能失败的问题。

Method: 使用从token概率或奖励模型推导的标量置信度信号构建候选答案的后验分布，当候选答案的后验质量超过阈值时自适应停止采样。

Result: 在五个推理基准测试中，CGES将平均模型调用次数减少约69%（例如从16.0次降至4.9次），同时准确性与自一致性方法相差不超过0.06个百分点。

Conclusion: CGES提供了一种高效的自适应采样停止方法，在保持准确性的同时大幅减少计算成本。

Abstract: Large language models (LLMs) are often queried multiple times at test time,
with predictions aggregated by majority vote. While effective, this
self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls
and can fail when the correct answer is rare. We introduce Confidence-Guided
Early Stopping (CGES), a Bayesian framework that forms posteriors over
candidate answers using scalar confidence signals derived from token
probabilities or reward models. CGES adaptively halts sampling once the
posterior mass of a candidate exceeds a threshold. We provide theoretical
guarantees for both perfectly calibrated confidences and realistic noisy
confidence signals. Across five reasoning benchmarks, CGES reduces the average
number of model calls by about 69 percent (for example, from 16.0 to 4.9) while
matching the accuracy of self-consistency within 0.06 percentage points.

</details>


### [12] [The Realignment Problem: When Right becomes Wrong in LLMs](https://arxiv.org/abs/2511.02623)
*Aakash Sen Sharma,Debdeep Sanyal,Vivek Srivastava,Shirish Karande,Murari Mandal*

Main category: cs.CL

TL;DR: TRACE框架通过程序化策略应用解决大语言模型与现实政策之间的对齐差距，实现精确的遗忘学习，在不损害模型性能的前提下更新对齐策略。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的对齐实践产生静态、脆弱且维护成本高的模型，无法跟上不断演变的规范和政策，存在对齐-现实差距问题。现有解决方案（大规模重新标注和标准遗忘方法）在经济上不可行或会损害模型效用。

Method: TRACE框架将重新对齐视为程序化策略应用问题，通过程序化筛选现有偏好数据与新政策的冲突，使用对齐影响评分识别高影响冲突，并应用混合优化来干净地反转、丢弃或保留偏好，同时保护模型性能。

Result: 在多种模型家族（Qwen2.5-7B、Gemma-2-9B、Llama-3.1-8B）上的实证结果表明，TRACE在合成基准和PKU-SafeRLHF数据集上的复杂策略转变下都能实现稳健的重新对齐，强制执行新原则而不降低一般能力。

Conclusion: TRACE为维护LLM对齐提供了一个可扩展、动态且经济高效的范式，为可持续和负责任的AI部署奠定了基础。

Abstract: The alignment of Large Language Models (LLMs) with human values is central to
their safe deployment, yet current practice produces static, brittle, and
costly-to-maintain models that fail to keep pace with evolving norms and
policies. This misalignment, which we term the Alignment-Reality Gap, poses a
growing challenge for reliable long-term use. Existing remedies are inadequate:
large-scale re-annotation is economically prohibitive, and standard unlearning
methods act as blunt instruments that erode utility rather than enable precise
policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict
Evaluation), a framework for principled unlearning that reconceives
re-alignment as a programmatic policy application problem. TRACE
programmatically triages existing preference data against a new policy,
identifies high-impact conflicts via a alignment impact score, and applies a
hybrid optimization that cleanly inverts, discards, or preserves preferences
while safeguarding model performance. Empirical results show that TRACE
achieves robust re-alignment across diverse model families (Qwen2.5-7B,
Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF
dataset under complex policy shift, TRACE enforces new principles without
degrading general capabilities. Our work establishes a scalable, dynamic, and
cost-effective paradigm for maintaining LLM alignment, providing a foundation
for sustainable and responsible AI deployment.

</details>


### [13] [Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation](https://arxiv.org/abs/2511.02626)
*Renfei Dang,Peng Hu,Changjiang Gao,Shujian Huang*

Main category: cs.CL

TL;DR: 本文研究发现，在LLMs微调中引入新知识会导致已知信息测试时产生事实幻觉，特定知识类型完全由新知识组成时幻觉倾向显著增加。作者提出KnownPatch方法，通过在训练后期添加少量已知知识样本来缓解这种幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有研究未深入探讨新知识微调导致的事实幻觉的具体表现和机制，本文旨在填补这一空白。

Method: 设计受控数据集Biography-Reasoning，在多种知识类型和两种任务类型（知识问答和知识推理）上进行细粒度分析，并提出KnownPatch方法在训练后期添加已知知识样本。

Result: 发现特定知识类型完全由新知识组成时，LLMs的幻觉倾向显著增加；新知识学习会降低模型对问题中关键实体的注意力，导致过度关注周围上下文；KnownPatch方法有效缓解了幻觉并提升了性能。

Conclusion: 特定知识类型的高陌生度是幻觉的主要驱动因素，KnownPatch通过修复模型对关键实体的注意力模式有效缓解了新知识诱导的幻觉问题。

Abstract: Previous studies show that introducing new knowledge during large language
models (LLMs) fine-tuning can lead to the generation of erroneous output when
tested on known information, thereby triggering factual hallucinations.
However, existing studies have not deeply investigated the specific
manifestations and underlying mechanisms of these hallucinations. Our work
addresses this gap by designing a controlled dataset Biography-Reasoning, and
conducting a fine-grained analysis across multiple knowledge types and two task
types, including knowledge question answering (QA) and knowledge reasoning
tasks. We find that when fine-tuned on a dataset in which a specific knowledge
type consists entirely of new knowledge, LLMs exhibit significantly increased
hallucination tendencies. This suggests that the high unfamiliarity of a
particular knowledge type, rather than the overall proportion of new knowledge,
is a stronger driver of hallucinations, and these tendencies can even affect
other knowledge types in QA tasks. To mitigate such factual hallucinations, we
propose KnownPatch, which patches a small number of known knowledge samples in
the later stages of training, effectively alleviating new-knowledge-induced
hallucinations. Through attention analysis, we find that learning new knowledge
reduces the model's attention to key entities in the question, thus causing
excessive focus on the surrounding context, which may increase the risk of
hallucination. Moreover, the attention pattern can propagate to similar
contexts, facilitating the spread of hallucinations to textually similar
questions. Our method effectively mitigates the disruption of new knowledge
learning to the model's attention on key entities, accompanied by improved
performance.

</details>


### [14] [Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes](https://arxiv.org/abs/2511.02681)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.CL

TL;DR: 本文提出了一种名为最优奇异值损伤的方法，用于高效存储预训练语言模型微调后的参数更新，通过结合低秩近似和稀疏化技术，在相同内存预算下实现更好的存储效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调后存储成本高，研究发现微调主要影响少量参数，需要更高效的存储方案。微调更新具有低秩和稀疏特性，但单独使用低秩近似或稀疏化可能丢失关键信息。

Method: 提出最优奇异值损伤方法，利用奇异向量的交错重要性，选择性地对低秩近似更新进行稀疏化，保留最有影响力的组件。

Result: 实验表明，在相同内存预算下，该方法比单独使用低秩近似或稀疏化具有显著更高的存储效率和准确性。

Conclusion: 结合低秩近似和稀疏化的方法能够更有效地存储微调模型参数更新，在有限内存下保持模型性能。

Abstract: Large language models (LLMs) are increasingly prevalent across diverse
applications. However, their enormous size limits storage and processing
capabilities to a few well-resourced stakeholders. As a result, most
applications rely on pre-trained LLMs, fine-tuned for specific tasks. However,
even storing the fine-tuned versions of these models remains a significant
challenge due to the wide range of tasks they address. Recently, studies show
that fine-tuning these models primarily affects a small fraction of parameters,
highlighting the need for more efficient storage of fine-tuned models. This
paper focuses on efficient storage of parameter updates in pre-trained models
after fine-tuning. To address this challenge, we leverage the observation that
fine-tuning updates are both low-rank and sparse, which can be utilized for
storage efficiency. However, using only low-rank approximation or
sparsification may discard critical singular components that enhance model
expressivity. We first observe that given the same memory budget, sparsified
low-rank approximations with larger ranks outperform standard low-rank
approximations with smaller ranks. Building on this, we propose our method,
optimal singular damage, that selectively sparsifies low-rank approximated
updates by leveraging the interleaved importance of singular vectors, ensuring
that the most impactful components are retained. We demonstrate through
extensive experiments that our proposed methods lead to significant storage
efficiency and superior accuracy within the same memory budget compared to
employing the low-rank approximation or sparsification individually.

</details>


### [15] [PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation](https://arxiv.org/abs/2511.02721)
*Doreen Osmelak,Koel Dutta Chowdhury,Uliana Sentsova,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: PragExTra是首个用于语用显化的多语言语料库和检测框架，涵盖8种语言对，通过空对齐和主动学习识别语用显化现象，如实体描述和测量转换等，准确率最高达0.88。


<details>
  <summary>Details</summary>
Motivation: 翻译理论中广泛讨论的语用显化现象（译者添加背景信息使隐含文化意义显化）很少被计算建模，需要建立可测量的跨语言检测框架。

Method: 构建PragExTra多语言语料库，覆盖TED-Multi和Europarl的8种语言对；通过空对齐识别候选显化案例，使用主动学习和人工标注进行精炼；开发分类器检测语用显化。

Result: 实体和系统级显化最为频繁；主动学习使分类器准确率提高7-8个百分点，跨语言准确率最高达0.88，F1分数0.82。

Conclusion: PragExTra将语用显化确立为可测量的跨语言现象，为构建文化感知的机器翻译迈出重要一步。

Abstract: Translators often enrich texts with background details that make implicit
cultural meanings explicit for new audiences. This phenomenon, known as
pragmatic explicitation, has been widely discussed in translation theory but
rarely modeled computationally. We introduce PragExTra, the first multilingual
corpus and detection framework for pragmatic explicitation. The corpus covers
eight language pairs from TED-Multi and Europarl and includes additions such as
entity descriptions, measurement conversions, and translator remarks. We
identify candidate explicitation cases through null alignments and refined
using active learning with human annotation. Our results show that entity and
system-level explicitations are most frequent, and that active learning
improves classifier accuracy by 7-8 percentage points, achieving up to 0.88
accuracy and 0.82 F1 across languages. PragExTra establishes pragmatic
explicitation as a measurable, cross-linguistic phenomenon and takes a step
towards building culturally aware machine translation. Keywords: translation,
multilingualism, explicitation

</details>


### [16] [AI Diffusion in Low Resource Language Countries](https://arxiv.org/abs/2511.02752)
*Amit Misra,Syed Waqas Zamir,Wassim Hamidouche,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

TL;DR: 研究发现低资源语言国家因AI在本地语言上表现不佳，导致AI用户比例比基准水平低约20%，表明语言可访问性是AI公平扩散的重要障碍。


<details>
  <summary>Details</summary>
Motivation: 人工智能在全球快速扩散但采用不均衡，前沿大语言模型在低资源语言上表现不佳，作者假设这种性能缺陷降低了AI的效用，从而减缓了低资源语言国家的采用速度。

Method: 使用加权回归模型从社会经济和人口因素中分离出语言效应。

Result: 低资源语言国家的AI用户比例相对于基准水平低约20%。

Conclusion: 语言可访问性是AI公平扩散的重要独立障碍。

Abstract: Artificial intelligence (AI) is diffusing globally at unprecedented speed,
but adoption remains uneven. Frontier Large Language Models (LLMs) are known to
perform poorly on low-resource languages due to data scarcity. We hypothesize
that this performance deficit reduces the utility of AI, thereby slowing
adoption in Low-Resource Language Countries (LRLCs). To test this, we use a
weighted regression model to isolate the language effect from socioeconomic and
demographic factors, finding that LRLCs have a share of AI users that is
approximately 20% lower relative to their baseline. These results indicate that
linguistic accessibility is a significant, independent barrier to equitable AI
diffusion.

</details>


### [17] [Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning](https://arxiv.org/abs/2511.02755)
*Bowen Jin,TJ Collins,Donghan Yu,Mert Cemri,Shenao Zhang,Mengyu Li,Jay Tang,Tian Qin,Zhiyang Xu,Jiarui Lu,Guoli Yin,Jiawei Han,Zirui Wang*

Main category: cs.CL

TL;DR: 提出了CoRL框架，通过强化学习优化多LLM系统的性能与成本权衡，实现集中式控制器选择性调用专家模型，在不同预算条件下保持高效性能。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化多LLM系统对每个输入都调用多个模型，导致推理成本高昂且不可控，需要设计成本高效且可控的多智能体LLM系统。

Method: 采用集中式多LLM框架，通过控制器LLM选择性协调专家模型池，使用强化学习框架优化性能与成本的权衡，支持多预算条件下的自适应行为。

Result: 在四个不同基准测试中，CoRL在高预算设置下超越最佳专家LLM，在低预算模式下仍保持强劲性能。

Conclusion: 集中式协调为可扩展且成本高效的多智能体LLM系统提供了有效解决方案，实现了性能与成本的良好平衡。

Abstract: Large language models (LLMs) exhibit complementary strengths across domains
and come with varying inference costs, motivating the design of multi-agent LLM
systems where specialized models collaborate efficiently. Existing approaches
predominantly rely on decentralized frameworks, which invoke multiple LLMs for
every input and thus lead to substantial and uncontrolled inference costs. In
this work, we introduce a centralized multi-LLM framework, where a controller
LLM selectively coordinates a pool of expert models in a cost-efficient and
cost-controllable manner. We formulate this coordination problem as
reinforcement learning with dual objectives: maximizing task performance while
minimizing the overall inference cost. In addition, we expect the multi-agent
system to have adapted behavior with different budget conditions during
inference. To this end, we propose CoRL, a reinforcement learning framework
that optimizes the performance cost trade-off in a controllable multi-budget
setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a
single system to surpass the best expert LLM under high-budget settings, while
maintaining strong performance in more economical low-budget modes,
highlighting the effectiveness of centralized coordination for scalable and
cost-efficient multi-agent LLM systems.

</details>


### [18] [Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval](https://arxiv.org/abs/2511.02770)
*Hung-Ting Chen,Xiang Liu,Shauli Ravfogel,Eunsol Choi*

Main category: cs.CL

TL;DR: 本文提出了一种新的检索器架构AMER，通过自回归生成多个查询向量来解决传统单一向量检索器在处理多模态相关文档分布时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统检索器只生成一个查询向量，但查询的相关文档分布可能是多模态的（如不同解释）。现有检索器在目标文档嵌入距离较大时表现不佳。

Method: 开发了自回归多嵌入检索器(AMER)，自回归生成多个查询向量，所有预测的查询向量都用于从语料库中检索文档。

Result: 在合成向量化数据上，该方法能完美捕捉多个目标分布，性能比单嵌入模型提高4倍。在真实世界多答案检索数据集上，相比单嵌入基线分别获得4%和21%的相对增益，在目标文档嵌入相似度较低的子集上增益更大。

Conclusion: 多查询向量检索器具有巨大潜力，为未来研究开辟了新方向。

Abstract: Most text retrievers generate \emph{one} query vector to retrieve relevant
documents. Yet, the conditional distribution of relevant documents for the
query may be multimodal, e.g., representing different interpretations of the
query. We first quantify the limitations of existing retrievers. All retrievers
we evaluate struggle more as the distance between target document embeddings
grows. To address this limitation, we develop a new retriever architecture,
\emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER).
Our model autoregressively generates multiple query vectors, and all the
predicted query vectors are used to retrieve documents from the corpus. We show
that on the synthetic vectorized data, the proposed method could capture
multiple target distributions perfectly, showing 4x better performance than
single embedding model. We also fine-tune our model on real-world multi-answer
retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative
gains over single-embedding baselines on two datasets we evaluate on.
Furthermore, we consistently observe larger gains on the subset of dataset
where the embeddings of the target documents are less similar to each other. We
demonstrate the potential of using a multi-query vector retriever and open up a
new direction for future work.

</details>


### [19] [Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities](https://arxiv.org/abs/2511.02817)
*Amanda Bertsch,Adithya Pratapa,Teruko Mitamura,Graham Neubig,Matthew R. Gormley*

Main category: cs.CL

TL;DR: Oolong是一个长上下文推理基准测试，包含合成任务和真实世界对话任务，要求模型分析文本块并聚合分析结果来回答分布性问题，当前前沿模型在128K上下文长度下准确率低于50%。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评估主要依赖检索任务，允许模型忽略大部分上下文作为噪声，这仅代表一种任务类型。需要评估模型在需要分析每个文本块并聚合推理的任务上的表现。

Method: Oolong分为两个任务集：Oolong-synth（自然合成任务，可轻松消融推理问题组件）和Oolong-real（需要推理真实世界对话数据）。任务要求模型推理大量示例，在上下文中进行分类和计数，并处理时间和用户关系。

Result: 即使是前沿模型在Oolong上也表现不佳，GPT-5、Claude-Sonnet-4和Gemini-2.5-Pro在128K上下文长度下在两个任务集上的准确率都低于50%。

Conclusion: 当前模型在需要分析大量文本并进行聚合推理的长上下文任务上仍面临挑战，发布Oolong数据和评估框架以促进能够推理大量文本的模型发展。

Abstract: As model context lengths continue to grow, concerns about whether models
effectively use the full context length have persisted. While several carefully
designed long-context evaluations have recently been released, these
evaluations tend to rely on retrieval from one or more sections of the context,
which allows nearly all of the context tokens to be disregarded as noise. This
represents only one type of task that might be performed with long context. We
introduce Oolong, a benchmark of long-context reasoning tasks that require
analyzing individual chunks of text on an atomic level, and then aggregating
these analyses to answer distributional questions. Oolong is separated into two
task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can
easily ablate components of the reasoning problem; and Oolong-real, a
downstream setting which requires reasoning over real-world conversational
data. Oolong requires models to reason over large quantities of examples, to
perform both classification and counting in-context, and to reason over
temporal and user relations. Even frontier models struggle on Oolong, with
GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy
on both splits at 128K. We release the data and evaluation harness for Oolong
to enable further development of models that can reason over large quantities
of text.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing](https://arxiv.org/abs/2511.02071)
*Xinyi Lin,Yuyang Zhang,Yuanhang Gan,Juntao Chen,Hao Shen,Yichun He,Lijun Li,Ze Yuan,Shuang Wang,Chaohao Wang,Rui Zhang,Na Li,Jia Liu*

Main category: cs.AI

TL;DR: 该论文提出了一种人类-AI共体现智能的新范式，将人类用户、智能AI和可穿戴硬件集成到一个系统中，用于现实世界的实验和智能制造。通过APEX系统演示，在柔性电子制造中实现了上下文感知推理、实时纠错和专业知识转移。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型局限于虚拟领域，而现实世界的实验和制造仍依赖人类监督和专业知识。机器智能与物理执行之间的差距限制了科学和制造工作流程的可重复性、可扩展性和可访问性。

Method: 引入人类-AI共体现智能，结合人类精确执行、智能AI的记忆和推理能力，以及可穿戴硬件接口。开发了APEX系统，通过混合现实将智能推理与物理执行耦合，观察和解释人类动作，提供3D视觉指导。

Result: 在洁净室柔性电子制造中实施APEX系统，实现了超过通用多模态大语言模型的上下文感知推理准确性，实时纠正错误，并将专业知识转移给初学者。

Conclusion: 建立了一类新的智能-物理-人类智能，将智能推理从计算领域扩展到物理领域，将科学研究和制造转变为自主、可追溯、可解释和可扩展的过程。

Abstract: Scientific experiment and manufacture rely on complex, multi-step procedures
that demand continuous human expertise for precise execution and
decision-making. Despite advances in machine learning and automation,
conventional models remain confined to virtual domains, while real-world
experiment and manufacture still rely on human supervision and expertise. This
gap between machine intelligence and physical execution limits reproducibility,
scalability, and accessibility across scientific and manufacture workflows.
Here, we introduce human-AI co-embodied intelligence, a new form of physical AI
that unites human users, agentic AI, and wearable hardware into an integrated
system for real-world experiment and intelligent manufacture. In this paradigm,
humans provide precise execution and control, while agentic AI contributes
memory, contextual reasoning, adaptive planning, and real-time feedback. The
wearable interface continuously captures the experimental and manufacture
processes, facilitates seamless communication between humans and AI for
corrective guidance and interpretable collaboration. As a demonstration, we
present Agentic-Physical Experimentation (APEX) system, coupling agentic
reasoning with physical execution through mixed-reality. APEX observes and
interprets human actions, aligns them with standard operating procedures,
provides 3D visual guidance, and analyzes every step. Implemented in a
cleanroom for flexible electronics fabrication, APEX system achieves
context-aware reasoning with accuracy exceeding general multimodal large
language models, corrects errors in real time, and transfers expertise to
beginners. These results establish a new class of agentic-physical-human
intelligence that extends agentic reasoning beyond computation into the
physical domain, transforming scientific research and manufacturing into
autonomous, traceable, interpretable, and scalable processes.

</details>


### [21] [Automated Reward Design for Gran Turismo](https://arxiv.org/abs/2511.02094)
*Michel Ma,Takuma Seno,Kaushik Subramanian,Peter R. Wurman,Peter Stone,Craig Sherstan*

Main category: cs.AI

TL;DR: 该论文展示了如何使用基础模型通过文本指令自动搜索奖励函数，为Gran Turismo 7赛车游戏生成理想的强化学习智能体，结合LLM奖励生成、VLM偏好评估和人类反馈，能够产生与冠军级智能体GT Sophy竞争的表现。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境（如自动驾驶赛车）中，将期望行为映射到奖励函数是一个困难的过程，传统方法需要大量人工设计。

Method: 使用LLM生成奖励函数，VLM进行偏好评估，并结合人类反馈，自动搜索奖励函数空间。

Result: 系统能够生成与冠军级RL赛车智能体GT Sophy竞争的赛车智能体，并能产生新颖行为。

Conclusion: 该方法为实际应用中的自动化奖励设计铺平了道路。

Abstract: When designing reinforcement learning (RL) agents, a designer communicates
the desired agent behavior through the definition of reward functions -
numerical feedback given to the agent as reward or punishment for its actions.
However, mapping desired behaviors to reward functions can be a difficult
process, especially in complex environments such as autonomous racing. In this
paper, we demonstrate how current foundation models can effectively search over
a space of reward functions to produce desirable RL agents for the Gran Turismo
7 racing game, given only text-based instructions. Through a combination of
LLM-based reward generation, VLM preference-based evaluation, and human
feedback we demonstrate how our system can be used to produce racing agents
competitive with GT Sophy, a champion-level RL racing agent, as well as
generate novel behaviors, paving the way for practical automated reward design
in real world applications.

</details>


### [22] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: Deep Value Benchmark (DVB) 是一个评估框架，用于测试大型语言模型是否学习基本人类价值观还是仅学习表面偏好。通过控制深层价值观和浅层特征的混淆，测量模型的深层价值观泛化率(DVGR)。


<details>
  <summary>Details</summary>
Motivation: 区分AI系统是否学习基本人类价值观或仅表面偏好对AI对齐至关重要。学习深层价值观的系统能更稳健地泛化人类意图，而仅学习表面模式可能导致不对齐行为。

Method: 使用新颖的实验设计，在训练阶段让LLMs接触深层和浅层特征相关的人类偏好数据，测试阶段打破这些相关性，测量模型基于底层价值观而非浅层特征进行泛化的概率(DVGR)。

Result: 在9个不同模型中，平均DVGR仅为0.30，所有模型对深层价值观的泛化都低于随机水平。较大模型的DVGR略低于较小模型。

Conclusion: DVB提供了一个可解释的衡量对齐核心特征的方法，当前LLMs在泛化深层人类价值观方面表现不佳。

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [23] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Dan M. Frangopol,Minghui Cheng*

Main category: cs.AI

TL;DR: 该研究提出了InsurAgent，一个基于大语言模型的智能体，用于模拟洪水保险购买决策行为。通过五个模块（感知、检索、推理、行动和记忆）的结合，解决了LLMs在定量概率估计上的不足，并能够模拟时间决策演化。


<details>
  <summary>Details</summary>
Motivation: 美国高风险人群的洪水保险参与率极低，需要理解保险决策的行为机制。大语言模型展现出了模拟人类决策的潜力，但在定量概率估计方面存在局限。

Method: 构建基准数据集评估LLMs能力，提出InsurAgent智能体框架，包含五个模块：感知、检索（使用RAG技术基于调查数据）、推理（利用LLM常识进行推断）、行动和记忆（支持时间决策模拟）。

Result: LLMs对影响因素有定性理解但定量概率估计不足；InsurAgent通过RAG实现了边际和双变量概率的准确估计，并能捕捉传统模型难以处理的上下文信息。

Conclusion: InsurAgent为行为建模和政策分析提供了有价值的工具，能够模拟复杂的保险决策行为和时间演化过程。

Abstract: Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [24] [Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.02130)
*Renos Zabounidis,Aditya Golatkar,Michael Kleinman,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: Re-FORC是一种自适应奖励预测方法，通过训练轻量级适配器来预测未来奖励，支持动态推理长度控制，显著提升计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决推理模型在计算效率和准确性之间的权衡问题，需要一种能够动态控制推理长度并预测未来奖励的方法，以实现更高效的推理过程。

Method: 在推理模型上训练轻量级适配器，根据上下文预测未来奖励作为未来思考token数量的函数，支持早期停止、模型选择和自适应测试时扩展。

Result: 减少26%计算量保持准确性；在相同计算量下提升4%准确性，或在相同准确性下减少55%计算量；高计算模式下提升11%准确性，低计算模式下提升7%准确性。

Conclusion: Re-FORC通过动态推理长度控制和奖励预测，实现了计算效率和准确性的显著提升，为推理模型提供了有效的优化方案。

Abstract: We propose Re-FORC, an adaptive reward prediction method that, given a
context, enables prediction of the expected future rewards as a function of the
number of future thinking tokens. Re-FORC trains a lightweight adapter on
reasoning models, demonstrating improved prediction with longer reasoning and
larger models. Re-FORC enables: 1) early stopping of unpromising reasoning
chains, reducing compute by 26% while maintaining accuracy, 2) optimized model
and thinking length selection that achieves 4% higher accuracy at equal compute
and 55% less compute at equal accuracy compared to the largest model, 3)
adaptive test-time scaling, which increases accuracy by 11% in high compute
regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with
length control via cost-per-token thresholds while estimating computation time
upfront.

</details>


### [25] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: 本文提出了ATHENA框架，通过结合符号效用建模和语义适应来个性化预测个体决策行为，在旅行方式和疫苗选择任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 个体决策模型与群体最优预测存在差距，因为个体决策过程受到数值属性（如成本、时间）和语言因素（如个人偏好和约束）的共同影响。

Method: ATHENA框架包含两个阶段：1）通过LLM增强的符号发现获得群体级别的符号效用函数；2）基于最优效用进行个体级别的语义适应，创建个性化的语义模板来建模个性化选择。

Result: 在真实世界的旅行模式和疫苗选择任务中，ATHENA持续优于基于效用、机器学习和其他基于LLM的模型，F1分数比最强前沿模型至少提高6.5%。消融研究证实两个阶段都至关重要且互补。

Conclusion: 通过有机整合符号效用建模和语义适应，ATHENA为建模以人为本的决策提供了新方案。

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [26] [TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data](https://arxiv.org/abs/2511.02219)
*Changjiang Jiang,Fengchang Yu,Haihua Chen,Wei Lu,Jin Zeng*

Main category: cs.AI

TL;DR: 提出了一个名为\method的框架，通过查询分解、表格清理和基于程序思维的推理器来提升大语言模型在复杂表格数值推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理复杂表格数据推理时表现不佳，主要由于复杂查询、噪声数据和数值能力有限等问题。

Method: 框架包含三个组件：(1)查询分解器分解复杂问题，(2)表格清理器清洗和过滤噪声表格，(3)基于程序思维的推理器生成可执行代码从清理后的表格中推导最终答案。

Result: 在TAT-QA、TableBench和\method数据集上分别实现了8.79%、6.08%和19.87%的准确率提升，达到最先进性能。

Conclusion: 该框架能有效提升大语言模型在复杂表格数值推理任务上的性能，并能与主流大语言模型无缝集成。

Abstract: Complex reasoning over tabular data is crucial in real-world data analysis,
yet large language models (LLMs) often underperform due to complex queries,
noisy data, and limited numerical capabilities. To address these issues, we
propose \method, a framework consisting of: (1) a query decomposer that breaks
down complex questions, (2) a table sanitizer that cleans and filters noisy
tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates
executable code to derive the final answer from the sanitized table. To ensure
unbiased evaluation and mitigate data leakage, we introduce a new dataset,
CalTab151, specifically designed for complex numerical reasoning over tables.
Experimental results demonstrate that \method consistently outperforms existing
methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and
19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively.
Moreover, our framework integrates seamlessly with mainstream LLMs, providing a
robust solution for complex tabular numerical reasoning. These findings
highlight the effectiveness of our framework in enhancing LLM performance for
complex tabular numerical reasoning. Data and code are available upon request.

</details>


### [27] [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)
*Zhuoran Zhang,Tengyue Wang,Xilin Gong,Yang Shi,Haotian Wang,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: 本文提出了一个分析多模态大语言模型模态跟随行为的新框架，将模态跟随分解为相对推理不确定性和内在模态偏好两个因素，揭示了模态跟随概率随相对不确定性单调下降的普遍规律。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅使用粗粒度的数据集级统计来衡量多模态大语言模型的模态跟随行为，忽略了模型在单模态推理中的置信度影响，需要更精细的分析框架。

Method: 构建可控数据集系统变化视觉和文本输入的推理难度，使用熵作为细粒度不确定性度量，通过层间预测探测揭示内部机制。

Result: 发现模态跟随概率随相对不确定性单调下降的普遍规律，在平衡点处模型在两个模态间摇摆，揭示了内在模态偏好的定量指标。

Conclusion: 相对不确定性和内在偏好是模态跟随的两个控制原则，提供了定量框架和机制洞察，有助于理解多模态大语言模型如何处理冲突信息。

Abstract: Multimodal large language models (MLLMs) must resolve conflicts when
different modalities provide contradictory information, a process we term
modality following. Prior work measured this behavior only with coarse
dataset-level statistics, overlooking the influence of model's confidence in
unimodal reasoning. In this paper, we introduce a new framework that decomposes
modality following into two fundamental factors: relative reasoning uncertainty
(the case-specific confidence gap between unimodal predictions) and inherent
modality preference( a model's stable bias when uncertainties are balanced). To
validate this framework, we construct a controllable dataset that
systematically varies the reasoning difficulty of visual and textual inputs.
Using entropy as a fine-grained uncertainty metric, we uncover a universal law:
the probability of following a modality decreases monotonically as its relative
uncertainty increases. At the relative difficulty level where the model tends
to follow both modalities with comparable probability what we call the balance
point, a practical indicator of the model's inherent preference. Unlike
traditional macro-level ratios, this measure offers a more principled and less
confounded way to characterize modality bias, disentangling it from unimodal
capabilities and dataset artifacts. Further, by probing layer-wise predictions,
we reveal the internal mechanism of oscillation: in ambiguous regions near the
balance point, models vacillate between modalities across layers, explaining
externally observed indecision. Together, these findings establish relative
uncertainty and inherent preference as the two governing principles of modality
following, offering both a quantitative framework and mechanistic insight into
how MLLMs resolve conflicting information.

</details>


### [28] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 本文分析了多智能体推理中的懒惰行为问题，提出因果影响测量方法和可验证奖励机制来改善协作效果。


<details>
  <summary>Details</summary>
Motivation: 在多智能体推理中，存在懒惰行为问题，即一个智能体主导而另一个贡献很少，削弱了协作效果，使多智能体设置退化为无效的单智能体。

Method: 1) 理论分析懒惰行为的产生原因；2) 引入稳定高效的因果影响测量方法；3) 提出可验证奖励机制，允许推理智能体丢弃噪声输出、整合指令并在必要时重启推理过程。

Result: 大量实验表明，该框架能够缓解懒惰智能体行为，释放多智能体框架在复杂推理任务中的全部潜力。

Conclusion: 通过因果影响测量和可验证奖励机制，可以有效解决多智能体推理中的协作问题，提升整体性能。

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [29] [Chronic Kidney Disease Prognosis Prediction Using Transformer](https://arxiv.org/abs/2511.02340)
*Yohan Lee,DongGyun Kang,SeHoon Park,Sa-Yoon Park,Kwangsoo Kim*

Main category: cs.AI

TL;DR: 提出基于Transformer的ProQ-BERT框架，用于预测慢性肾脏病进展，整合多模态电子健康记录数据，在91,816患者队列中表现优异，ROC-AUC达0.995。


<details>
  <summary>Details</summary>
Motivation: 慢性肾脏病影响全球近10%人口，准确预测疾病进展对于及时干预和资源优化至关重要。

Method: 使用基于Transformer的框架，整合人口统计学、临床和实验室数据，采用量化分词处理连续实验室值，通过掩码语言建模预训练和二元分类微调。

Result: 在91,816患者队列中，模型持续优于CEHR-BERT，短期预测ROC-AUC达0.995，PR-AUC达0.989。

Conclusion: Transformer架构和时间设计选择在临床预后建模中效果显著，为个性化CKD护理提供了有前景的方向。

Abstract: Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and
often progresses to end-stage renal failure. Accurate prognosis prediction is
vital for timely interventions and resource optimization. We present a
transformer-based framework for predicting CKD progression using multi-modal
electronic health records (EHR) from the Seoul National University Hospital
OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates
demographic, clinical, and laboratory data, employing quantization-based
tokenization for continuous lab values and attention mechanisms for
interpretability. The model was pretrained with masked language modeling and
fine-tuned for binary classification tasks predicting progression from stage 3a
to stage 5 across varying follow-up and assessment periods. Evaluated on a
cohort of 91,816 patients, our model consistently outperformed CEHR-BERT,
achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.
These results highlight the effectiveness of transformer architectures and
temporal design choices in clinical prognosis modeling, offering a promising
direction for personalized CKD care.

</details>


### [30] [Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients](https://arxiv.org/abs/2511.02392)
*Muhammad Sheharyar Liaqat*

Main category: cs.AI

TL;DR: 基于模糊软集理论的专家系统，使用BMI、胰岛素、瘦素、脂联素和年龄等临床参数来评估乳腺癌风险，支持医疗专业人员识别高风险患者。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性主要死因之一，早期诊断对有效治疗和提高生存率至关重要。但由于疾病复杂性和患者风险因素变异性，及时检测仍面临挑战。

Method: 开发基于模糊软集理论的专家系统，整合BMI、胰岛素水平、瘦素水平、脂联素水平和年龄作为输入变量，通过模糊推理规则和软集计算估计乳腺癌风险。使用UCI机器学习库数据集进行模型开发和验证。

Result: 提出的系统能够通过常规血液分析获得参数，提供无创且易于获取的初步评估方法。

Conclusion: 该专家系统旨在帮助医疗专业人员识别高风险患者，并确定是否需要进一步诊断程序如活检。

Abstract: Breast cancer remains one of the leading causes of mortality among women
worldwide, with early diagnosis being critical for effective treatment and
improved survival rates. However, timely detection continues to be a challenge
due to the complex nature of the disease and variability in patient risk
factors. This study presents a fuzzy soft set theory-based expert system
designed to assess the risk of breast cancer in patients using measurable
clinical and physiological parameters. The proposed system integrates Body Mass
Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input
variables to estimate breast cancer risk through a set of fuzzy inference rules
and soft set computations. These parameters can be obtained from routine blood
analyses, enabling a non-invasive and accessible method for preliminary
assessment. The dataset used for model development and validation was obtained
from the UCI Machine Learning Repository. The proposed expert system aims to
support healthcare professionals in identifying high-risk patients and
determining the necessity of further diagnostic procedures such as biopsies.

</details>


### [31] [A New Perspective on Precision and Recall for Generative Models](https://arxiv.org/abs/2511.02414)
*Benjamin Sykes,Loïc Simon,Julien Rabin,Jalal Fadili*

Main category: cs.AI

TL;DR: 本文提出了一种基于二元分类视角的新框架来估计生成模型的精确率-召回率曲线，进行了统计分析和风险上界推导，并扩展了现有文献中的PR指标。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型在图像和文本领域的成功，其评估方法受到广泛关注。虽然现有方法主要依赖标量指标，但精确率-召回率曲线为生成模型提供了更丰富的分析途径，但其估计面临诸多挑战。

Method: 提出基于二元分类视角的新框架来估计整个PR曲线，进行统计分析和风险上界推导，并扩展现有文献中仅限于曲线极值的PR指标。

Result: 获得了PR估计风险的极小极大上界，框架能够扩展到文献中的多个重要PR指标，并在不同实验设置下研究了曲线的不同行为。

Conclusion: 该框架为生成模型的PR曲线估计提供了新的理论基础和实用方法，能够更全面地分析生成模型的性能。

Abstract: With the recent success of generative models in image and text, the question
of their evaluation has recently gained a lot of attention. While most methods
from the state of the art rely on scalar metrics, the introduction of Precision
and Recall (PR) for generative model has opened up a new avenue of research.
The associated PR curve allows for a richer analysis, but their estimation
poses several challenges. In this paper, we present a new framework for
estimating entire PR curves based on a binary classification standpoint. We
conduct a thorough statistical analysis of the proposed estimates. As a
byproduct, we obtain a minimax upper bound on the PR estimation risk. We also
show that our framework extends several landmark PR metrics of the literature
which by design are restrained to the extreme values of the curve. Finally, we
study the different behaviors of the curves obtained experimentally in various
settings.

</details>


### [32] [Auditable-choice reframing unlocks RL-based verification for open-ended tasks](https://arxiv.org/abs/2511.02463)
*Mengyu Zhang,Xubo Liu,Siyu Ding,Weichong Yin,Yu Sun,Hua Wu,Wenya Guo,Ying Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种可验证多选择重构（VMR）方法，将强化学习与可验证奖励范式扩展到开放领域任务，解决了传统方法在缺乏标准答案的开放任务中无法应用的问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习与可验证奖励方法在数学和编程等有标准答案的领域表现出色，但在缺乏标准答案的开放任务（如创意写作、指令遵循）中被视为非推理场景，忽略了推理能力的潜在价值。

Method: 提出可验证多选择重构方法，将开放数据重构为可验证的多选择格式，使得在缺乏明确标准答案的情况下仍能进行有效训练。

Result: 在多个基准测试上的实验结果表明，该方法能有效提升大语言模型在开放任务上的性能，在八个开放基准上平均比基线提升5.99分。

Conclusion: VMR方法成功将强化学习与可验证奖励范式扩展到开放领域，证明了加强推理能力可以提升开放任务性能，为开放领域推理训练提供了有效解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great
potential in enhancing the reasoning capabilities of large language models
(LLMs), achieving remarkable progress in domains such as mathematics and
programming where standard answers are available. However, for open-ended tasks
lacking ground-truth solutions (e.g., creative writing and instruction
following), existing studies typically regard them as non-reasoning scenarios,
thereby overlooking the latent value of reasoning capabilities. This raises a
key question: Can strengthening reasoning improve performance in open-ended
tasks? To address this, we explore the transfer of the RLVR paradigm to the
open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose
the existence of standard answers, it cannot be directly applied to open-ended
tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice
Reformulation (VMR), a novel training strategy that restructures open-ended
data into verifiable multiple-choice formats, enabling effective training even
in the absence of explicit ground truth. Experimental results on multiple
benchmarks validate the effectiveness of our method in improving LLM
performance on open-ended tasks. Notably, across eight open-ended benchmarks,
our VMR-based training delivers an average gain of 5.99 points over the
baseline. Code will be released upon acceptance to facilitate reproducibility.

</details>


### [33] [Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting](https://arxiv.org/abs/2511.02534)
*Enhong Mu,Jinyu Cai,Yijun Lu,Mingyue Zhang,Kenji Tei,Jialong Li*

Main category: cs.AI

TL;DR: KLPEG框架通过构建知识图谱来系统建模游戏元素、任务依赖和因果关系，利用LLM解析更新日志并基于知识图谱进行多跳推理，生成针对更新的测试用例，显著提升游戏测试的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现代视频游戏的快速迭代和频繁更新对测试效率和针对性提出了重大挑战，现有基于大语言模型的自动化测试方法缺乏结构化知识积累机制，难以针对增量游戏更新进行精确高效的测试。

Method: 提出KLPEG框架，构建和维护知识图谱来系统建模游戏元素、任务依赖和因果关系；利用LLM解析自然语言更新日志，通过知识图谱多跳推理识别影响范围，生成针对更新的测试用例。

Result: 在Overcooked和Minecraft两个代表性游戏环境中的实验表明，KLPEG能更准确地定位受更新影响的功能，并以更少的步骤完成测试，显著提高了测试效果和效率。

Conclusion: KLPEG框架通过知识图谱和LLM的结合，有效解决了游戏增量更新测试的挑战，实现了知识的积累和重用，为游戏自动化测试提供了有效的解决方案。

Abstract: The rapid iteration and frequent updates of modern video games pose
significant challenges to the efficiency and specificity of testing. Although
automated playtesting methods based on Large Language Models (LLMs) have shown
promise, they often lack structured knowledge accumulation mechanisms, making
it difficult to conduct precise and efficient testing tailored for incremental
game updates. To address this challenge, this paper proposes a KLPEG framework.
The framework constructs and maintains a Knowledge Graph (KG) to systematically
model game elements, task dependencies, and causal relationships, enabling
knowledge accumulation and reuse across versions. Building on this foundation,
the framework utilizes LLMs to parse natural language update logs, identify the
scope of impact through multi-hop reasoning on the KG, enabling the generation
of update-tailored test cases. Experiments in two representative game
environments, Overcooked and Minecraft, demonstrate that KLPEG can more
accurately locate functionalities affected by updates and complete tests in
fewer steps, significantly improving both playtesting effectiveness and
efficiency.

</details>


### [34] [The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models](https://arxiv.org/abs/2511.02589)
*Claudia Herambourg,Dawid Siuda,Anna Szczepanek,Julia Kopczyńska,Joao R. L. Santos,Wojciech Sas,Joanna Śmietańska-Nowak*

Main category: cs.AI

TL;DR: ORCA基准测试评估大语言模型在多领域定量推理任务上的表现，涵盖金融、物理、健康、统计等领域，结果显示当前最先进模型准确率仅为45-63%，主要错误源于舍入和计算错误。


<details>
  <summary>Details</summary>
Motivation: 创建ORCA基准测试是为了评估大语言模型在真实多领域定量推理任务中的表现，弥补传统数学数据集在评估逐步推理、数值精度和领域泛化能力方面的不足。

Method: 使用Omni计算引擎验证的输出来评估500个自然语言任务，涵盖金融、物理、健康、统计等多个领域，分析五个最先进大语言模型的表现。

Result: 五个最先进系统（ChatGPT-5、Gemini 2.5 Flash、Claude Sonnet 4.5、Grok 4和DeepSeek V3.2）准确率仅为45-63%，主要错误类型为舍入错误（35%）和计算错误（33%）。模型在数学和工程领域表现较强，但在物理和自然科学领域较弱。

Conclusion: 大语言模型在定量推理任务中存在显著局限性，不同模型具有部分互补性而非冗余性，需要改进数值计算精度和领域泛化能力。

Abstract: We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel
benchmark that evaluates large language models (LLMs) on multi-domain,
real-life quantitative reasoning using verified outputs from Omni's calculator
engine. In 500 natural-language tasks across domains such as finance, physics,
health, and statistics, the five state-of-the-art systems (ChatGPT-5,
Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only
$45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$)
and calculation mistakes ($33\,\%$). Results in specific domains indicate
strengths in mathematics and engineering, but weaknesses in physics and natural
sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the
models often fail together but differ in the types of errors they make,
highlighting their partial complementarity rather than redundancy. Unlike
standard math datasets, ORCA evaluates step-by-step reasoning, numerical
precision, and domain generalization across real problems from finance,
physics, health, and statistics.

</details>


### [35] [Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning](https://arxiv.org/abs/2511.02605)
*Tiberiu-Andrei Georgescu,Alexander W. Goodall,Dalal Alrajeh,Francesco Belardinelli,Sebastian Uchitel*

Main category: cs.AI

TL;DR: 本文提出了首个基于GR(1)规范的自适应屏蔽框架，通过运行时检测环境假设违规并使用归纳逻辑编程在线修复规范，确保屏蔽器优雅演化。


<details>
  <summary>Details</summary>
Motivation: 传统静态屏蔽方法假设固定的逻辑规范和手工抽象，在环境假设被违反时无法适应。需要开发能够动态调整的安全强化学习屏蔽框架。

Method: 基于GR(1)规范开发自适应屏蔽框架，使用归纳逻辑编程在线自动修复规范，系统化且可解释地调整屏蔽器。

Result: 在Minepump和Atari Seaquest案例研究中，自适应屏蔽相比静态屏蔽能保持接近最优的奖励和完美的逻辑合规性。

Conclusion: 自适应屏蔽框架能够有效处理环境假设违规，确保安全性和活性，在优化辅助奖励时显著优于静态方法。

Abstract: Shielding is widely used to enforce safety in reinforcement learning (RL),
ensuring that an agent's actions remain compliant with formal specifications.
Classical shielding approaches, however, are often static, in the sense that
they assume fixed logical specifications and hand-crafted abstractions. While
these static shields provide safety under nominal assumptions, they fail to
adapt when environment assumptions are violated. In this paper, we develop the
first adaptive shielding framework - to the best of our knowledge - based on
Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and
expressive fragment of Linear Temporal Logic (LTL) that captures both safety
and liveness properties. Our method detects environment assumption violations
at runtime and employs Inductive Logic Programming (ILP) to automatically
repair GR(1) specifications online, in a systematic and interpretable way. This
ensures that the shield evolves gracefully, ensuring liveness is achievable and
weakening goals only when necessary. We consider two case studies: Minepump and
Atari Seaquest; showing that (i) static symbolic controllers are often severely
suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped
with our adaptive shield maintain near-optimal reward and perfect logical
compliance compared with static shields.

</details>


### [36] [DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning](https://arxiv.org/abs/2511.02627)
*Lachlan McPheat,Navdeep Kaur,Robert Blackwell,Alessandra Russo,Anthony G. Cohn,Pranava Madhyastha*

Main category: cs.AI

TL;DR: DecompSR是一个用于分析组合空间推理能力的大型基准数据集和生成框架，包含500多万个数据点，通过程序化构建确保正确性，并用于评估大型语言模型在组合推理方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在组合空间推理能力方面存在不足，特别是生产性和系统性泛化能力较弱，需要专门的基准数据集来精确评估这些能力。

Method: 通过程序化生成框架构建数据集，独立控制组合性的多个方面：生产力（推理深度）、可替换性（实体和语言变异性）、过度泛化（输入顺序、干扰项）和系统性（新语言元素），并使用符号求解器验证数据集的正确性。

Result: 评估显示大型语言模型在空间推理任务中难以进行生产性和系统性泛化，但对语言变异性具有更强的鲁棒性。

Conclusion: DecompSR提供了一个可证明正确且严格的基准数据集，能够独立控制组合性的多个关键方面，为评估大型语言模型的组合推理能力提供了精细化的探测工具。

Abstract: We introduce DecompSR, decomposed spatial reasoning, a large benchmark
dataset (over 5m datapoints) and generation framework designed to analyse
compositional spatial reasoning ability. The generation of DecompSR allows
users to independently vary several aspects of compositionality, namely:
productivity (reasoning depth), substitutivity (entity and linguistic
variability), overgeneralisation (input order, distractors) and systematicity
(novel linguistic elements). DecompSR is built procedurally in a manner which
makes it is correct by construction, which is independently verified using a
symbolic solver to guarantee the correctness of the dataset. DecompSR is
comprehensively benchmarked across a host of Large Language Models (LLMs) where
we show that LLMs struggle with productive and systematic generalisation in
spatial reasoning tasks whereas they are more robust to linguistic variation.
DecompSR provides a provably correct and rigorous benchmarking dataset with a
novel ability to independently vary the degrees of several key aspects of
compositionality, allowing for robust and fine-grained probing of the
compositional reasoning abilities of LLMs.

</details>


### [37] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: 该研究提出了一个协作迷宫求解基准，评估了32个领先的AI模型在单独、同质和异质配对中的协作能力，发现存在'协作鸿沟'：单独表现好的模型在协作时性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，我们将越来越多地依赖由独立开发的异构智能体组成的系统，这些系统的成功关键在于智能体间的有效协作，但目前缺乏大规模评估智能体协作能力的实证研究。

Method: 设计了一个协作迷宫求解基准框架，该框架能够隔离协作能力、调节问题复杂度、实现可扩展的自动评分，并保持生态合理性。评估了32个领先的开源和闭源模型在不同配对模式下的表现。

Result: 发现存在明显的'协作鸿沟'：单独表现良好的模型在协作时性能显著下降。小规模蒸馏模型在某些配对中可能完全失败。研究发现从较强智能体开始可以改善结果，提出了'接力推理'方法。

Conclusion: 研究主张：(1)进行协作感知的评估，(2)开发增强协作能力的训练策略，(3)设计能够可靠激发智能体潜在技能的交互方式，这些指导原则适用于AI-AI和人类-AI协作。

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [38] [Using Span Queries to Optimize for Cache and Attention Locality](https://arxiv.org/abs/2511.02749)
*Paul Castro,Nick Mitchell,Nathan Ordonez,Thomas Parnell,Mudhakar Srivatsa,Antoni Viros i Martin*

Main category: cs.AI

TL;DR: 本文提出了span query概念，将推理服务器接口泛化，支持聊天、RAG、推理时缩放和智能体工作负载，通过交换性约束优化KV缓存局部性，在vLLM中实现高性能执行。


<details>
  <summary>Details</summary>
Motivation: 现有推理服务器主要针对聊天完成优化，而客户端已发展出多种推理时缩放和深度推理技术。先前工作虽能提升KV缓存命中率，但仅针对单一用例RAG。需要更通用的接口来支持多样化推理工作负载。

Method: 引入span query作为推理调用的表达式树，通过交换性约束链接。描述其语法和语义，展示如何自动优化KV缓存局部性。在vLLM中实现小规模修改（仅492行代码）以支持高性能span query执行。

Result: span query在两种不同非聊天用例中实现TTFT降低10-20倍。注意力优化的span query在2b参数模型上的准确率远超标准推理服务器使用8b模型的表现。

Conclusion: span query提供了一种通用接口，能够有效支持多样化推理工作负载，显著提升性能并避免中间丢失问题，为推理服务器演进提供了可行路径。

Abstract: Clients are evolving beyond chat completion, and now include a variety of
innovative inference-time scaling and deep reasoning techniques. At the same
time, inference servers remain heavily optimized for chat completion. Prior
work has shown that large improvements to KV cache hit rate are possible if
inference servers evolve towards these non-chat use cases. However, they offer
solutions that are also optimized for a single use case, RAG. In this paper, we
introduce the span query to generalize the interface to the inference server.
We demonstrate that chat, RAG, inference-time scaling, and agentic workloads
can all be expressed as span queries. We show how the critical distinction that
had been assumed by prior work lies in whether the order of the inputs matter
-- do they commute? In chat, they do not. In RAG, they often do. This paper
introduces span queries, which are expression trees of inference calls, linked
together with commutativity constraints. We describe span query syntax and
semantics. We show how they can be automatically optimized to improve KV cache
locality. We show how a small change to vLLM (affecting only 492 lines) can
enable high-performance execution of span queries. Using this stack, we
demonstrate that span queries can achieve 10-20x reductions in TTFT for two
distinct non-chat use cases. Finally, we show that span queries can also be
optimized to improve attention locality, so as to avoid the so-called
lost-in-the-middle problem. We demonstrate that an attention-optimized span
query on a 2b parameter model vastly outperforms the accuracy of a stock
inference server using an 8b model.

</details>


### [39] [LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer](https://arxiv.org/abs/2511.02759)
*Julius Fiedler,Carsten Knoll,Klaus Röbenack*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid growth of research output in control engineering calls for new
approaches to structure and formalize domain knowledge. This paper briefly
describes an LLM-supported method for semi-automated generation of formal
knowledge representations that combine human readability with machine
interpretability and increased expressiveness. Based on the Imperative
Representation of Knowledge (PyIRK) framework, we demonstrate how language
models can assist in transforming natural-language descriptions and
mathematical definitions (available as LaTeX source code) into a formalized
knowledge graph. As a first application we present the generation of an
``interactive semantic layer'' to enhance the source documents in order to
facilitate knowledge transfer. From our perspective this contributes to the
vision of easily accessible, collaborative, and verifiable knowledge bases for
the control engineering domain.

</details>


### [40] [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)
*Chloe Loughridge,Paul Colognese,Avery Griffin,Tyler Tracy,Jon Kutasov,Joe Benton*

Main category: cs.AI

TL;DR: 本文提出了一种在复杂AI环境中优化攻击策略的方法，通过将攻击能力分解为五个技能组件并分别优化，解决了数据不足的问题，显著提升了攻击强度。


<details>
  <summary>Details</summary>
Motivation: 随着AI部署变得更加复杂和高风险，准确评估其风险变得至关重要。AI控制框架需要强大的攻击策略来进行评估，但在复杂环境中由于计算限制导致数据不足，这带来了挑战。

Method: 将攻击能力分解为五个组成技能：怀疑建模、攻击选择、计划合成、执行和隐蔽性，并分别优化每个组件。为解决数据不足问题，开发了攻击动态的概率模型，在模拟中优化攻击超参数，然后将结果迁移到SHADE-Arena环境中。

Result: 该方法显著提高了攻击强度，将安全分数从基线0.87降低到0.41，表明攻击效果得到了实质性改善。

Conclusion: 通过技能分解和概率建模的方法，可以在数据有限的复杂环境中有效优化攻击策略，为AI风险评估提供了更可靠的工具。

Abstract: As AI deployments become more complex and high-stakes, it becomes
increasingly important to be able to estimate their risk. AI control is one
framework for doing so. However, good control evaluations require eliciting
strong attack policies. This can be challenging in complex agentic environments
where compute constraints leave us data-poor. In this work, we show how to
optimize attack policies in SHADE-Arena, a dataset of diverse realistic control
environments. We do this by decomposing attack capability into five constituent
skills -- suspicion modeling, attack selection, plan synthesis, execution and
subtlety -- and optimizing each component individually. To get around the
constraint of limited data, we develop a probabilistic model of attack
dynamics, optimize our attack hyperparameters using this simulation, and then
show that the results transfer to SHADE-Arena. This results in a substantial
improvement in attack strength, reducing safety score from a baseline of 0.87
to 0.41 using our scaffold.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization](https://arxiv.org/abs/2511.01884)
*Zijian Zhang,Rong Wang,Shiyang Li,Yuebo Luo,Mingyi Hong,Caiwen Ding*

Main category: cs.LG

TL;DR: CudaForge是一个无需训练的多智能体工作流，用于自动生成和优化CUDA内核，通过Coder和Judge两个LLM智能体迭代改进内核代码，结合硬件性能反馈，在保持高正确率的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 手动设计CUDA内核成本高且耗时，现有自动生成方法往往产生低效内核、计算开销大且泛化能力差，需要更高效的自动化解决方案。

Method: 采用无需训练的多智能体工作流，包含Coder和Judge两个LLM智能体，迭代生成、测试、分析和优化CUDA内核，集成Nsight Compute等硬件性能指标反馈。

Result: 在KernelBench上达到97.6%的正确率，平均比PyTorch基准快1.68倍，显著超越OpenAI-o3和Kevin等最先进模型，在多种GPU和基础模型上表现出强泛化能力。

Conclusion: 多智能体、无需训练的工作流能够实现成本效益高、泛化能力强且高性能的CUDA内核优化，生成优化内核仅需26.5分钟和0.3美元API成本。

Abstract: Developing efficient CUDA kernels is increasingly critical for AI
applications such as large-scale LLM training. However, manual kernel design is
both costly and time-consuming, motivating automatic approaches that leverage
LLMs for code generation. Existing methods for automatic kernel generation,
however, often produce low-efficiency kernels, incur high computational
overhead, and fail to generalize across settings. In this work, we propose
CudaForge, a training-free multi-agent workflow for CUDA kernel generation and
optimization. Our workflow is inspired by the iterative workflow of human
experts, which contains steps such as developing initial kernels, testing
correctness, analyzing hardware feedback, and iterative improvement. More
specifically, CudaForge employs two LLM agents: a Coder and a Judge, that
iteratively generate, correct, and optimize CUDA kernels, while integrating
hardware feedback such as Nsight Compute (NCU) metrics. In extensive
evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3,
achieves 97.6\% correctness of generated kernels and an average 1.68$\times$
speedup over PyTorch baselines, substantially surpassing state-of-the-art
models including OpenAI-o3 and Kevin on KernelBench. Beyond accuracy and speed,
CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090,
3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4,
QwQ-32B), while maintaining high efficiency. In particular, generating an
optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \$
0.3 API cost, which is significantly cheaper than existing agentic work that
costs 6 H100 hours and \$ 5 API cost per kernel. Our results highlight that
multi-agent, training-free workflows can enable cost-effective, generalizable,
and high-performance CUDA kernel optimization. Code available at
https://github.com/OptimAI-Lab/CudaForge

</details>


### [42] [Retrieval-Augmented Multimodal Depression Detection](https://arxiv.org/abs/2511.01892)
*Ruibo Hou,Shiyu Teng,Jiaqing Liu,Shurong Chai,Yinhao Li,Lanfen Lin,Yen-Wei Chen*

Main category: cs.LG

TL;DR: 提出一种基于检索增强生成(RAG)的多模态抑郁症检测框架，通过从情感数据集中检索相关情感内容并生成情感提示，增强情绪表征和可解释性，在AVEC 2019数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于情感分析的多模态抑郁症检测方法存在计算成本高、领域不匹配和静态知识限制等问题，需要更有效的解决方案。

Method: 使用RAG框架，对抑郁症相关文本从情感数据集中检索语义相关的情感内容，然后利用大语言模型生成情感提示作为辅助模态。

Result: 在AVEC 2019数据集上达到CCC 0.593和MAE 3.95的SOTA性能，超越了之前的迁移学习和多任务学习基线方法。

Conclusion: RAG框架通过检索相关情感内容并生成情感提示，有效增强了多模态抑郁症检测的情绪表征能力和可解释性，取得了显著性能提升。

Abstract: Multimodal deep learning has shown promise in depression detection by
integrating text, audio, and video signals. Recent work leverages sentiment
analysis to enhance emotional understanding, yet suffers from high
computational cost, domain mismatch, and static knowledge limitations. To
address these issues, we propose a novel Retrieval-Augmented Generation (RAG)
framework. Given a depression-related text, our method retrieves semantically
relevant emotional content from a sentiment dataset and uses a Large Language
Model (LLM) to generate an Emotion Prompt as an auxiliary modality. This prompt
enriches emotional representation and improves interpretability. Experiments on
the AVEC 2019 dataset show our approach achieves state-of-the-art performance
with CCC of 0.593 and MAE of 3.95, surpassing previous transfer learning and
multi-task learning baselines.

</details>


### [43] [The Eigenvalues Entropy as a Classifier Evaluation Measure](https://arxiv.org/abs/2511.01904)
*Doulaye Dembélé*

Main category: cs.LG

TL;DR: 本文提出使用特征值熵作为分类问题的评估指标，特别适用于类别不平衡的数据集。对于二分类问题，建立了特征值与常用指标（如灵敏度、特异性、AUC和Gini指数）之间的关系，并提供了处理类别不平衡问题的混淆矩阵估计方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标在类别不平衡数据集上准确性较低，需要一种更有效的评估方法来衡量分类器的性能。

Method: 使用特征值熵作为评估指标，对于二分类问题建立特征值与常用评估指标之间的关系，并开发处理类别不平衡的混淆矩阵估计方法。

Result: 通过多个数据示例证明，所提出的评估指标在性能上优于文献中的黄金标准指标。

Conclusion: 特征值熵是一种有效的分类问题评估指标，特别适用于类别不平衡的情况，能够提供比传统指标更准确的性能评估。

Abstract: Classification is a machine learning method used in many practical
applications: text mining, handwritten character recognition, face recognition,
pattern classification, scene labeling, computer vision, natural langage
processing. A classifier prediction results and training set information are
often used to get a contingency table which is used to quantify the method
quality through an evaluation measure. Such measure, typically a numerical
value, allows to choose a suitable method among several. Many evaluation
measures available in the literature are less accurate for a dataset with
imbalanced classes. In this paper, the eigenvalues entropy is used as an
evaluation measure for a binary or a multi-class problem. For a binary problem,
relations are given between the eigenvalues and some commonly used measures,
the sensitivity, the specificity, the area under the operating receiver
characteristic curve and the Gini index. A by-product result of this paper is
an estimate of the confusion matrix to deal with the curse of the imbalanced
classes. Various data examples are used to show the better performance of the
proposed evaluation measure over the gold standard measures available in the
literature.

</details>


### [44] [Superpositional Gradient Descent: Harnessing Quantum Principles for Model Training](https://arxiv.org/abs/2511.01918)
*Ahmet Erdem Pamuk,Emir Kaan Özdemir,Şuayp Talha Kocabay*

Main category: cs.LG

TL;DR: 提出了一种名为叠加梯度下降（SGD）的新型优化器，通过注入量子电路扰动将梯度更新与量子叠加联系起来，在合成序列分类和大规模LLM微调中比AdamW收敛更快且最终损失更低。


<details>
  <summary>Details</summary>
Motivation: 探索量子启发方法如何增强经典训练机制，研究量子计算与深度学习的交叉领域，寻找利用量子原理控制和增强模型行为的实用途径。

Method: 引入叠加梯度下降（SGD）优化器，建立数学框架，在PyTorch和Qiskit中实现混合量子经典电路，通过注入量子电路扰动将梯度更新与量子叠加联系起来。

Result: 在合成序列分类和大规模LLM微调任务中，SGD比AdamW收敛更快且最终损失更低，但可扩展性和硬件约束限制了其应用。

Conclusion: 这项工作为量子计算与深度学习的交叉领域提供了新见解，提出了利用量子原理控制和增强模型行为的实用途径。

Abstract: Large language models (LLMs) are increasingly trained with classical
optimization techniques like AdamW to improve convergence and generalization.
However, the mechanisms by which quantum-inspired methods enhance classical
training remain underexplored. We introduce Superpositional Gradient Descent
(SGD), a novel optimizer linking gradient updates with quantum superposition by
injecting quantum circuit perturbations. We present a mathematical framework
and implement hybrid quantum-classical circuits in PyTorch and Qiskit. On
synthetic sequence classification and large-scale LLM fine-tuning, SGD
converges faster and yields lower final loss than AdamW. Despite promising
results, scalability and hardware constraints limit adoption. Overall, this
work provides new insights into the intersection of quantum computing and deep
learning, suggesting practical pathways for leveraging quantum principles to
control and enhance model behavior.

</details>


### [45] [Neural Green's Functions](https://arxiv.org/abs/2511.01924)
*Seungwoo Yoo,Kyeongmin Yeo,Jisung Hwang,Minhyuk Sung*

Main category: cs.LG

TL;DR: 提出神经格林函数，一种用于线性偏微分方程的神经解算子，通过模仿格林函数的行为实现跨不规则几何和边界条件的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的解算子在泛化到未见过的源函数或边界函数时存在困难，需要设计对训练中使用的具体函数不敏感的框架。

Method: 从表示问题域的体素点云中提取每点特征，预测解算子的分解，然后通过数值积分评估解。

Result: 在MCB数据集的机械零件稳态热分析中，神经格林函数优于最先进的神经算子，平均误差降低13.9%，比需要计算昂贵网格的数值求解器快350倍。

Conclusion: 神经格林函数框架能够实现鲁棒和高效的泛化，在几何和函数变化下保持良好性能。

Abstract: We introduce Neural Green's Function, a neural solution operator for linear
partial differential equations (PDEs) whose differential operators admit
eigendecompositions. Inspired by Green's functions, the solution operators of
linear PDEs that depend exclusively on the domain geometry, we design Neural
Green's Function to imitate their behavior, achieving superior generalization
across diverse irregular geometries and source and boundary functions.
Specifically, Neural Green's Function extracts per-point features from a
volumetric point cloud representing the problem domain and uses them to predict
a decomposition of the solution operator, which is subsequently applied to
evaluate solutions via numerical integration. Unlike recent learning-based
solution operators, which often struggle to generalize to unseen source or
boundary functions, our framework is, by design, agnostic to the specific
functions used during training, enabling robust and efficient generalization.
In the steady-state thermal analysis of mechanical part geometries from the MCB
dataset, Neural Green's Function outperforms state-of-the-art neural operators,
achieving an average error reduction of 13.9\% across five shape categories,
while being up to 350 times faster than a numerical solver that requires
computationally expensive meshing.

</details>


### [46] [DeepContour: A Hybrid Deep Learning Framework for Accelerating Generalized Eigenvalue Problem Solving via Efficient Contour Design](https://arxiv.org/abs/2511.01927)
*Yeqiu Chen,Ziyan Liu,Hong Wang*

Main category: cs.LG

TL;DR: DeepContour是一个解决大规模广义特征值问题的混合框架，通过深度学习预测特征值分布，结合核密度估计自动设计积分轮廓，显著加速求解过程。


<details>
  <summary>Details</summary>
Motivation: 传统轮廓积分方法在求解广义特征值问题时，积分轮廓的选择严重依赖于对特征值分布的先验知识，不当选择会导致计算开销大和数值精度问题。

Method: 提出DeepContour框架：1）使用傅里叶神经算子快速预测GEP的谱分布；2）应用核密度估计自动确定合适的积分轮廓；3）用优化后的轮廓指导轮廓积分求解器高效找到目标特征值。

Result: 在多个数据集上的实验表明，DeepContour显著加速了GEP求解，实现了最高5.63倍的加速比。

Conclusion: 该工作开创性地将深度学习的预测能力与经典求解器的数值严谨性相结合，为处理高维矩阵的困难广义特征值问题提供了高效且鲁棒的范式。

Abstract: Solving large-scale Generalized Eigenvalue Problems (GEPs) is a fundamental
yet computationally prohibitive task in science and engineering. As a promising
direction, contour integral (CI) methods, such as the CIRR algorithm, offer an
efficient and parallelizable framework. However, their performance is
critically dependent on the selection of integration contours -- improper
selection without reliable prior knowledge of eigenvalue distribution can incur
significant computational overhead and compromise numerical accuracy. To
address this challenge, we propose DeepContour, a novel hybrid framework that
integrates a deep learning-based spectral predictor with Kernel Density
Estimation for principled contour design. Specifically, DeepContour first
employs a Fourier Neural Operator (FNO) to rapidly predict the spectral
distribution of a given GEP. Subsequently, Kernel Density Estimation (KDE) is
applied to the predicted spectrum to automatically and systematically determine
proper integration contours. Finally, these optimized contours guide the CI
solver to efficiently find the desired eigenvalues. We demonstrate the
effectiveness of our method on diverse challenging scientific problems. In our
main experiments, DeepContour accelerates GEP solving across multiple datasets,
achieving up to a 5.63$\times$ speedup. By combining the predictive power of
deep learning with the numerical rigor of classical solvers, this work pioneers
an efficient and robust paradigm for tackling difficult generalized eigenvalue
involving matrices of high dimension.

</details>


### [47] [Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models](https://arxiv.org/abs/2511.01932)
*Haoming Wang,Wei Gao*

Main category: cs.LG

TL;DR: FineXL是一个为个性化图像生成模型提供细粒度自然语言解释的新技术，能够识别多个个性化方面及其不同程度的个性化水平。


<details>
  <summary>Details</summary>
Motivation: 现有个性化模型缺乏可解释性，特别是自然语言解释方法过于粗粒度，无法精确识别多个个性化方面及其不同程度的个性化水平。

Method: 提出了FineXL技术，通过自然语言描述每个个性化方面，并提供定量分数来指示每个方面的个性化程度。

Result: 实验结果显示，FineXL在不同个性化场景应用于多种图像生成模型时，可将解释准确性提高56%。

Conclusion: FineXL能够有效解决个性化图像生成模型缺乏细粒度自然语言解释的问题，显著提升了解释的准确性和实用性。

Abstract: Image generation models are usually personalized in practical uses in order
to better meet the individual users' heterogeneous needs, but most personalized
models lack explainability about how they are being personalized. Such
explainability can be provided via visual features in generated images, but is
difficult for human users to understand. Explainability in natural language is
a better choice, but the existing approaches to explainability in natural
language are limited to be coarse-grained. They are unable to precisely
identify the multiple aspects of personalization, as well as the varying levels
of personalization in each aspect. To address such limitation, in this paper we
present a new technique, namely \textbf{FineXL}, towards \textbf{Fine}-grained
e\textbf{X}plainability in natural \textbf{L}anguage for personalized image
generation models. FineXL can provide natural language descriptions about each
distinct aspect of personalization, along with quantitative scores indicating
the level of each aspect of personalization. Experiment results show that
FineXL can improve the accuracy of explainability by 56\%, when different
personalization scenarios are applied to multiple types of image generation
models.

</details>


### [48] [Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch](https://arxiv.org/abs/2511.01934)
*Yirong Zeng,Xiao Ding,Yutai Hou,Yuxian Wang,Li Du,Juyi Dai,Qiuyang Ding,Duyu Tang,Dandan Tu,Weiwen Liu,Bing Qin,Ting Liu*

Main category: cs.LG

TL;DR: 提出了一种基于纯强化学习的工具增强LLM训练方法，通过动态泛化引导的奖励设计，使基础模型能够自主使用通用工具，在跨数据集和数据集内评估中均优于SFT和RL-with-SFT模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于监督微调的工具增强方法在泛化到不熟悉或复杂工具使用场景时存在困难，而强化学习能够赋予LLMs更好的推理和泛化能力。

Method: 提出动态泛化引导的奖励设计，逐步将奖励从探索性转向利用性工具使用模式，并基于此设计引入Tool-Zero系列模型，直接从基础模型进行纯强化学习扩展。

Result: 实验结果表明，在相同实验设置下，该方法相比SFT和RL-with-SFT模型实现了超过7%的性能提升，在跨数据集和数据集内评估中均表现出有效性和鲁棒性。

Conclusion: 纯强化学习能够有效激发模型内在推理能力并增强工具无关的泛化能力，为工具增强LLMs训练提供了新的有效途径。

Abstract: Training tool-augmented LLMs has emerged as a promising approach to enhancing
language models' capabilities for complex tasks. The current supervised
fine-tuning paradigm relies on constructing extensive domain-specific datasets
to train models. However, this approach often struggles to generalize
effectively to unfamiliar or intricate tool-use scenarios. Recently,
reinforcement learning (RL) paradigm can endow LLMs with superior reasoning and
generalization abilities. In this work, we address a key question: Can the pure
RL be used to effectively elicit a model's intrinsic reasoning capabilities and
enhance the tool-agnostic generalization? We propose a dynamic
generalization-guided reward design for rule-based RL, which progressively
shifts rewards from exploratory to exploitative tool-use patterns. Based on
this design, we introduce the Tool-Zero series models. These models are trained
to enable LLMs to autonomously utilize general tools by directly scaling up RL
from Zero models (i.e., base models without post-training). Experimental
results demonstrate that our models achieve over 7% performance improvement
compared to both SFT and RL-with-SFT models under the same experimental
settings. These gains are consistently replicated across cross-dataset and
intra-dataset evaluations, validating the effectiveness and robustness of our
methods.

</details>


### [49] [Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR](https://arxiv.org/abs/2511.01937)
*Abdelaziz Bounhar,Hadi Abdine,Evan Dufraisse,Ahmad Chamma,Amr Mohamed,Dani Bouch,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.LG

TL;DR: 论文提出通过保留并适度加权中等难度问题作为隐式长度正则化器，解决LLM推理训练中输出冗长的问题，实现无需显式长度惩罚的简洁推理。


<details>
  <summary>Details</summary>
Motivation: 标准RLVR训练流程过滤掉简单问题，导致模型主要在需要长推理链的难题上训练，使模型将'思考更久'与'思考更好'混淆，产生过度冗长的输出。

Method: 保留并适度加权中等难度问题作为隐式长度正则化器，让模型接触可解决的短链任务来约束输出分布，防止冗长失控。

Result: 在Qwen3-4B-Thinking-2507上的RLVR实验表明，该方法在保持基线AIME25准确率的同时，生成的解决方案平均缩短近两倍。

Conclusion: 该方法实现了'免费出现的简洁性'，模型学会解决更难问题而不增加输出长度，无需任何显式长度惩罚。

Abstract: Large language models (LLMs) trained for step-by-step reasoning often become
excessively verbose, raising inference cost. Standard Reinforcement Learning
with Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for
training efficiency, leaving the model to train primarily on harder problems
that require longer reasoning chains. This skews the output length distribution
upward, resulting in a \textbf{model that conflates ``thinking longer'' with
``thinking better''}. In this work, we show that retaining and modestly
up-weighting moderately easy problems acts as an implicit length regularizer.
Exposing the model to solvable short-chain tasks constrains its output
distribution and prevents runaway verbosity. The result is
\textbf{\emph{emergent brevity for free}}: the model learns to solve harder
problems without inflating the output length, \textbf{ despite the absence of
any explicit length penalization}. RLVR experiments using this approach on
\textit{Qwen3-4B-Thinking-2507} (with a 16k token limit) achieve baseline
pass@1 AIME25 accuracy while generating solutions that are, on average, nearly
twice as short. The code is available at
\href{https://github.com/MBZUAI-Paris/Frugal-AI}{GitHub}, with datasets and
models on
\href{https://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc}{Hugging
Face}.

</details>


### [50] [Learning a Distance for the Clustering of Patients with Amyotrophic Lateral Sclerosis](https://arxiv.org/abs/2511.01945)
*Guillaume Tejedor,Veronika Peralta,Nicolas Labroche,Patrick Marcel,Hélène Blasco,Hugo Alarcan*

Main category: cs.LG

TL;DR: 提出一种基于疾病进展评分和多种距离度量的ALS患者聚类方法，在生存分析方面优于现有技术，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: ALS患者治疗反应差异大，需要个性化护理，但现有研究受限于小样本、异质性数据和缺乏明确定义的临床患者分群。

Method: 使用疾病进展评分对序列进行聚类，整合医学专业知识，研究多种距离度量方法，包括现成距离和弱监督学习方法，并与聚类方法配对。

Result: 在353名ALS患者数据集上的评估显示，该方法在生存分析方面优于最先进方法，同时达到相当的轮廓分数，学习到的距离增强了结果的相关性和可解释性。

Conclusion: 该方法为ALS患者提供了有效的聚类解决方案，在生存分析性能上有所提升，同时保持了医学专家的可解释性。

Abstract: Amyotrophic lateral sclerosis (ALS) is a severe disease with a typical
survival of 3-5 years after symptom onset. Current treatments offer only
limited life extension, and the variability in patient responses highlights the
need for personalized care. However, research is hindered by small,
heterogeneous cohorts, sparse longitudinal data, and the lack of a clear
definition for clinically meaningful patient clusters. Existing clustering
methods remain limited in both scope and number. To address this, we propose a
clustering approach that groups sequences using a disease progression
declarative score. Our approach integrates medical expertise through multiple
descriptive variables, investigating several distance measures combining such
variables, both by reusing off-the-shelf distances and employing a
weak-supervised learning method. We pair these distances with clustering
methods and benchmark them against state-of-the-art techniques. The evaluation
of our approach on a dataset of 353 ALS patients from the University Hospital
of Tours, shows that our method outperforms state-of-the-art methods in
survival analysis while achieving comparable silhouette scores. In addition,
the learned distances enhance the relevance and interpretability of results for
medical experts.

</details>


### [51] [COFAP: A Universal Framework for COFs Adsorption Prediction through Designed Multi-Modal Extraction and Cross-Modal Synergy](https://arxiv.org/abs/2511.01946)
*Zihan Li,Mingyang Wan,Mingyu Gao,Zhongshan Chen,Xiangke Wang,Feifan Zhang*

Main category: cs.LG

TL;DR: 提出COFAP框架，通过深度学习提取多模态结构化学特征，使用跨模态注意力机制融合特征，无需传统气体相关特征即可实现高效COFs吸附预测。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习预测器依赖特定气体相关特征，这些特征计算耗时且限制可扩展性，导致效率低下和劳动密集型过程。

Method: 开发COFAP框架，通过深度学习提取多模态结构化学特征，使用跨模态注意力机制融合这些互补特征，无需亨利系数或吸附热。

Result: 在hypoCOFs数据集上超越先前方法，达到新的SOTA性能；发现高性能分离COFs集中在狭窄的孔径和表面积范围内。

Conclusion: COFAP具有卓越的效率和准确性，可直接部署于晶体多孔材料中，并开发了权重可调优先排序方案，为研究人员提供灵活的应用特定候选COFs排序。

Abstract: Covalent organic frameworks (COFs) are promising adsorbents for gas
adsorption and separation, while identifying the optimal structures among their
vast design space requires efficient high-throughput screening. Conventional
machine-learning predictors rely heavily on specific gas-related features.
However, these features are time-consuming and limit scalability, leading to
inefficiency and labor-intensive processes. Herein, a universal COFs adsorption
prediction framework (COFAP) is proposed, which can extract multi-modal
structural and chemical features through deep learning, and fuse these
complementary features via cross-modal attention mechanism. Without Henry
coefficients or adsorption heat, COFAP sets a new SOTA by outperforming
previous approaches on hypoCOFs dataset. Based on COFAP, we also found that
high-performing COFs for separation concentrate within a narrow range of pore
size and surface area. A weight-adjustable prioritization scheme is also
developed to enable flexible, application-specific ranking of candidate COFs
for researchers. Superior efficiency and accuracy render COFAP directly
deployable in crystalline porous materials.

</details>


### [52] [NeuroClean: A Generalized Machine-Learning Approach to Neural Time-Series Conditioning](https://arxiv.org/abs/2511.01951)
*Manuel A. Hernandez Alonso,Michael Depass,Stephan Quessy,Numa Dancause,Ignasi Cos*

Main category: cs.LG

TL;DR: NeuroClean是一个无监督的EEG/LFP预处理流程，通过五步处理过程自动去除脑电信号中的伪影和噪声，使用机器学习分类器确保任务相关信息得以保留，在运动任务分类中准确率从74%提升到97%以上。


<details>
  <summary>Details</summary>
Motivation: 脑电图(EEG)和局部场电位(LFP)记录中存在大量伪影和噪声，需要自动化、无监督的预处理方法来避免人为干预带来的偏差和可重复性问题。

Method: 设计了一个五步预处理流程，包括带通滤波、线路噪声滤波、坏通道剔除，并引入了高效的独立成分分析和基于聚类算法的自动成分剔除，使用机器学习分类器确保任务相关信息保留。

Result: NeuroClean成功去除了多种常见伪影，在运动任务分类中，经过清理的数据在优化多项式逻辑回归模型中的准确率达到97%以上（机会水平为33.3%），而原始数据仅为74%。

Conclusion: NeuroClean是一个有前景的预处理流程和工作流，可应用于未来的研究和工作中，以实现机器学习流程更好的泛化性和性能。

Abstract: Electroencephalography (EEG) and local field potentials (LFP) are two widely
used techniques to record electrical activity from the brain. These signals are
used in both the clinical and research domains for multiple applications.
However, most brain data recordings suffer from a myriad of artifacts and noise
sources other than the brain itself. Thus, a major requirement for their use is
proper and, given current volumes of data, a fully automatized conditioning. As
a means to this end, here we introduce an unsupervised, multipurpose EEG/LFP
preprocessing method, the NeuroClean pipeline. In addition to its completeness
and reliability, NeuroClean is an unsupervised series of algorithms intended to
mitigate reproducibility issues and biases caused by human intervention. The
pipeline is designed as a five-step process, including the common bandpass and
line noise filtering, and bad channel rejection. However, it incorporates an
efficient independent component analysis with an automatic component rejection
based on a clustering algorithm. This machine learning classifier is used to
ensure that task-relevant information is preserved after each step of the
cleaning process. We used several data sets to validate the pipeline.
NeuroClean removed several common types of artifacts from the signal. Moreover,
in the context of motor tasks of varying complexity, it yielded more than 97%
accuracy (vs. a chance-level of 33.3%) in an optimized Multinomial Logistic
Regression model after cleaning the data, compared to the raw data, which
performed at 74% accuracy. These results show that NeuroClean is a promising
pipeline and workflow that can be applied to future work and studies to achieve
better generalization and performance on machine learning pipelines.

</details>


### [53] [TapOut: A Bandit-Based Approach to Dynamic Speculative Decoding](https://arxiv.org/abs/2511.02017)
*Aditya Sridhar,Nish Sinnadurai,Sean Lie,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: TapOut是一种基于多臂老虎机的动态推测解码算法，无需训练即可智能选择推测策略，在多个模型和数据集上实现竞争性或更优的加速效果。


<details>
  <summary>Details</summary>
Motivation: 推测解码通过轻量级草稿模型生成令牌并用大型目标模型并行验证来加速LLMs，但确定最佳推测令牌数量是限制方法有效性的关键挑战。现有方法依赖手动调整的敏感阈值，成本高且跨模型和领域泛化能力差。

Method: 提出TapOut算法，使用多臂老虎机在线选择无参数动态推测策略，基于历史奖励和探索选择最佳策略，无需超参数调优。

Result: 在多样化模型对和数据集上的广泛实验表明，TapOut相比成熟的动态推测基线方法实现了竞争性或更优的加速效果。

Conclusion: TapOut提供了一种无需训练、即插即用的动态推测策略选择方法，有效解决了推测解码中令牌数量选择的关键挑战。

Abstract: Speculative decoding accelerates LLMs by using a lightweight draft model to
generate tokens autoregressively before verifying them in parallel with a
larger target model. However, determining the optimal number of tokens to draft
remains a key challenge limiting the approach's effectiveness. Dynamic
speculative decoding aims to intelligently decide how many tokens to draft to
achieve maximum speedups. Existing methods often rely on hand-tuned, sensitive
thresholds (e.g., token entropy), which are costly to set and generalize poorly
across models and domains. We propose TapOut, an online, training-free,
plug-and-play algorithm for dynamic speculation policy selection using
multi-armed bandits. Our approach employs a meta-algorithm that selects among
multiple parameter-free dynamic speculation strategies based on past reward and
exploration. We conduct extensive experiments across diverse model pairs and
datasets, showing that TapOut achieves competitive or superior speedups
compared to well-established dynamic speculation baselines without any
hyperparameter tuning.

</details>


### [54] [Shared Parameter Subspaces and Cross-Task Linearity in Emergently Misaligned Behavior](https://arxiv.org/abs/2511.02022)
*Daniel Aarao Reis Arturi,Eric Zhang,Andrew Ansah,Kevin Zhu,Ashwinee Panda,Aishwarya Balwani*

Main category: cs.LG

TL;DR: 本文从几何角度研究大型语言模型在微调后出现的突发错位现象，发现不同有害任务在参数空间中具有线性结构，共享相似的参数方向和低维子空间。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型在窄域有害数据集上微调后为何会发展出广泛错位行为的根本机制，这种现象被称为突发错位。

Method: 采用几何视角分析突发错位，通过余弦相似度、主角度和投影重叠测量参数收敛性，并使用线性模式连接性验证功能等价性。

Result: 发现突发错位参数在不同任务间高度收敛，微调权重更新具有较高的余弦相似度，共享低维子空间，插值模型保持一致的广泛错位行为。

Conclusion: 突发错位源于不同窄域任务发现相同的共享参数方向，表明有害行为可能组织在权重景观的特定可预测区域中。

Abstract: Recent work has discovered that large language models can develop broadly
misaligned behaviors after being fine-tuned on narrowly harmful datasets, a
phenomenon known as emergent misalignment (EM). However, the fundamental
mechanisms enabling such harmful generalization across disparate domains remain
poorly understood. In this work, we adopt a geometric perspective to study EM
and demonstrate that it exhibits a fundamental cross-task linear structure in
how harmful behavior is encoded across different datasets. Specifically, we
find a strong convergence in EM parameters across tasks, with the fine-tuned
weight updates showing relatively high cosine similarities, as well as shared
lower-dimensional subspaces as measured by their principal angles and
projection overlaps. Furthermore, we also show functional equivalence via
linear mode connectivity, wherein interpolated models across narrow
misalignment tasks maintain coherent, broadly misaligned behavior. Our results
indicate that EM arises from different narrow tasks discovering the same set of
shared parameter directions, suggesting that harmful behaviors may be organized
into specific, predictable regions of the weight landscape. By revealing this
fundamental connection between parametric geometry and behavioral outcomes, we
hope our work catalyzes further research on parameter space interpretability
and weight-based interventions.

</details>


### [55] [Regularization Through Reasoning: Systematic Improvements in Language Model Classification via Explanation-Enhanced Fine-Tuning](https://arxiv.org/abs/2511.02044)
*Vivswan Shah,Randy Cogill,Hanwei Yue,Gopinath Chennupati,Rinat Khaziev*

Main category: cs.LG

TL;DR: 在LLM分类任务微调中，为标签附加简短解释能提升模型性能。研究发现即使使用语义不连贯但词汇对齐的伪解释，也能获得大部分性能提升，表明这种改进主要源于结构而非语义。


<details>
  <summary>Details</summary>
Motivation: 探索在LLM分类任务微调过程中，为每个标签附加简短解释是否能产生更好的模型，以及这种改进背后的机制。

Method: 使用多个LLM生成的数据集，对7B参数模型进行微调，比较标签加解释训练与仅标签训练的效果。特别研究了用语义不连贯但词汇对齐的伪解释替代真实解释的情况。

Result: 在18个数据集和任务设置中，标签加解释训练始终优于仅标签基线。即使使用伪解释（如乱序或词袋变体），也能显著提高准确率，缩小与真实解释的差距。模型内部分析显示解释增强模型在中间层具有更高的激活熵，在输出层有更尖锐的预测分布。

Conclusion: 解释增强的微调（无论是真实解释还是精心构建的随机标记序列）都能提高LLM分类的准确性和可靠性，同时阐明了标记级支架如何在推理过程中塑造计算过程。

Abstract: Fine-tuning LLMs for classification typically maps inputs directly to labels.
We ask whether attaching brief explanations to each label during fine-tuning
yields better models. We evaluate conversational response quality along three
axes: naturalness, comprehensiveness, and on-topic adherence, each rated on
5-point scales. Using ensemble-generated data from multiple LLMs, we fine-tune
a 7B-parameter model and test across six diverse conversational datasets.
Across 18 dataset, task settings, label-plus-explanation training outperforms
label-only baselines.
  A central and unexpected result concerns random tokens. We replace
human-written explanations with text that is syntactically incoherent yet
vocabulary-aligned with the originals (e.g., shuffled or bag-of-words
variants). Despite lacking semantics, these pseudo-explanations still improve
accuracy over label-only training and often narrow much of the gap to true
explanations. The effect persists across datasets and training seeds,
indicating that gains arise less from meaning than from structure: the extra
token budget encourages richer intermediate computation and acts as a
regularizer that reduces over-confident shortcuts.
  Internal analyses support this view: explanation-augmented models exhibit
higher activation entropy in intermediate layers alongside sharper predictive
mass at the output layer, consistent with increased deliberation before
decision. Overall, explanation-augmented fine-tuning, whether with genuine
rationales or carefully constructed random token sequences, improves accuracy
and reliability for LLM classification while clarifying how token-level
scaffolding shapes computation during inference.

</details>


### [56] [Quantum-Enhanced Generative Models for Rare Event Prediction](https://arxiv.org/abs/2511.02042)
*M. Z. Haider,M. U. Ghouri,Tayyaba Noreen,M. Salman*

Main category: cs.LG

TL;DR: 提出量子增强生成模型(QEGM)，一种混合经典-量子框架，通过结合深度潜变量模型和变分量子电路来改进罕见事件的建模。


<details>
  <summary>Details</summary>
Motivation: 罕见事件（如金融崩溃、极端气候、生物异常）由于稀缺性和重尾分布而难以建模，传统深度生成模型难以捕捉这些罕见事件，容易出现模式崩溃或不确定性估计不准的问题。

Method: QEGM框架包含两个关键创新：(1) 混合损失函数同时优化重建保真度和尾部感知似然；(2) 量子随机性驱动的噪声注入以增强样本多样性和缓解模式崩溃。训练通过混合循环进行，经典参数通过反向传播更新，量子参数通过参数偏移梯度优化。

Result: 在合成高斯混合和真实世界数据集（金融、气候、蛋白质结构）上的评估显示，QEGM相比最先进基线（GAN、VAE、Diffusion）将尾部KL散度降低高达50%，同时改善了罕见事件召回率和覆盖校准。

Conclusion: QEGM作为罕见事件预测的原则性方法具有潜力，提供了超越纯经典方法的鲁棒性。

Abstract: Rare events such as financial crashes, climate extremes, and biological
anomalies are notoriously difficult to model due to their scarcity and
heavy-tailed distributions. Classical deep generative models often struggle to
capture these rare occurrences, either collapsing low-probability modes or
producing poorly calibrated uncertainty estimates. In this work, we propose the
Quantum-Enhanced Generative Model (QEGM), a hybrid classical-quantum framework
that integrates deep latent-variable models with variational quantum circuits.
The framework introduces two key innovations: (1) a hybrid loss function that
jointly optimizes reconstruction fidelity and tail-aware likelihood, and (2)
quantum randomness-driven noise injection to enhance sample diversity and
mitigate mode collapse. Training proceeds via a hybrid loop where classical
parameters are updated through backpropagation while quantum parameters are
optimized using parameter-shift gradients. We evaluate QEGM on synthetic
Gaussian mixtures and real-world datasets spanning finance, climate, and
protein structure. Results demonstrate that QEGM reduces tail KL divergence by
up to 50 percent compared to state-of-the-art baselines (GAN, VAE, Diffusion),
while improving rare-event recall and coverage calibration. These findings
highlight the potential of QEGM as a principled approach for rare-event
prediction, offering robustness beyond what is achievable with purely classical
methods.

</details>


### [57] [LLM Probing with Contrastive Eigenproblems: Improving Understanding and Applicability of CCS](https://arxiv.org/abs/2511.02089)
*Stefan F. Schouten,Peter Bloem*

Main category: cs.LG

TL;DR: 本文重新审视了对比一致性搜索（CCS）方法，将其重新表述为特征值问题，提供了闭式解和可解释的特征值，并扩展到多变量情况，避免了随机初始化的敏感性问题。


<details>
  <summary>Details</summary>
Motivation: CCS作为一种无监督探测方法，能够测试大语言模型是否在其内部激活中表示二元特征（如句子真值），但其双项目标函数仅被部分理解。本文旨在澄清其机制并扩展其适用性。

Method: 提出相对对比一致性的优化目标，将CCS重新表述为特征值问题，得到闭式解和可解释的特征值，并自然扩展到多变量情况。

Result: 在多个数据集上的评估表明，新方法恢复了与CCS相似的性能，同时避免了随机初始化的敏感性问题。

Conclusion: 相对化对比一致性不仅改进了对CCS的理解，还为更广泛的探测和机制可解释性方法开辟了途径。

Abstract: Contrast-Consistent Search (CCS) is an unsupervised probing method able to
test whether large language models represent binary features, such as sentence
truth, in their internal activations. While CCS has shown promise, its two-term
objective has been only partially understood. In this work, we revisit CCS with
the aim of clarifying its mechanisms and extending its applicability. We argue
that what should be optimized for, is relative contrast consistency. Building
on this insight, we reformulate CCS as an eigenproblem, yielding closed-form
solutions with interpretable eigenvalues and natural extensions to multiple
variables. We evaluate these approaches across a range of datasets, finding
that they recover similar performance to CCS, while avoiding problems around
sensitivity to random initialization. Our results suggest that relativizing
contrast consistency not only improves our understanding of CCS but also opens
pathways for broader probing and mechanistic interpretability methods.

</details>


### [58] [Flashlight: PyTorch Compiler Extensions to Accelerate Attention Variants](https://arxiv.org/abs/2511.02043)
*Bozhi You,Irene Wang,Zelal Su Mustafaoglu,Abhinav Jangda,Angélica Moreira,Roshan Dathathri,Divya Mahajan,Keshav Pingali*

Main category: cs.LG

TL;DR: Flashlight是一个PyTorch生态系统中的编译器原生框架，能够自动为任意基于注意力的程序生成融合的FlashAttention风格内核，无需依赖静态模板或预定义内核特化。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力优化方法如FlashAttention和FlexAttention存在局限性，前者需要专门的内核，后者仅支持部分注意力变体且无法处理数据依赖的注意力公式。需要更灵活高效的解决方案。

Method: 利用PyTorch的编译工作流透明地融合和分块注意力计算，支持任意注意力模式，包括FlexAttention无法处理的数据依赖注意力公式。

Result: Flashlight生成的内核性能与FlexAttention相当或更优，同时提供原生PyTorch代码的灵活性。

Conclusion: Flashlight在保持高性能的同时提供了更大的灵活性，使开发者能够快速探索新的注意力模型而无需牺牲性能。

Abstract: Bad charactors when submitting to arXiv: Attention is a fundamental building
block of large language models (LLMs), so there have been many efforts to
implement it efficiently. For example, FlashAttention leverages tiling and
kernel fusion to optimize attention. Recently, a number of variants of
attention have been introduced to enhance model quality or efficiency.
Supporting them efficiently remains difficult since they usually require
specialized kernels or hand-tuned implementations. FlexAttention recently
addressed part of this gap by using static programming templates to support
FlashAttention-like kernels for a subset of attention variants.
  In this paper, we introduce Flashlight, a compiler-native framework within
the PyTorch ecosystem that automatically generates fused, FlashAttention-style
kernels for arbitrary attention-based programs, without relying on static
templates or predefined kernel specializations. Flashlight leverages PyTorch's
compilation workflow to fuse and tile attention computations transparently,
enabling efficient execution for diverse attention patterns. Not only does it
support all variants expressible in the FlexAttention model but it also handles
more general, data-dependent attention formulations that are beyond the
capabilities of FlexAttention.
  Our results show that Flashlight produces kernels with competitive or
superior performance to FlexAttention, while offering the flexibility of native
PyTorch code, enabling developers to rapidly explore new attention models
without sacrificing performance.

</details>


### [59] [A Dual-Use Framework for Clinical Gait Analysis: Attention-Based Sensor Optimization and Automated Dataset Auditing](https://arxiv.org/abs/2511.02047)
*Hamidreza Sadeghsalehi*

Main category: cs.LG

TL;DR: 提出了一种多流注意力深度学习框架，既能优化传感器配置，又能自动审计数据集完整性。该模型在步态分析任务中发现了严重的侧向性偏差，并提出了数据驱动的传感器协同方案。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感器和AI在客观步态分析中至关重要，但模型容易受到隐藏数据集偏差的影响，且特定任务的传感器优化仍然具有挑战性。

Method: 采用多流注意力深度学习框架，作为传感器优化器和自动数据审计器，应用于多队列步态数据集上的四个临床任务。

Result: 模型注意力机制定量发现了严重的数据集混淆，在OA和CVA筛查任务中，模型将超过70%注意力分配给右脚，而统计上忽略左脚（小于0.1%注意力），这反映了公共数据集中严重的侧向性偏差。

Conclusion: 主要贡献是方法论上的，证明可解释框架可以自动审计数据集完整性；次要发现是模型提出了新颖的数据驱动传感器协同方案，为未来优化协议提供假设。

Abstract: Objective gait analysis using wearable sensors and AI is critical for
managing neurological and orthopedic conditions. However, models are vulnerable
to hidden dataset biases, and task-specific sensor optimization remains a
challenge. We propose a multi-stream attention-based deep learning framework
that functions as both a sensor optimizer and an automated data auditor.
Applied to the Voisard et al. (2025) multi-cohort gait dataset on four clinical
tasks (PD, OA, CVA screening; PD vs CVA differential), the model's attention
mechanism quantitatively discovered a severe dataset confound. For OA and CVA
screening, tasks where bilateral assessment is clinically essential, the model
assigned more than 70 percent attention to the Right Foot while statistically
ignoring the Left Foot (less than 0.1 percent attention, 95 percent CI
[0.0-0.1]). This was not a clinical finding but a direct reflection of a severe
laterality bias (for example, 15 of 15 right-sided OA) in the public dataset.
The primary contribution of this work is methodological, demonstrating that an
interpretable framework can automatically audit dataset integrity. As a
secondary finding, the model proposes novel, data-driven sensor synergies (for
example, Head plus Foot for PD screening) as hypotheses for future optimized
protocols.

</details>


### [60] [Finding Probably Approximate Optimal Solutions by Training to Estimate the Optimal Values of Subproblems](https://arxiv.org/abs/2511.02048)
*Nimrod Megiddo,Segev Wasserkrug,Orit Davidovich,Shimrit Shtern*

Main category: cs.LG

TL;DR: 开发一种用于最大化二元变量实值函数的求解器，该求解器基于估计目标函数及其子实例分布的最优目标函数值的算法。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要计算策略值或依赖已解决的实例，这在大规模问题中效率低下。本文旨在开发一种不依赖这些传统要素的求解方法。

Method: 使用基于不等式的估计器训练方法，以期望总偏差作为损失函数，而非直接使用目标函数本身。该方法通过估计最优目标函数值来指导求解过程。

Result: 提出了一种新型求解器，能够有效处理二元变量优化问题，无需计算策略值或依赖已解决的实例。

Conclusion: 该方法提供了一种更高效的优化求解途径，特别适用于大规模二元变量优化问题，具有较好的实用价值。

Abstract: The paper is about developing a solver for maximizing a real-valued function
of binary variables. The solver relies on an algorithm that estimates the
optimal objective-function value of instances from the underlying distribution
of objectives and their respective sub-instances. The training of the estimator
is based on an inequality that facilitates the use of the expected total
deviation from optimality conditions as a loss function rather than the
objective-function itself. Thus, it does not calculate values of policies, nor
does it rely on solved instances.

</details>


### [61] [Energy Loss Functions for Physical Systems](https://arxiv.org/abs/2511.02087)
*Sékou-Oumar Kaba,Kusha Sareen,Daniel Levy,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 提出一种将物理信息直接融入损失函数的框架，通过假设数据样本处于热平衡状态，使用反向KL散度推导出基于能量的损失函数，在分子生成和自旋基态预测任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在架构层面融入物理知识，需要开发一种能够直接在损失函数中利用物理信息的通用框架，以更好地指导机器学习模型学习科学系统的物理规律。

Method: 假设每个数据样本相对于近似能量景观处于热平衡状态，使用反向KL散度结合玻尔兹曼分布，推导出数据与模型预测之间的能量差作为损失函数。该方法与架构无关且计算高效。

Result: 在分子生成和自旋基态预测任务中，该方法相比基线方法取得了显著改进，能量损失函数能够更好地与有效配置对齐，并固有地尊重物理对称性。

Conclusion: 提出的基于能量的损失函数框架能够有效利用物理知识，提供物理上有意义的训练目标，在科学机器学习应用中表现出优越性能，同时保持架构无关性和计算效率。

Abstract: Effectively leveraging prior knowledge of a system's physics is crucial for
applications of machine learning to scientific domains. Previous approaches
mostly focused on incorporating physical insights at the architectural level.
In this paper, we propose a framework to leverage physical information directly
into the loss function for prediction and generative modeling tasks on systems
like molecules and spins. We derive energy loss functions assuming that each
data sample is in thermal equilibrium with respect to an approximate energy
landscape. By using the reverse KL divergence with a Boltzmann distribution
around the data, we obtain the loss as an energy difference between the data
and the model predictions. This perspective also recasts traditional objectives
like MSE as energy-based, but with a physically meaningless energy. In
contrast, our formulation yields physically grounded loss functions with
gradients that better align with valid configurations, while being
architecture-agnostic and computationally efficient. The energy loss functions
also inherently respect physical symmetries. We demonstrate our approach on
molecular generation and spin ground-state prediction and report significant
improvements over baselines.

</details>


### [62] [Can LLMs subtract numbers?](https://arxiv.org/abs/2511.02795)
*Mayank Jobanputra,Nils Philipp Walter,Maitrey Mehta,Blerta Veseli,Evan Parker Kelly Chapple,Yifan Wang,Sneha Chetani,Ellie Pavlick,Antonio Vergari,Vera Demberg*

Main category: cs.LG

TL;DR: 该论文系统研究了大型语言模型在减法运算中的表现，发现减法准确率远低于加法，主要问题出现在a<b时模型会忽略负号，但指令调优可以显著改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 尽管现有基准测试主要关注加法和乘法，但减法作为非交换运算具有结构性差异，却很少受到关注，需要系统研究LLMs在减法运算中的表现。

Method: 评估了来自四个家族的八个预训练LLMs在加减法问题上的表现，通过探测分析了解模型内部是否编码了负号信息，并测试了少样本学习和指令调优技术。

Result: 减法准确率大幅落后于加法，错误主要集中在a<b的情况，模型经常产生正确数值但忽略负号。指令调优模型在生成负号方面达到接近完美的准确率。

Conclusion: 研究揭示了LLMs在减法运算中的局限性，特别是负号生成问题，但通过指令调优可以显著恢复模型的算术能力。

Abstract: We present a systematic study of subtraction in large language models (LLMs).
While prior benchmarks emphasize addition and multiplication, subtraction has
received comparatively little attention despite being structurally distinct as
a non-commutative operation. We evaluate eight pretrained LLMs spanning four
families on addition and subtraction problems. Our experiments reveal that
subtraction accuracy lags behind addition by a wide margin. We find that the
errors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs
frequently produce the correct magnitude but omit the negative sign. Probing
analyses show that LLMs internally encode whether results should be negative,
yet this information is often not reflected in generated outputs. We further
test well-known techniques such as few-shot learning and instruction-tuning to
see if they can improve the LLMs' performance. Our results suggest that while
few-shot prompting yields modest gains, the instruction-tuned models achieve
near-perfect accuracies in generating the negative sign. Together, these
findings provide a clearer characterization of the limitations and
recoverability of LLMs' arithmetic capabilities in subtraction.

</details>


### [63] [Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science](https://arxiv.org/abs/2511.02092)
*Kishansingh Rajput,Malachi Schram,Brian Sammuli,Sen Lin*

Main category: cs.LG

TL;DR: 本文提出了一种基于不确定性引导的在线集成方法，用于处理聚变装置中非平稳数据流的预测问题，显著提高了托罗伊德场线圈偏转预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 聚变数据呈现非平稳特性，存在分布漂移，传统机器学习模型假设数据分布平稳，在遇到非平稳数据流时性能下降。在线学习技术在其他领域已有应用，但在聚变应用中尚未充分探索。

Method: 采用在线学习技术持续适应漂移数据流，并提出不确定性引导的在线集成方法。利用深度高斯过程近似技术进行校准的不确定性估计，然后使用不确定性值指导元算法，基于在不同历史数据范围上训练的多个学习器生成预测。

Result: 在线学习相比静态模型将误差降低了80%。在线集成和不确定性引导的在线集成分别比标准单模型在线学习进一步降低了约6%和10%的预测误差。

Conclusion: 在线学习对于维持机器学习模型在非平稳聚变数据流中的性能至关重要，提出的不确定性引导在线集成方法能进一步改善性能，同时为决策者提供带有不确定性估计的预测。

Abstract: Machine Learning (ML) is poised to play a pivotal role in the development and
operation of next-generation fusion devices. Fusion data shows non-stationary
behavior with distribution drifts, resulted by both experimental evolution and
machine wear-and-tear. ML models assume stationary distribution and fail to
maintain performance when encountered with such non-stationary data streams.
Online learning techniques have been leveraged in other domains, however it has
been largely unexplored for fusion applications. In this paper, we present an
application of online learning to continuously adapt to drifting data stream
for prediction of Toroidal Field (TF) coils deflection at the DIII-D fusion
facility. The results demonstrate that online learning is critical to maintain
ML model performance and reduces error by 80% compared to a static model.
Moreover, traditional online learning can suffer from short-term performance
degradation as ground truth is not available before making the predictions. As
such, we propose an uncertainty guided online ensemble method to further
improve the performance. The Deep Gaussian Process Approximation (DGPA)
technique is leveraged for calibrated uncertainty estimation and the
uncertainty values are then used to guide a meta-algorithm that produces
predictions based on an ensemble of learners trained on different horizon of
historical data. The DGPA also provides uncertainty estimation along with the
predictions for decision makers. The online ensemble and the proposed
uncertainty guided online ensemble reduces predictions error by about 6%, and
10% respectively over standard single model based online learning.

</details>


### [64] [Geometric Data Valuation via Leverage Scores](https://arxiv.org/abs/2511.02100)
*Rodrigo Mendoza-Smith*

Main category: cs.LG

TL;DR: 本文提出了一种基于统计杠杆得分的几何替代方法，用于数据估值，解决了Shapley值计算复杂度高的问题，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: Shapley数据估值虽然提供了原则性的数据重要性评估框架，但其组合性质导致计算复杂度极高，难以在大规模数据上应用。

Method: 使用统计杠杆得分作为几何替代方法，量化数据点在表示空间中的结构影响，通过测量其对数据集张成空间的扩展程度和对训练问题有效维度的贡献。

Result: 提出的得分满足Shapley估值的虚拟性、效率和对称性公理，岭杠杆得分具有严格正的边际增益，与经典A-和D-最优设计准则自然连接。基于杠杆采样的子集训练得到的模型参数和预测风险与全数据最优解的差距在O(ε)范围内。

Conclusion: 统计杠杆得分提供了一种计算高效的数据估值方法，在主动学习实验中，岭杠杆采样无需梯度或反向传播就能超越标准基线方法，建立了数据估值与下游决策质量之间的严格联系。

Abstract: Shapley data valuation provides a principled, axiomatic framework for
assigning importance to individual datapoints, and has gained traction in
dataset curation, pruning, and pricing. However, it is a combinatorial measure
that requires evaluating marginal utility across all subsets of the data,
making it computationally infeasible at scale. We propose a geometric
alternative based on statistical leverage scores, which quantify each
datapoint's structural influence in the representation space by measuring how
much it extends the span of the dataset and contributes to the effective
dimensionality of the training problem. We show that our scores satisfy the
dummy, efficiency, and symmetry axioms of Shapley valuation and that extending
them to \emph{ridge leverage scores} yields strictly positive marginal gains
that connect naturally to classical A- and D-optimal design criteria. We
further show that training on a leverage-sampled subset produces a model whose
parameters and predictive risk are within $O(\varepsilon)$ of the full-data
optimum, thereby providing a rigorous link between data valuation and
downstream decision quality. Finally, we conduct an active learning experiment
in which we empirically demonstrate that ridge-leverage sampling outperforms
standard baselines without requiring access gradients or backward passes.

</details>


### [65] [Measuring the Intrinsic Dimension of Earth Representations](https://arxiv.org/abs/2511.02101)
*Arjun Rao,Marc Rußwurm,Konstantin Klemmer,Esther Rolf*

Main category: cs.LG

TL;DR: 本文首次研究地理隐式神经表示（INRs）的内在维度，发现其值在2到10之间，与下游任务性能相关，可作为无监督评估和模型选择的指标。


<details>
  <summary>Details</summary>
Motivation: 地理INRs旨在将地球数据压缩为紧凑的学习友好表示，但缺乏对这些表示信息含量的理解，特别是信息在何处集中。

Method: 通过分析内在维度（衡量数据集局部变异性所需自由度数量），研究不同环境维度（256-512）的地理INRs，考察空间分辨率和输入模态变化的影响。

Result: 地理INRs的内在维度在2到10之间，对预训练中的空间分辨率和输入模态变化敏感，且与下游任务性能相关，能捕捉空间伪影。

Conclusion: 内在维度提供了架构无关、无标签的信息含量度量，可用于INRs的无监督评估、模型选择和预训练设计。

Abstract: Within the context of representation learning for Earth observation,
geographic Implicit Neural Representations (INRs) embed low-dimensional
location inputs (longitude, latitude) into high-dimensional embeddings, through
models trained on geo-referenced satellite, image or text data. Despite the
common aim of geographic INRs to distill Earth's data into compact,
learning-friendly representations, we lack an understanding of how much
information is contained in these Earth representations, and where that
information is concentrated. The intrinsic dimension of a dataset measures the
number of degrees of freedom required to capture its local variability,
regardless of the ambient high-dimensional space in which it is embedded. This
work provides the first study of the intrinsic dimensionality of geographic
INRs. Analyzing INRs with ambient dimension between 256 and 512, we find that
their intrinsic dimensions fall roughly between 2 and 10 and are sensitive to
changing spatial resolution and input modalities during INR pre-training.
Furthermore, we show that the intrinsic dimension of a geographic INR
correlates with downstream task performance and can capture spatial artifacts,
facilitating model evaluation and diagnostics. More broadly, our work offers an
architecture-agnostic, label-free metric of information content that can enable
unsupervised evaluation, model selection, and pre-training design across INRs.

</details>


### [66] [Matrix Sensing with Kernel Optimal Loss: Robustness and Optimization Landscape](https://arxiv.org/abs/2511.02122)
*Xinyuan Song,Jiaye Teng,Ziye Ma*

Main category: cs.LG

TL;DR: 本文研究了非凸优化问题中损失函数选择对鲁棒性和优化景观的影响，通过噪声矩阵感知任务，提出了一种基于非参数回归的鲁棒损失函数，该函数在非高斯或重尾噪声下保持稳定。


<details>
  <summary>Details</summary>
Motivation: 传统回归任务中常用的均方误差损失在非高斯或重尾噪声下不可靠，需要开发更鲁棒的损失函数来提高机器学习任务在复杂噪声环境下的稳定性。

Method: 采用基于非参数回归的鲁棒损失函数，使用核密度估计残差密度并最大化估计的对数似然，该公式在高斯误差下与MSE损失一致，但在更一般设置下保持稳定。

Result: 通过理论和实证分析表明，新损失函数在处理大噪声和保持对各种噪声分布的鲁棒性方面表现出色，同时分析了限制等距性质常数的上界以消除虚假局部最小值。

Conclusion: 这项工作通过简单地改变损失函数，基于直观且广泛适用的分析框架，为增强机器学习任务的鲁棒性提供了初步见解。

Abstract: In this paper we study how the choice of loss functions of non-convex
optimization problems affects their robustness and optimization landscape,
through the study of noisy matrix sensing. In traditional regression tasks,
mean squared error (MSE) loss is a common choice, but it can be unreliable for
non-Gaussian or heavy-tailed noise. To address this issue, we adopt a robust
loss based on nonparametric regression, which uses a kernel-based estimate of
the residual density and maximizes the estimated log-likelihood. This robust
formulation coincides with the MSE loss under Gaussian errors but remains
stable under more general settings. We further examine how this robust loss
reshapes the optimization landscape by analyzing the upper-bound of restricted
isometry property (RIP) constants for spurious local minima to disappear.
Through theoretical and empirical analysis, we show that this new loss excels
at handling large noise and remains robust across diverse noise distributions.
This work offers initial insights into enhancing the robustness of machine
learning tasks through simply changing the loss, guided by an intuitive and
broadly applicable analytical framework.

</details>


### [67] [Variance-Aware Feel-Good Thompson Sampling for Contextual Bandits](https://arxiv.org/abs/2511.02123)
*Xuheng Li,Quanquan Gu*

Main category: cs.LG

TL;DR: 本文提出了FGTS-VA算法，一种方差感知的Thompson Sampling方法，用于具有一般奖励函数的上下文赌博机问题，并获得了最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 当前大多数方差相关遗憾界研究集中在UCB类算法上，而基于采样的算法如Thompson Sampling研究较少。现有的LinVDTS算法仅限于线性奖励函数且遗憾界在模型维度上不是最优的。

Method: 提出了FGTS-VA算法，扩展了Feel-good Thompson Sampling中使用的解耦系数技术，用新的解耦系数dc来反映模型空间的复杂性。

Result: FGTS-VA实现了遗憾界$\tilde{O}(\sqrt{\mathrm{dc}\cdot\log|\mathcal{F}|\sum_{t=1}^T\sigma_t^2}+\mathrm{dc})$，其中$|\mathcal{F}|$是模型空间大小，$T$是总轮数，$\sigma_t^2$是第t轮的噪声子高斯范数。

Conclusion: 在上下文线性赌博机设置中，FGTS-VA的遗憾界与使用加权线性回归的UCB类算法相匹配，为基于采样的算法提供了最优的方差相关遗憾界。

Abstract: Variance-dependent regret bounds have received increasing attention in recent
studies on contextual bandits. However, most of these studies are focused on
upper confidence bound (UCB)-based bandit algorithms, while sampling based
bandit algorithms such as Thompson sampling are still understudied. The only
exception is the LinVDTS algorithm (Xu et al., 2023), which is limited to
linear reward function and its regret bound is not optimal with respect to the
model dimension. In this paper, we present FGTSVA, a variance-aware Thompson
Sampling algorithm for contextual bandits with general reward function with
optimal regret bound. At the core of our analysis is an extension of the
decoupling coefficient, a technique commonly used in the analysis of Feel-good
Thompson sampling (FGTS) that reflects the complexity of the model space. With
the new decoupling coefficient denoted by $\mathrm{dc}$, FGTS-VA achieves the
regret of
$\tilde{O}(\sqrt{\mathrm{dc}\cdot\log|\mathcal{F}|\sum_{t=1}^T\sigma_t^2}+\mathrm{dc})$,
where $|\mathcal{F}|$ is the size of the model space, $T$ is the total number
of rounds, and $\sigma_t^2$ is the subgaussian norm of the noise (e.g.,
variance when the noise is Gaussian) at round $t$. In the setting of contextual
linear bandits, the regret bound of FGTSVA matches that of UCB-based algorithms
using weighted linear regression (Zhou and Gu, 2022).

</details>


### [68] [Disentangling Causal Substructures for Interpretable and Generalizable Drug Synergy Prediction](https://arxiv.org/abs/2511.02146)
*Yi Luo,Haochen Zhao,Xiao Liang,Yiwei Liu,Yuye Zhang,Xinyu Li,Jianxin Wang*

Main category: cs.LG

TL;DR: CausalDDS是一个新颖的药物协同作用预测框架，通过将药物分子分解为因果和伪子结构，利用因果子结构表示来预测药物协同作用，提高模型的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有药物协同作用预测方法通常作为黑盒预测器，主要依赖药物特征与结果之间的统计相关性，缺乏对因果关系的理解。

Method: CausalDDS将药物分子分解为因果和伪子结构，采用条件干预机制（干预基于配对分子结构），并引入基于充分性和独立性原则的新优化目标。

Result: 广泛实验表明该方法优于基线模型，特别是在冷启动和分布外设置下。CausalDDS能有效识别药物协同作用的关键子结构。

Conclusion: CausalDDS作为预测药物协同作用和促进药物发现的实用工具具有巨大潜力，为理解药物组合在分子水平上的作用机制提供了清晰见解。

Abstract: Drug synergy prediction is a critical task in the development of effective
combination therapies for complex diseases, including cancer. Although existing
methods have shown promising results, they often operate as black-box
predictors that rely predominantly on statistical correlations between drug
characteristics and results. To address this limitation, we propose CausalDDS,
a novel framework that disentangles drug molecules into causal and spurious
substructures, utilizing the causal substructure representations for predicting
drug synergy. By focusing on causal sub-structures, CausalDDS effectively
mitigates the impact of redundant features introduced by spurious
substructures, enhancing the accuracy and interpretability of the model. In
addition, CausalDDS employs a conditional intervention mechanism, where
interventions are conditioned on paired molecular structures, and introduces a
novel optimization objective guided by the principles of sufficiency and
independence. Extensive experiments demonstrate that our method outperforms
baseline models, particularly in cold start and out-of-distribution settings.
Besides, CausalDDS effectively identifies key substructures underlying drug
synergy, providing clear insights into how drug combinations work at the
molecular level. These results underscore the potential of CausalDDS as a
practical tool for predicting drug synergy and facilitating drug discovery.

</details>


### [69] [CFL: On the Use of Characteristic Function Loss for Domain Alignment in Machine Learning](https://arxiv.org/abs/2511.02148)
*Abdullah Almansour,Ozan Tonguz*

Main category: cs.LG

TL;DR: 本文提出使用特征函数作为频域方法来衡量高维空间中的分布偏移，为分布偏移问题和领域自适应提供了一种强大的替代方案。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在现实世界中部署时经常因分布偏移问题而表现不佳，特别是在高风险应用中可能导致灾难性后果。传统统计方法在量化分布偏移方面存在局限，需要更有效的解决方案。

Method: 采用特征函数作为频域方法，用于测量高维空间中的分布偏移和领域自适应。

Result: 特征函数方法被证明是衡量高维空间分布偏移和领域自适应的强大替代方案。

Conclusion: 特征函数作为频域方法为分布偏移问题提供了有效的解决方案，特别是在高维空间和领域自适应应用中具有显著优势。

Abstract: Machine Learning (ML) models are extensively used in various applications due
to their significant advantages over traditional learning methods. However, the
developed ML models often underperform when deployed in the real world due to
the well-known distribution shift problem. This problem can lead to a
catastrophic outcomes when these decision-making systems have to operate in
high-risk applications. Many researchers have previously studied this problem
in ML, known as distribution shift problem, using statistical techniques (such
as Kullback-Leibler, Kolmogorov-Smirnov Test, Wasserstein distance, etc.) to
quantify the distribution shift. In this letter, we show that using
Characteristic Function (CF) as a frequency domain approach is a powerful
alternative for measuring the distribution shift in high-dimensional space and
for domain adaptation.

</details>


### [70] [ProtoTSNet: Interpretable Multivariate Time Series Classification With Prototypical Parts](https://arxiv.org/abs/2511.02152)
*Bartłomiej Małkus,Szymon Bobek,Grzegorz J. Nalepa*

Main category: cs.LG

TL;DR: ProtoTSNet是一种可解释的多变量时间序列分类方法，通过改进ProtoPNet架构来捕获动态模式并处理特征重要性变化，在30个UEA数据集上表现优于其他可解释方法。


<details>
  <summary>Details</summary>
Motivation: 在工业和医疗等关键领域，时间序列数据很常见，需要既准确又可解释的算法，因为这些领域的决策具有重大影响。

Method: 使用改进的卷积编码器，采用分组卷积，可作为自编码器预训练，旨在保留和量化特征重要性，专门针对时间序列分析的独特挑战。

Result: 在30个多变量时间序列数据集上的评估显示，该方法在前瞻可解释方法中表现最佳，与非可解释和后验可解释方法保持竞争力。

Conclusion: ProtoTSNet为领域专家提供了可访问的可解释结果，在保持准确性的同时实现了可解释性。

Abstract: Time series data is one of the most popular data modalities in critical
domains such as industry and medicine. The demand for algorithms that not only
exhibit high accuracy but also offer interpretability is crucial in such
fields, as decisions made there bear significant consequences. In this paper,
we present ProtoTSNet, a novel approach to interpretable classification of
multivariate time series data, through substantial enhancements to the
ProtoPNet architecture. Our method is tailored to overcome the unique
challenges of time series analysis, including capturing dynamic patterns and
handling varying feature significance. Central to our innovation is a modified
convolutional encoder utilizing group convolutions, pre-trainable as part of an
autoencoder and designed to preserve and quantify feature importance. We
evaluated our model on 30 multivariate time series datasets from the UEA
archive, comparing our approach with existing explainable methods as well as
non-explainable baselines. Through comprehensive evaluation and ablation
studies, we demonstrate that our approach achieves the best performance among
ante-hoc explainable methods while maintaining competitive performance with
non-explainable and post-hoc explainable approaches, providing interpretable
results accessible to domain experts.

</details>


### [71] [OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning](https://arxiv.org/abs/2511.02205)
*Kevin Valencia,Thilina Balasooriya,Xihaier Luo,Shinjae Yoo,David Keetae Park*

Main category: cs.LG

TL;DR: OmniField是一个多模态时空学习框架，能够处理稀疏、不规则、有噪声的跨模态相关数据，并适应训练和测试时任意可用的模态子集。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界实验数据中多模态时空学习面临的两个挑战：模态内测量稀疏、不规则、有噪声但跨模态相关；可用模态集在时空上变化，需要模型能够适应任意子集。

Method: 提出连续性感知框架，学习以可用模态为条件的连续神经场，通过多模态串扰块架构和迭代跨模态精炼在解码器前对齐信号，实现统一的重建、插值、预测和跨模态预测。

Result: 广泛评估显示OmniField持续优于八个强大的多模态时空基线方法。在重度模拟传感器噪声下，性能仍接近干净输入水平，表现出对损坏测量的鲁棒性。

Conclusion: OmniField框架能够有效处理多模态时空数据中的稀疏性、噪声和模态变化问题，具有强大的鲁棒性和性能优势。

Abstract: Multimodal spatiotemporal learning on real-world experimental data is
constrained by two challenges: within-modality measurements are sparse,
irregular, and noisy (QA/QC artifacts) but cross-modally correlated; the set of
available modalities varies across space and time, shrinking the usable record
unless models can adapt to arbitrary subsets at train and test time. We propose
OmniField, a continuity-aware framework that learns a continuous neural field
conditioned on available modalities and iteratively fuses cross-modal context.
A multimodal crosstalk block architecture paired with iterative cross-modal
refinement aligns signals prior to the decoder, enabling unified
reconstruction, interpolation, forecasting, and cross-modal prediction without
gridding or surrogate preprocessing. Extensive evaluations show that OmniField
consistently outperforms eight strong multimodal spatiotemporal baselines.
Under heavy simulated sensor noise, performance remains close to clean-input
levels, highlighting robustness to corrupted measurements.

</details>


### [72] [Learning Interactive World Model for Object-Centric Reinforcement Learning](https://arxiv.org/abs/2511.02225)
*Fan Feng,Phillip Lippe,Sara Magliacane*

Main category: cs.LG

TL;DR: FIOC-WM是一个统一框架，学习对象及其交互的结构化表示，通过解耦和模块化的对象交互表示来捕获环境动态，提高策略学习的样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大多数基于对象的RL方法按单个对象分解状态，而将交互隐式处理。需要显式学习对象交互的结构化表示，以获得更鲁棒和可迁移的策略。

Method: 首先从像素中学习对象中心潜在表示和交互结构，利用预训练视觉编码器。然后世界模型将任务分解为可组合的交互原语，并训练分层策略：高层选择交互类型和顺序，低层执行交互。

Result: 在模拟机器人和具身AI基准测试中，FIOC-WM相比世界模型基线提高了策略学习的样本效率和泛化能力。

Conclusion: 显式、模块化的交互学习对于鲁棒控制至关重要。

Abstract: Agents that understand objects and their interactions can learn policies that
are more robust and transferable. However, most object-centric RL methods
factor state by individual objects while leaving interactions implicit. We
introduce the Factored Interactive Object-Centric World Model (FIOC-WM), a
unified framework that learns structured representations of both objects and
their interactions within a world model. FIOC-WM captures environment dynamics
with disentangled and modular representations of object interactions, improving
sample efficiency and generalization for policy learning. Concretely, FIOC-WM
first learns object-centric latents and an interaction structure directly from
pixels, leveraging pre-trained vision encoders. The learned world model then
decomposes tasks into composable interaction primitives, and a hierarchical
policy is trained on top: a high level selects the type and order of
interactions, while a low level executes them. On simulated robotic and
embodied-AI benchmarks, FIOC-WM improves policy-learning sample efficiency and
generalization over world-model baselines, indicating that explicit, modular
interaction learning is crucial for robust control.

</details>


### [73] [Opportunistic Expert Activation: Batch-Aware Expert Routing for Faster Decode Without Retraining](https://arxiv.org/abs/2511.02237)
*Costin-Andrei Oncescu,Qingyang Wu,Wai Tong Chung,Robert Wu,Bryan Gopal,Junxiong Wang,Tri Dao,Ben Athiwaratkun*

Main category: cs.LG

TL;DR: 提出了一种动态重路由token到专家映射的框架，通过降低激活专家数量来减少MoE模型的解码延迟，同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: MoE模型在自回归生成时容易进入内存受限状态，因为平均专家负载增长缓慢，导致延迟主要由激活专家数量决定。

Method: 使用批量感知路由方法，让token共享同一批次中其他token已加载到内存的关键专家，动态重路由token到专家的映射。

Result: 在Qwen3-30B和Qwen3-235B模型上，批量大小为16时，MoE层解码延迟分别降低39%和15%，且没有统计显著性的精度损失。

Conclusion: 提出的动态重路由框架能有效降低MoE模型的解码延迟，同时保持模型质量，为MoE模型的实际部署提供了实用的优化方案。

Abstract: An increasing number of LLMs employ Mixture-of-Experts (MoE) architectures
where the feed-forward layer is replaced by a pool of experts and each token
only activates a small subset of them. During autoregressive generation, these
models often enter a memory-bound regime even for moderate batch sizes because
the average expert load grows more slowly than in an equivalent dense
feedforward layer. Consequently, MoE latency is governed by the number of
activated experts. We introduce a framework for dynamically re-routing
token-to-expert mapping to lower this number (and thus, the decode latency)
while preserving a comparable quality. Our best results use a batch-aware
routing that works by having tokens piggyback experts that have already been
loaded into memory due to being crucial to other tokens within the same batch.
Empirically, we evaluate our method on the Qwen3-30B and Qwen3-235B models with
a batch size of $16$. Without any statistically significant loss in accuracy,
our approach achieves latency reductions of $39\%$ and $15\%$ in the MoE layer
decode latency, respectively.

</details>


### [74] [Probabilistic Graph Cuts](https://arxiv.org/abs/2511.02272)
*Ayoub Ghriss*

Main category: cs.LG

TL;DR: 提出了一个统一的概率框架，为包括归一化割在内的多种图割方法提供可微分的替代方案，通过积分表示和高斯超几何函数提供紧致的分析上界，支持端到端和在线学习。


<details>
  <summary>Details</summary>
Motivation: 现有的概率松弛图割方法主要集中于RatioCut，缺乏通用保证和原则性梯度，需要更统一的框架来覆盖更广泛的图割类型。

Method: 使用积分表示和高斯超几何函数构建概率框架，为多种图割提供紧致的分析上界，支持闭式前向和反向传播。

Result: 开发了一个统一、数值稳定的概率框架，支持可扩展、可微分的图划分，适用于多种聚类和对比学习目标。

Conclusion: 该框架为图划分提供了严谨、数值稳定的基础，能够处理广泛的聚类和对比学习目标，支持端到端学习。

Abstract: Probabilistic relaxations of graph cuts offer a differentiable alternative to
spectral clustering, enabling end-to-end and online learning without
eigendecompositions, yet prior work centered on RatioCut and lacked general
guarantees and principled gradients. We present a unified probabilistic
framework that covers a wide class of cuts, including Normalized Cut. Our
framework provides tight analytic upper bounds on expected discrete cuts via
integral representations and Gauss hypergeometric functions with closed-form
forward and backward. Together, these results deliver a rigorous, numerically
stable foundation for scalable, differentiable graph partitioning covering a
wide range of clustering and contrastive learning objectives.

</details>


### [75] [Gradient-Variation Online Adaptivity for Accelerated Optimization with Hölder Smoothness](https://arxiv.org/abs/2511.02276)
*Yuheng Zhao,Yu-Hu Yan,Kfir Yehuda Levy,Peng Zhao*

Main category: cs.LG

TL;DR: 本文研究了Hölder光滑函数的在线学习问题，并将其与离线优化联系起来。通过设计自适应梯度变化在线学习算法，实现了在光滑和非光滑情况下的最优遗憾保证，并进一步提出了首个通用的离线加速优化方法。


<details>
  <summary>Details</summary>
Motivation: 光滑性在离线优化加速和在线学习的梯度变化遗憾最小化中都很重要。本文旨在探索Hölder光滑函数（包含光滑和非光滑函数）的在线学习问题，并研究其对离线优化的影响。

Method: 设计了自适应梯度变化在线学习算法，无需预先知道Hölder光滑参数；通过在线到批次的转换获得随机凸优化的通用方法；结合在线自适应性和基于检测的猜测检查程序，提出通用离线方法。

Result: 在线学习算法在光滑和非光滑情况下都能实现最优遗憾保证；通过在线到批次转换获得Hölder光滑条件下随机凸优化的最优通用方法；首次提出通用离线方法，在光滑情况下实现加速收敛，在非光滑情况下保持近似最优收敛。

Conclusion: 本文成功建立了在线学习和离线优化之间的桥梁，提出了自适应算法和通用方法，在Hölder光滑条件下实现了最优性能，特别是在离线强凸优化中首次实现了通用加速方法。

Abstract: Smoothness is known to be crucial for acceleration in offline optimization,
and for gradient-variation regret minimization in online learning.
Interestingly, these two problems are actually closely connected -- accelerated
optimization can be understood through the lens of gradient-variation online
learning. In this paper, we investigate online learning with H\"older smooth
functions, a general class encompassing both smooth and non-smooth (Lipschitz)
functions, and explore its implications for offline optimization. For
(strongly) convex online functions, we design the corresponding
gradient-variation online learning algorithm whose regret smoothly interpolates
between the optimal guarantees in smooth and non-smooth regimes. Notably, our
algorithms do not require prior knowledge of the H\"older smoothness parameter,
exhibiting strong adaptivity over existing methods. Through online-to-batch
conversion, this gradient-variation online adaptivity yields an optimal
universal method for stochastic convex optimization under H\"older smoothness.
However, achieving universality in offline strongly convex optimization is more
challenging. We address this by integrating online adaptivity with a
detection-based guess-and-check procedure, which, for the first time, yields a
universal offline method that achieves accelerated convergence in the smooth
regime while maintaining near-optimal convergence in the non-smooth one.

</details>


### [76] [Federated Quantum Kernel Learning for Anomaly Detection in Multivariate IoT Time-Series](https://arxiv.org/abs/2511.02301)
*Kuan-Cheng Chen,Samuel Yen-Chi Chen,Chen-Yu Liu,Kin K. Leung*

Main category: cs.LG

TL;DR: 本文提出了一种联邦量子核学习框架，将量子特征映射与联邦聚合相结合，用于工业物联网中的分布式、隐私保护异常检测。


<details>
  <summary>Details</summary>
Motivation: 工业物联网系统的快速增长给高维多变量时间序列中的异常检测带来了新挑战，其中隐私、可扩展性和通信效率至关重要。传统联邦学习方法虽然通过去中心化训练缓解了隐私问题，但在处理高度非线性决策边界和不平衡异常分布方面存在困难。

Method: 提出联邦量子核学习框架，量子边缘节点使用参数化量子电路本地计算压缩核统计量，仅与中央服务器共享这些摘要信息。中央服务器构建全局Gram矩阵并训练决策函数。

Result: 在合成工业物联网基准测试上的实验结果表明，与经典联邦基线相比，FQKL在捕捉复杂时间相关性方面具有更好的泛化能力，同时显著降低了通信开销。

Conclusion: 这项工作突出了量子核在联邦设置中的潜力，为下一代物联网基础设施的可扩展、鲁棒和量子增强智能开辟了道路。

Abstract: The rapid growth of industrial Internet of Things (IIoT) systems has created
new challenges for anomaly detection in high-dimensional, multivariate
time-series, where privacy, scalability, and communication efficiency are
critical. Classical federated learning approaches mitigate privacy concerns by
enabling decentralized training, but they often struggle with highly non-linear
decision boundaries and imbalanced anomaly distributions. To address this gap,
we propose a Federated Quantum Kernel Learning (FQKL) framework that integrates
quantum feature maps with federated aggregation to enable distributed,
privacy-preserving anomaly detection across heterogeneous IoT networks. In our
design, quantum edge nodes locally compute compressed kernel statistics using
parameterized quantum circuits and share only these summaries with a central
server, which constructs a global Gram matrix and trains a decision function
(e.g., Fed-QSVM). Experimental results on synthetic IIoT benchmarks demonstrate
that FQKL achieves superior generalization in capturing complex temporal
correlations compared to classical federated baselines, while significantly
reducing communication overhead. This work highlights the promise of quantum
kernels in federated settings, advancing the path toward scalable, robust, and
quantum-enhanced intelligence for next-generation IoT infrastructures.

</details>


### [77] [FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error](https://arxiv.org/abs/2511.02302)
*Fengjuan Wang,Zhiyi Su,Xingzhu Hu,Cheng Wang,Mou Sun*

Main category: cs.LG

TL;DR: FP8-Flow-MoE是一种针对大型MoE模型的FP8训练方法，通过量化一致的数据流和融合算子，显著提升训练效率并减少内存使用，同时保持收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型训练计算和内存需求巨大，低精度训练虽然能加速计算和减少内存占用，但现有FP8实现仍依赖BF16数据流，频繁的量化-反量化转换削弱了FP8的理论效率。

Method: 提出FP8-Flow-MoE方法，采用量化一致的FP8中心数据流，包含缩放感知的转置操作和融合的FP8算子，将显式转换操作从12个减少到2个。

Result: 在671B参数的MoE模型上评估，相比BF16和原始FP8基线，吞吐量提升高达21%，每个GPU内存使用减少16.5GB，同时保持稳定的收敛性。

Conclusion: FP8-Flow-MoE提供了一种即插即用的FP8训练方案，兼容TransformerEngine和Megatron-LM，将开源发布，为大型MoE模型训练提供了高效的解决方案。

Abstract: Training large Mixture-of-Experts (MoE) models remains computationally
prohibitive due to their extreme compute and memory demands. Although
low-precision training promises to accelerate computation and reduce memory
footprint, existing implementations still rely on BF16-dominated dataflows with
frequent quantize-dequantize (Q/DQ) conversions. These redundant casts erode
much of FP8's theoretical efficiency. However, naively removing these casts by
keeping dataflows entirely in FP8 introduces double quantization error: tensors
quantized along different dimensions accumulate inconsistent scaling factors,
degrading numerical stability.
  We propose FP8-Flow-MoE, an FP8 training recipe featuring a
quantization-consistent FP8-centric dataflow with a scaling-aware transpose and
fused FP8 operators that streamline computation and eliminate explicit cast
operations from 12 to 2. Evaluations on a 671B-parameter MoE model demonstrate
up to 21\% higher throughput and 16.5 GB lower memory usage per GPU compared to
BF16 and na\"ive FP8 baselines, while maintaining stable convergence. We
provide a plug-and-play FP8 recipe compatible with TransformerEngine and
Megatron-LM, which will be open-sourced soon.

</details>


### [78] [The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute](https://arxiv.org/abs/2511.02309)
*Aman Sharma,Paras Chopra*

Main category: cs.LG

TL;DR: 本文研究发现，在同等计算资源下，顺序推理链（迭代优化）比并行自一致性解码更有效，在95.6%的配置中表现更优，准确率提升最高达46.7%。


<details>
  <summary>Details</summary>
Motivation: 重新审视语言模型推理中的测试时扩展策略，探究在相同token预算和计算资源下，并行运行多个独立推理链与运行较少但迭代优化的顺序推理链哪种更优。

Method: 在5个最先进开源模型和3个挑战性推理基准上进行全面评估，比较并行自一致性与顺序推理链的性能，并引入逆熵加权投票方法来提升顺序推理的准确性。

Result: 顺序扩展策略在95.6%的配置中优于并行自一致性范式，准确率提升最高达46.7%。逆熵加权投票进一步提高了顺序推理的成功率。

Conclusion: 研究结果从根本上挑战了自Wang等人（2022）以来主导测试时扩展的并行推理正统观念，将顺序精化定位为现代LLM推理的稳健默认策略，需要在推理时优化方法上进行范式转变。

Abstract: We revisit test-time scaling for language model reasoning and ask a
fundamental question: at equal token budget and compute, is it better to run
multiple independent chains in parallel, or to run fewer chains that
iteratively refine through sequential steps? Through comprehensive evaluation
across 5 state-of-the-art open source models and 3 challenging reasoning
benchmarks, we find that sequential scaling where chains explicitly build upon
previous attempts consistently outperforms the dominant parallel
self-consistency paradigm in 95.6% of configurations with gains in accuracy
upto 46.7%. Further, we introduce inverse-entropy weighted voting, a novel
training-free method to further boost the accuracy of sequential scaling. By
weighing answers in proportion to the inverse entropy of their reasoning
chains, we increase our success rate over parallel majority and establish it as
the optimal test-time scaling strategy. Our findings fundamentally challenge
the parallel reasoning orthodoxy that has dominated test-time scaling since
Wang et al.'s self-consistency decoding (Wang et al., 2022), positioning
sequential refinement as the robust default for modern LLM reasoning and
necessitating a paradigm shift in how we approach inference-time optimization.

</details>


### [79] [Large-scale automatic carbon ion treatment planning for head and neck cancers via parallel multi-agent reinforcement learning](https://arxiv.org/abs/2511.02314)
*Jueye Zhang,Chao Yang,Youfang Lai,Kai-Wen Li,Wenting Yan,Yunzhou Xia,Haimei Zhang,Jingjing Zhou,Gen Yang,Chen Lin,Tian Li,Yibao Zhang*

Main category: cs.LG

TL;DR: 提出了一种可扩展的多智能体强化学习框架，用于并行调优头颈癌碳离子治疗中的45个治疗计划参数，生成与专家手动计划相当或更好的治疗方案。


<details>
  <summary>Details</summary>
Motivation: 头颈癌治疗计划制定困难，多个关键危及器官靠近复杂靶区。碳离子调强治疗具有优越的剂量适形性和危及器官保护能力，但由于相对生物有效性建模复杂，导致治疗计划参数调优过程缓慢、依赖经验且往往非最优。

Method: 使用集中训练分散执行的多智能体强化学习框架，结合Double DQN、Dueling DQN和循环编码技术。采用紧凑的历史DVH向量作为状态输入，线性动作到值变换映射小离散动作到统一参数调整，设计基于临床知识的绝对分段奖励函数。通过同步多进程工作系统与PHOENIX治疗计划系统交互进行并行优化。

Result: 在头颈癌数据集上，该方法同时调优45个参数，生成的治疗计划与专家手动计划相当或更好（相对计划评分：强化学习85.93±7.85% vs 手动85.02±6.92%），对5个危及器官有显著改善（p值<0.05）。

Conclusion: 该框架能够高效探索高维治疗计划参数空间，通过与治疗计划系统直接交互生成具有临床竞争力的碳离子调强治疗计划，显著改善了危及器官保护效果。

Abstract: Head-and-neck cancer (HNC) planning is difficult because multiple critical
organs-at-risk (OARs) are close to complex targets. Intensity-modulated
carbon-ion therapy (IMCT) offers superior dose conformity and OAR sparing but
remains slow due to relative biological effectiveness (RBE) modeling, leading
to laborious, experience-based, and often suboptimal tuning of many
treatment-planning parameters (TPPs). Recent deep learning (DL) methods are
limited by data bias and plan feasibility, while reinforcement learning (RL)
struggles to efficiently explore the exponentially large TPP search space. We
propose a scalable multi-agent RL (MARL) framework for parallel tuning of 45
TPPs in IMCT. It uses a centralized-training decentralized-execution (CTDE)
QMIX backbone with Double DQN, Dueling DQN, and recurrent encoding (DRQN) for
stable learning in a high-dimensional, non-stationary environment. To enhance
efficiency, we (1) use compact historical DVH vectors as state inputs, (2)
apply a linear action-to-value transform mapping small discrete actions to
uniform parameter adjustments, and (3) design an absolute, clinically informed
piecewise reward aligned with plan scores. A synchronous multi-process worker
system interfaces with the PHOENIX TPS for parallel optimization and
accelerated data collection. On a head-and-neck dataset (10 training, 10
testing), the method tuned 45 parameters simultaneously and produced plans
comparable to or better than expert manual ones (relative plan score: RL
$85.93\pm7.85%$ vs Manual $85.02\pm6.92%$), with significant (p-value $<$ 0.05)
improvements for five OARs. The framework efficiently explores high-dimensional
TPP spaces and generates clinically competitive IMCT plans through direct TPS
interaction, notably improving OAR sparing.

</details>


### [80] [RoME: Domain-Robust Mixture-of-Experts for MILP Solution Prediction across Domains](https://arxiv.org/abs/2511.02331)
*Tianle Pu,Zijie Geng,Haoyang Liu,Shixuan Liu,Jie Wang,Li Zeng,Chao Chen,Changjun Fan*

Main category: cs.LG

TL;DR: RoME是一个领域鲁棒的专家混合框架，用于跨领域预测混合整数线性规划（MILP）的解。通过动态路由问题实例到专门专家，并采用两级分布鲁棒优化策略，显著提升了模型在未见领域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法主要在单领域设置中开发和评估，限制了它们对未见问题分布的泛化能力，这阻碍了构建可扩展和通用的学习型求解器。

Method: RoME框架基于学习到的任务嵌入动态路由问题实例到专门专家，采用两级分布鲁棒优化策略：跨领域缓解全局偏移，领域内通过任务嵌入扰动增强局部鲁棒性。

Result: 在三个领域训练的单一RoME模型在五个不同领域评估时平均提升67.7%。在MIPLIB上的零样本测试显示，该模型能够在现有学习方法难以泛化的挑战性现实实例上提供可测量的性能增益。

Conclusion: 跨领域训练不仅增强了模型对未见领域的泛化能力，还通过鼓励模型捕捉更一般的固有组合模式来提升每个单独领域的性能。

Abstract: Mixed-Integer Linear Programming (MILP) is a fundamental and powerful
framework for modeling complex optimization problems across diverse domains.
Recently, learning-based methods have shown great promise in accelerating MILP
solvers by predicting high-quality solutions. However, most existing approaches
are developed and evaluated in single-domain settings, limiting their ability
to generalize to unseen problem distributions. This limitation poses a major
obstacle to building scalable and general-purpose learning-based solvers. To
address this challenge, we introduce RoME, a domain-Robust Mixture-of-Experts
framework for predicting MILP solutions across domains. RoME dynamically routes
problem instances to specialized experts based on learned task embeddings. The
model is trained using a two-level distributionally robust optimization
strategy: inter-domain to mitigate global shifts across domains, and
intra-domain to enhance local robustness by introducing perturbations on task
embeddings. We reveal that cross-domain training not only enhances the model's
generalization capability to unseen domains but also improves performance
within each individual domain by encouraging the model to capture more general
intrinsic combinatorial patterns. Specifically, a single RoME model trained on
three domains achieves an average improvement of 67.7% then evaluated on five
diverse domains. We further test the pretrained model on MIPLIB in a zero-shot
setting, demonstrating its ability to deliver measurable performance gains on
challenging real-world instances where existing learning-based approaches often
struggle to generalize.

</details>


### [81] [Reducing normalizing flow complexity for MCMC preconditioning](https://arxiv.org/abs/2511.02345)
*David Nabergoj,Erik Štrumbelj*

Main category: cs.LG

TL;DR: 本文提出了一种因子化预条件架构，将线性预条件器与条件归一化流结合，降低复杂度并提高对目标几何的适应性，在复杂分布和层次贝叶斯模型中显著提升了采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有非线性预条件器使用过参数化的归一化流会降低采样效率和拟合质量，且无法自适应目标分布结构，需要更高效的预条件方法。

Method: 提出因子化预条件架构：对近似高斯维度使用线性预条件器，对复杂维度使用条件归一化流，通过预热样本估计维度特性。

Result: 在两个复杂合成分布上获得更好的尾部样本，在稀疏逻辑回归后验上表现一致更好，在弱似然强漏斗几何的层次贝叶斯模型后验上获得更高有效样本量。

Conclusion: 该方法特别适用于数据有限的层次贝叶斯模型分析，可为神经MCMC设计提供理论和方法指导。

Abstract: Preconditioning is a key component of MCMC algorithms that improves sampling
efficiency by facilitating exploration of geometrically complex target
distributions through an invertible map. While linear preconditioners are often
sufficient for moderately complex target distributions, recent work has
explored nonlinear preconditioning with invertible neural networks as
components of normalizing flows (NFs). However, empirical and theoretical
studies show that overparameterized NF preconditioners can degrade sampling
efficiency and fit quality. Moreover, existing NF-based approaches do not adapt
their architectures to the target distribution. Related work outside of MCMC
similarly finds that suitably parameterized NFs can achieve comparable or
superior performance with substantially less training time or data. We propose
a factorized preconditioning architecture that reduces NF complexity by
combining a linear component with a conditional NF, improving adaptability to
target geometry. The linear preconditioner is applied to dimensions that are
approximately Gaussian, as estimated from warmup samples, while the conditional
NF models more complex dimensions. Our method yields significantly better tail
samples on two complex synthetic distributions and consistently better
performance on a sparse logistic regression posterior across varying likelihood
and prior strengths. It also achieves higher effective sample sizes on
hierarchical Bayesian model posteriors with weak likelihoods and strong funnel
geometries. This approach is particularly relevant for hierarchical Bayesian
model analyses with limited data and could inform current theoretical and
software strides in neural MCMC design.

</details>


### [82] [Human-Machine Ritual: Synergic Performance through Real-Time Motion Recognition](https://arxiv.org/abs/2511.02351)
*Zhuodi Cai,Ziyu Xu,Juan Pampin*

Main category: cs.LG

TL;DR: 提出了一种轻量级实时运动识别系统，通过可穿戴IMU传感器数据、MiniRocket时间序列分类和响应式多媒体控制，实现人机协同表演。


<details>
  <summary>Details</summary>
Motivation: 探索一种替代性的人机协作方法，通过将舞者特定动作映射到声音，保持表演身体的表达深度，同时利用机器学习进行观察和响应。

Method: 使用可穿戴IMU传感器收集数据，采用MiniRocket时间序列分类算法，结合多媒体控制技术，通过体感记忆和关联实现动作到声音的映射。

Result: 系统可靠支持高精度分类（延迟<50毫秒），为将舞蹈智能机器集成到创意、教育和现场表演环境提供了可复制的框架。

Conclusion: 该人本设计方法成功实现了低延迟高精度的运动识别，为人机协同表演提供了有效的技术解决方案。

Abstract: We introduce a lightweight, real-time motion recognition system that enables
synergic human-machine performance through wearable IMU sensor data, MiniRocket
time-series classification, and responsive multimedia control. By mapping
dancer-specific movement to sound through somatic memory and association, we
propose an alternative approach to human-machine collaboration, one that
preserves the expressive depth of the performing body while leveraging machine
learning for attentive observation and responsiveness. We demonstrate that this
human-centered design reliably supports high accuracy classification (<50 ms
latency), offering a replicable framework to integrate dance-literate machines
into creative, educational, and live performance contexts.

</details>


### [83] [A Spatially Informed Gaussian Process UCB Method for Decentralized Coverage Control](https://arxiv.org/abs/2511.02398)
*Gennaro Guidone,Luca Monegaglia,Elia Raimondi,Han Wang,Mattia Bianchi,Florian Dörfler*

Main category: cs.LG

TL;DR: 提出了一种基于高斯过程的未知空间环境覆盖控制去中心化算法，通过结合期望位置成本和方差探索项来平衡探索与利用，实现完全去中心化操作。


<details>
  <summary>Details</summary>
Motivation: 解决未知空间环境中的覆盖控制问题，需要在探索环境不确定性和利用已知信息之间取得平衡，同时实现完全去中心化的操作模式。

Method: 每个智能体通过最小化局部成本函数自主确定轨迹，该成本函数结合了期望位置成本和基于方差的探索项，灵感来自GP-UCB获取函数；使用贪婪选择策略定期更新诱导点，实现可扩展的在线高斯过程更新。

Result: 算法在仿真中表现出有效性，能够引导智能体同时朝向预测密度高和模型不确定性大的区域移动。

Conclusion: 提出的去中心化算法成功解决了未知空间环境中的覆盖控制问题，通过平衡探索与利用实现了高效的覆盖性能，且仅依赖局部观测和邻近智能体通信。

Abstract: We present a novel decentralized algorithm for coverage control in unknown
spatial environments modeled by Gaussian Processes (GPs). To trade-off between
exploration and exploitation, each agent autonomously determines its trajectory
by minimizing a local cost function. Inspired by the GP-UCB (Upper Confidence
Bound for GPs) acquisition function, the proposed cost combines the expected
locational cost with a variance-based exploration term, guiding agents toward
regions that are both high in predicted density and model uncertainty. Compared
to previous work, our algorithm operates in a fully decentralized fashion,
relying only on local observations and communication with neighboring agents.
In particular, agents periodically update their inducing points using a greedy
selection strategy, enabling scalable online GP updates. We demonstrate the
effectiveness of our algorithm in simulation.

</details>


### [84] [Improving Unlearning with Model Updates Probably Aligned with Gradients](https://arxiv.org/abs/2511.02435)
*Virgile Dine,Teddy Furon,Charly Faure*

Main category: cs.LG

TL;DR: 该论文将机器遗忘问题表述为约束优化问题，提出基于掩码的可行更新方法，通过选择值得更新的参数来在保持模型效用的同时实现遗忘，并可作为附加组件集成到任何一阶近似遗忘方法中。


<details>
  <summary>Details</summary>
Motivation: 统一机器遗忘文献中的一阶方法，解决在保持模型效用的同时实现数据遗忘的问题。

Method: 将机器遗忘表述为约束优化问题，引入基于掩码的可行更新概念，通过选择参数更新方向并考虑梯度估计噪声来提供统计保证。

Result: 计算机视觉分类器的实验验证了该方法的有效性。

Conclusion: 提出的可行更新技术可作为附加组件集成到任何一阶近似遗忘方法中，在保持模型效用的同时实现有效遗忘。

Abstract: We formulate the machine unlearning problem as a general constrained
optimization problem. It unifies the first-order methods from the approximate
machine unlearning literature. This paper then introduces the concept of
feasible updates as the model's parameter update directions that help with
unlearning while not degrading the utility of the initial model. Our design of
feasible updates is based on masking, \ie\ a careful selection of the model's
parameters worth updating. It also takes into account the estimation noise of
the gradients when processing each batch of data to offer a statistical
guarantee to derive locally feasible updates. The technique can be plugged in,
as an add-on, to any first-order approximate unlearning methods. Experiments
with computer vision classifiers validate this approach.

</details>


### [85] [Accounting for Underspecification in Statistical Claims of Model Superiority](https://arxiv.org/abs/2511.02453)
*Thomas Sanchez,Pedro M. Gordaliza,Meritxell Bach Cuadra*

Main category: cs.LG

TL;DR: 本文扩展了统计框架，将机器学习模型中的不确定性（underspecification）作为额外方差分量纳入考虑，证明即使很小的随机种子变化也会显著增加支持性能优越性声明所需的证据量


<details>
  <summary>Details</summary>
Motivation: 机器学习在医学影像中的应用日益增多，但许多报告的改进缺乏统计稳健性。现有分析未考虑不确定性——即验证分数相似的模型可能因随机初始化或训练动态而在未见数据上表现不同

Method: 扩展现有的统计框架，将不确定性建模为额外的方差分量，通过模拟实验分析随机种子变化对性能优越性声明的影响

Result: 模拟实验表明，即使很小的随机种子变化（约1%）也会显著增加支持优越性声明所需的证据量

Conclusion: 在验证医学影像系统时，需要明确建模训练方差，以提高统计结论的可靠性

Abstract: Machine learning methods are increasingly applied in medical imaging, yet
many reported improvements lack statistical robustness: recent works have
highlighted that small but significant performance gains are highly likely to
be false positives. However, these analyses do not take
\emph{underspecification} into account -- the fact that models achieving
similar validation scores may behave differently on unseen data due to random
initialization or training dynamics. Here, we extend a recent statistical
framework modeling false outperformance claims to include underspecification as
an additional variance component. Our simulations demonstrate that even modest
seed variability ($\sim1\%$) substantially increases the evidence required to
support superiority claims. Our findings underscore the need for explicit
modeling of training variance when validating medical imaging systems.

</details>


### [86] [SKGE: Spherical Knowledge Graph Embedding with Geometric Regularization](https://arxiv.org/abs/2511.02460)
*Xuan-Truong Quan,Xuan-Son Quan,Duc Do Minh,Vinh Nguyen Van*

Main category: cs.LG

TL;DR: SKGE是一种在超球面流形上构建的知识图谱嵌入模型，通过球面几何约束显著提升了TransE等欧几里得模型的性能，特别是在大规模数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得空间的知识图谱嵌入模型存在建模复杂关系的固有局限性，可能导致训练效率低下。

Method: 提出球形知识图谱嵌入模型，将实体表示约束在紧凑的超球面流形上，使用可学习的非线性球化层将实体映射到球面，将关系解释为平移-投影混合变换。

Result: 在FB15k-237、CoDEx-S和CoDEx-M三个基准数据集上的实验表明，SKGE始终显著优于其欧几里得对应模型TransE，特别是在大规模基准测试中。

Conclusion: 流形选择不仅是实现细节，而是基本设计原则，提倡将几何先验作为设计下一代强大稳定KGE模型的基石。

Abstract: Knowledge graph embedding (KGE) has become a fundamental technique for
representation learning on multi-relational data. Many seminal models, such as
TransE, operate in an unbounded Euclidean space, which presents inherent
limitations in modeling complex relations and can lead to inefficient training.
In this paper, we propose Spherical Knowledge Graph Embedding (SKGE), a model
that challenges this paradigm by constraining entity representations to a
compact manifold: a hypersphere. SKGE employs a learnable, non-linear
Spherization Layer to map entities onto the sphere and interprets relations as
a hybrid translate-then-project transformation. Through extensive experiments
on three benchmark datasets, FB15k-237, CoDEx-S, and CoDEx-M, we demonstrate
that SKGE consistently and significantly outperforms its strong Euclidean
counterpart, TransE, particularly on large-scale benchmarks such as FB15k-237
and CoDEx-M, demonstrating the efficacy of the spherical geometric prior. We
provide an in-depth analysis to reveal the sources of this advantage, showing
that this geometric constraint acts as a powerful regularizer, leading to
comprehensive performance gains across all relation types. More fundamentally,
we prove that the spherical geometry creates an "inherently hard negative
sampling" environment, naturally eliminating trivial negatives and forcing the
model to learn more robust and semantically coherent representations. Our
findings compellingly demonstrate that the choice of manifold is not merely an
implementation detail but a fundamental design principle, advocating for
geometric priors as a cornerstone for designing the next generation of powerful
and stable KGE models.

</details>


### [87] [BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring](https://arxiv.org/abs/2511.02490)
*Rajan Das Gupta,Md Kishor Morol,Nafiz Fahad,Md Tanzib Hosain,Sumaya Binte Zilani Choya,Md Jakir Hossen*

Main category: cs.LG

TL;DR: BRAINS系统利用大型语言模型进行阿尔茨海默病早期检测，通过认知诊断和病例检索双模块架构，结合神经影像数据和临床评估，实现可扩展、可解释的疾病筛查。


<details>
  <summary>Details</summary>
Motivation: 随着全球阿尔茨海默病负担增加，在医疗资源有限地区实现早期准确检测变得至关重要。

Method: 采用双模块架构：认知诊断模块使用在认知和神经影像数据集上微调的LLM进行结构化风险评估；病例检索模块将患者档案编码为潜在表示，从知识库检索相似病例，通过病例融合层增强上下文理解。

Result: 在真实数据集上的评估显示BRAINS在疾病严重程度分类和认知衰退早期迹象识别方面表现有效。

Conclusion: 该系统作为辅助工具在可扩展、可解释的阿尔茨海默病早期检测方面显示出强大潜力，并为未来应用提供希望。

Abstract: As the global burden of Alzheimer's disease (AD) continues to grow, early and
accurate detection has become increasingly critical, especially in regions with
limited access to advanced diagnostic tools. We propose BRAINS (Biomedical
Retrieval-Augmented Intelligence for Neurodegeneration Screening) to address
this challenge. This novel system harnesses the powerful reasoning capabilities
of Large Language Models (LLMs) for Alzheimer's detection and monitoring.
BRAINS features a dual-module architecture: a cognitive diagnostic module and a
case-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on
cognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain
volume metrics -- to perform structured assessments of Alzheimer's risk.
Meanwhile, the Case Retrieval Module encodes patient profiles into latent
representations and retrieves similar cases from a curated knowledge base.
These auxiliary cases are fused with the input profile via a Case Fusion Layer
to enhance contextual understanding. The combined representation is then
processed with clinical prompts for inference. Evaluations on real-world
datasets demonstrate BRAINS effectiveness in classifying disease severity and
identifying early signs of cognitive decline. This system not only shows strong
potential as an assistive tool for scalable, explainable, and early-stage
Alzheimer's disease detection, but also offers hope for future applications in
the field.

</details>


### [88] [An End-to-End Learning Approach for Solving Capacitated Location-Routing Problems](https://arxiv.org/abs/2511.02525)
*Changhao Miao,Yuntian Zhang,Tongyu Wu,Fang Deng,Chen Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习的异构查询方法（DRLHQ）来解决容量限制选址路径问题（CLRP）和开放CLRP（OCLRP），这是首个针对CLRPs的端到端学习方法。


<details>
  <summary>Details</summary>
Motivation: 容量限制选址路径问题是组合优化中的经典问题，需要同时做出选址和路径决策。由于复杂的约束条件和各种决策之间的复杂关系，该问题难以求解。虽然深度强化学习已广泛应用于车辆路径问题及其变体，但针对CLRPs的研究仍需探索。

Method: 采用编码器-解码器结构，将CLRPs重新表述为针对各种决策量身定制的马尔可夫决策过程。引入一种新颖的异构查询注意力机制，能够动态适应不同的决策阶段，以更好地处理选址和路径决策之间的相互依赖关系。

Result: 在合成数据集和基准数据集上的实验结果表明，该方法在解决CLRP和OCLRP时，相比代表性的传统方法和基于DRL的基线方法，具有更优的求解质量和更好的泛化性能。

Conclusion: 本文提出的DRLHQ方法是首个针对CLRPs的端到端学习方法，通过异构查询注意力机制有效处理了选址和路径决策的相互依赖关系，在求解质量和泛化性能方面均优于现有方法。

Abstract: The capacitated location-routing problems (CLRPs) are classical problems in
combinatorial optimization, which require simultaneously making location and
routing decisions. In CLRPs, the complex constraints and the intricate
relationships between various decisions make the problem challenging to solve.
With the emergence of deep reinforcement learning (DRL), it has been
extensively applied to address the vehicle routing problem and its variants,
while the research related to CLRPs still needs to be explored. In this paper,
we propose the DRL with heterogeneous query (DRLHQ) to solve CLRP and open CLRP
(OCLRP), respectively. We are the first to propose an end-to-end learning
approach for CLRPs, following the encoder-decoder structure. In particular, we
reformulate the CLRPs as a markov decision process tailored to various
decisions, a general modeling framework that can be adapted to other DRL-based
methods. To better handle the interdependency across location and routing
decisions, we also introduce a novel heterogeneous querying attention mechanism
designed to adapt dynamically to various decision-making stages. Experimental
results on both synthetic and benchmark datasets demonstrate superior solution
quality and better generalization performance of our proposed approach over
representative traditional and DRL-based baselines in solving both CLRP and
OCLRP.

</details>


### [89] [Rawlsian many-to-one matching with non-linear utility](https://arxiv.org/abs/2511.02533)
*Hortence Nana,Andreas Athanasopoulos,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 该论文研究具有非线性效用函数的大学招生匹配问题，提出基于罗尔斯公平的替代解决方案概念，设计确定性及随机算法来最大化最差处境大学的效用。


<details>
  <summary>Details</summary>
Motivation: 传统稳定匹配在考虑学生多样性时可能不存在，需要新的公平分配方法来解决非线性效用函数下的匹配问题。

Method: 提出基于罗尔斯公平的解决方案概念，设计迭代改进最差处境大学效用的确定性及随机算法。

Result: 当经典稳定匹配不存在时，所提出的算法能够提供公平的分配结果。

Conclusion: 在考虑多样性的大学招生匹配中，罗尔斯公平原则提供了可行的替代方案，所设计的算法具有实际应用价值。

Abstract: We study a many-to-one matching problem, such as the college admission
problem, where each college can admit multiple students. Unlike classical
models, colleges evaluate sets of students through non-linear utility functions
that capture diversity between them. In this setting, we show that classical
stable matchings may fail to exist. To address this, we propose alternative
solution concepts based on Rawlsian fairness, aiming to maximize the minimum
utility across colleges. We design both deterministic and stochastic algorithms
that iteratively improve the outcome of the worst-off college, offering a
practical approach to fair allocation when stability cannot be guaranteed.

</details>


### [90] [Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning](https://arxiv.org/abs/2511.02567)
*Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文提出了一种新的邻域约束方法来解决离线强化学习中的外推误差问题，通过将贝尔曼目标中的动作选择限制在数据集动作邻域的并集中，既避免了过度保守又无需行为策略建模。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临由分布外动作引起的外推误差问题。现有的密度约束、支持约束和样本约束方法各有局限性：密度和样本约束过于保守，而支持约束难以准确建模行为策略。

Method: 提出邻域约束方法，将贝尔曼目标中的动作选择限制在数据集动作邻域的并集中。使用数据质量作为适应标准设计自适应邻域约束，并基于高效的双层优化框架开发ANQ算法。

Result: 理论上证明该约束能在特定条件下限制外推误差和分布偏移，且无需行为策略建模即可近似支持约束。实验表明ANQ在标准离线RL基准上达到最先进性能，在噪声或有限数据场景下表现出强鲁棒性。

Conclusion: 邻域约束方法有效解决了现有离线RL约束方法的局限性，在保持灵活性的同时实现了点级保守性，为离线强化学习提供了新的有效解决方案。

Abstract: Offline reinforcement learning (RL) suffers from extrapolation errors induced
by out-of-distribution (OOD) actions. To address this, offline RL algorithms
typically impose constraints on action selection, which can be systematically
categorized into density, support, and sample constraints. However, we show
that each category has inherent limitations: density and sample constraints
tend to be overly conservative in many scenarios, while the support constraint,
though least restrictive, faces challenges in accurately modeling the behavior
policy. To overcome these limitations, we propose a new neighborhood constraint
that restricts action selection in the Bellman target to the union of
neighborhoods of dataset actions. Theoretically, the constraint not only bounds
extrapolation errors and distribution shift under certain conditions, but also
approximates the support constraint without requiring behavior policy modeling.
Moreover, it retains substantial flexibility and enables pointwise conservatism
by adapting the neighborhood radius for each data point. In practice, we employ
data quality as the adaptation criterion and design an adaptive neighborhood
constraint. Building on an efficient bilevel optimization framework, we develop
a simple yet effective algorithm, Adaptive Neighborhood-constrained Q learning
(ANQ), to perform Q learning with target actions satisfying this constraint.
Empirically, ANQ achieves state-of-the-art performance on standard offline RL
benchmarks and exhibits strong robustness in scenarios with noisy or limited
data.

</details>


### [91] [Dynamic Priors in Bayesian Optimization for Hyperparameter Optimization](https://arxiv.org/abs/2511.02570)
*Lukas Fehring,Marcel Wever,Maximilian Spliethöver,Leona Hennig,Henning Wachsmuth,Marius Lindauer*

Main category: cs.LG

TL;DR: 本文提出了一种允许用户在贝叶斯优化过程中进行在线干预的方法，通过用户输入的先验分布来指导超参数优化过程，同时引入了误导性先验检测机制来防止有害输入。


<details>
  <summary>Details</summary>
Motivation: 传统的超参数优化方法由于其黑盒性质和有限的用户控制，有时难以被机器学习专家接受。现有方法虽然支持用专家知识初始化优化过程，但不允许在优化过程中进行在线指导。

Method: 本文扩展了现有的πBO方法，允许用户在超参数优化过程中反复干预，通过指定先验分布来注入专家知识和用户偏好。同时引入了误导性先验检测方案来保护优化过程免受有害用户输入的影响。

Result: 实验评估表明，该方法能够有效整合多个先验信息，利用信息丰富的先验，同时可靠地拒绝或克服误导性先验，从而保持与未受干扰的贝叶斯优化相当的竞争力。

Conclusion: 所提出的方法成功实现了在超参数优化过程中结合用户指导的能力，既保留了理论保证，又通过检测机制确保了优化过程的鲁棒性。

Abstract: Hyperparameter optimization (HPO), for example, based on Bayesian
optimization (BO), supports users in designing models well-suited for a given
dataset. HPO has proven its effectiveness on several applications, ranging from
classical machine learning for tabular data to deep neural networks for
computer vision and transformers for natural language processing. However, HPO
still sometimes lacks acceptance by machine learning experts due to its
black-box nature and limited user control. Addressing this, first approaches
have been proposed to initialize BO methods with expert knowledge. However,
these approaches do not allow for online steering during the optimization
process. In this paper, we introduce a novel method that enables repeated
interventions to steer BO via user input, specifying expert knowledge and user
preferences at runtime of the HPO process in the form of prior distributions.
To this end, we generalize an existing method, $\pi$BO, preserving theoretical
guarantees. We also introduce a misleading prior detection scheme, which allows
protection against harmful user inputs. In our experimental evaluation, we
demonstrate that our method can effectively incorporate multiple priors,
leveraging informative priors, whereas misleading priors are reliably rejected
or overcome. Thereby, we achieve competitiveness to unperturbed BO.

</details>


### [92] [Natural-gas storage modelling by deep reinforcement learning](https://arxiv.org/abs/2511.02646)
*Tiziano Balaconi,Aldo Glielmo,Marco Taboga*

Main category: cs.LG

TL;DR: GasRL是一个模拟器，将天然气市场校准表示与基于深度强化学习的存储运营商策略模型相结合，用于分析最优库存管理如何影响均衡价格及供需动态。研究发现SAC算法表现最佳，能实现盈利、稳健市场出清和价格稳定等多重目标，且产生的价格动态特征与真实价格高度匹配。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析最优天然气库存管理对市场均衡价格和供需动态的影响，并评估欧盟强制最低存储阈值政策的效果。

Method: 开发GasRL模拟器，结合天然气市场校准模型和基于深度强化学习的存储运营商策略模型，测试多种RL算法（发现SAC表现最佳）。

Result: SAC算法成功实现存储运营商的多个目标，产生的均衡价格动态特征（波动性和季节性）与真实价格高度匹配，且无需显式价格数据校准。欧盟最低存储阈值政策能提高市场对供应冲击的韧性。

Conclusion: GasRL模拟器能有效分析天然气市场动态，SAC算法在库存管理中表现优异，欧盟最低存储阈值政策有助于增强市场韧性，防止大规模供应冲击导致的市场混乱。

Abstract: We introduce GasRL, a simulator that couples a calibrated representation of
the natural gas market with a model of storage-operator policies trained with
deep reinforcement learning (RL). We use it to analyse how optimal stockpile
management affects equilibrium prices and the dynamics of demand and supply. We
test various RL algorithms and find that Soft Actor Critic (SAC) exhibits
superior performance in the GasRL environment: multiple objectives of storage
operators - including profitability, robust market clearing and price
stabilisation - are successfully achieved. Moreover, the equilibrium price
dynamics induced by SAC-derived optimal policies have characteristics, such as
volatility and seasonality, that closely match those of real-world prices.
Remarkably, this adherence to the historical distribution of prices is obtained
without explicitly calibrating the model to price data. We show how the
simulator can be used to assess the effects of EU-mandated minimum storage
thresholds. We find that such thresholds have a positive effect on market
resilience against unanticipated shifts in the distribution of supply shocks.
For example, with unusually large shocks, market disruptions are averted more
often if a threshold is in place.

</details>


### [93] [Directional-Clamp PPO](https://arxiv.org/abs/2511.02577)
*Gilad Karpel,Ruida Zhou,Shoham Sabach,Mohammad Ghavamzadeh*

Main category: cs.LG

TL;DR: 本文提出了一种改进的PPO算法DClamp-PPO，通过在"错误"方向区域引入惩罚机制来解决PPO优化过程中重要性比率经常向错误方向移动的问题。


<details>
  <summary>Details</summary>
Motivation: PPO算法在优化过程中，由于rollout的随机性和策略优化的随机性，重要性比率经常向"错误"方向移动，这是阻碍PPO改进的关键因素，但一直被忽视。

Method: 提出DClamp-PPO算法，在严格"错误"方向区域（优势为正时重要性比率低于1-β，优势为负时重要性比率高于1+β）施加惩罚，通过在这些区域强制执行更陡峭的损失斜率来实现。

Result: DClamp-PPO在各种MuJoCo环境中，使用不同随机种子的情况下，始终优于PPO及其变体。

Conclusion: 提出的方法在理论和实证上都表明能更好地避免"错误"方向更新，同时保持重要性比率更接近1。

Abstract: Proximal Policy Optimization (PPO) is widely regarded as one of the most
successful deep reinforcement learning algorithms, known for its robustness and
effectiveness across a range of problems.
  The PPO objective encourages the importance ratio between the current and
behavior policies to move to the "right" direction -- starting from importance
sampling ratios equal to 1, increasing the ratios for actions with positive
advantages and decreasing those with negative advantages. A clipping function
is introduced to prevent over-optimization when updating the importance ratio
in these "right" direction regions. Many PPO variants have been proposed to
extend its success, most of which modify the objective's behavior by altering
the clipping in the "right" direction regions. However, due to randomness in
the rollouts and stochasticity of the policy optimization, we observe that the
ratios frequently move to the "wrong" direction during the PPO optimization.
This is a key factor hindering the improvement of PPO, but it has been largely
overlooked. To address this, we propose the Directional-Clamp PPO algorithm
(DClamp-PPO), which further penalizes the actions going to the strict "wrong"
direction regions, where the advantage is positive (negative) and importance
ratio falls below (above) $1 - \beta$ ($1+\beta$),
  for a tunable parameter $\beta \in (0, 1)$. The penalty is by enforcing a
steeper loss slope, i.e., a clamp, in those regions. We demonstrate that
DClamp-PPO consistently outperforms PPO, as well as its variants, by focusing
on modifying the objective's behavior in the "right" direction, across various
MuJoCo environments, using different random seeds. The proposed method is
shown, both theoretically and empirically, to better avoid "wrong" direction
updates while keeping the importance ratio closer to 1.

</details>


### [94] [A Large Language Model for Corporate Credit Scoring](https://arxiv.org/abs/2511.02593)
*Chitro Majumdar,Sergio Scandizzo,Ratanlal Mahanta,Avradip Mandal,Swarnendu Bhattacharjee*

Main category: cs.LG

TL;DR: Omega^2是一个基于大语言模型的框架，结合结构化财务数据和机器学习来提升企业信用评分的预测可靠性和可解释性，在多个评级机构数据集上取得了超过0.93的平均测试AUC。


<details>
  <summary>Details</summary>
Motivation: 传统企业信用评分方法在预测可靠性和可解释性方面存在局限，需要结合语言推理和定量学习来创建透明且机构级的信用风险评估基础。

Method: 整合CatBoost、LightGBM和XGBoost模型，通过贝叶斯搜索在时间验证下进行优化，结合结构化财务数据（杠杆率、盈利能力、流动性比率等）和语言模型推理。

Result: 在包含7,800个企业信用评分的多机构数据集上，Omega^2实现了平均测试AUC超过0.93，展现出跨评级系统的泛化能力和时间一致性。

Conclusion: 将基于语言的推理与定量学习相结合，为可靠的企业信用风险评估创建了透明且机构级的基础，证明了该框架的有效性和实用性。

Abstract: We introduce Omega^2, a Large Language Model-driven framework for corporate
credit scoring that combines structured financial data with advanced machine
learning to improve predictive reliability and interpretability. Our study
evaluates Omega^2 on a multi-agency dataset of 7,800 corporate credit ratings
drawn from Moody's, Standard & Poor's, Fitch, and Egan-Jones, each containing
detailed firm-level financial indicators such as leverage, profitability, and
liquidity ratios. The system integrates CatBoost, LightGBM, and XGBoost models
optimized through Bayesian search under temporal validation to ensure
forward-looking and reproducible results. Omega^2 achieved a mean test AUC
above 0.93 across agencies, confirming its ability to generalize across rating
systems and maintain temporal consistency. These results show that combining
language-based reasoning with quantitative learning creates a transparent and
institution-grade foundation for reliable corporate credit-risk assessment.

</details>


### [95] [Recursively Enumerably Representable Classes and Computable Versions of the Fundamental Theorem of Statistical Learning](https://arxiv.org/abs/2511.02644)
*David Kattermann,Lothar Sebastian Krapp*

Main category: cs.LG

TL;DR: 本文研究了可计算概率近似正确（CPAC）学习，探讨了可计算学习器与递归可枚举可表示（RER）类之间的关系，发现有效VC维度可以取任意大于传统VC维度的值，并建立了CPAC可学习性与RER类包含之间的新联系。


<details>
  <summary>Details</summary>
Motivation: 传统统计学习的基本定理在可计算学习框架中不再成立，需要寻找可计算设置下的类似定理，通过引入有效VC维度来恢复基本定理的类似结果。

Method: 研究CPAC学习与递归可枚举可表示（RER）类之间的联系，分析有效VC维度与传统VC维度的关系，探讨各种CPAC学习概念的非示例，并建立CPAC可学习性的新特征化方法。

Result: 发现有效VC维度可以取任意大于传统VC维度的值，即使对于RER类也是如此；证明了CPAC可学习类与RER类之间的包含关系特征化；展示了满足唯一识别属性的CPAC可学习类必然是RER类；建立了RER类在非均匀CPAC学习下的不可知学习保证。

Conclusion: CPAC学习框架中有效VC维度与传统VC维度存在显著差异，但两者在满足强CPAC学习概念时重合；RER类在CPAC学习中扮演重要角色，提供了不可知学习的新保证。

Abstract: We study computable probably approximately correct (CPAC) learning, where
learners are required to be computable functions. It had been previously
observed that the Fundamental Theorem of Statistical Learning, which
characterizes PAC learnability by finiteness of the Vapnik-Chervonenkis
(VC-)dimension, no longer holds in this framework. Recent works recovered
analogs of the Fundamental Theorem in the computable setting, for instance by
introducing an effective VC-dimension. Guided by this, we investigate the
connection between CPAC learning and recursively enumerable representable (RER)
classes, whose members can be algorithmically listed. Our results show that the
effective VC-dimensions can take arbitrary values above the traditional one,
even for RER classes, which creates a whole family of (non-)examples for
various notions of CPAC learning. Yet the two dimensions coincide for classes
satisfying sufficiently strong notions of CPAC learning. We then observe that
CPAC learnability can also be characterized via containment of RER classes that
realize the same samples. Furthermore, it is shown that CPAC learnable classes
satisfying a unique identification property are necessarily RER. Finally, we
establish that agnostic learnability can be guaranteed for RER classes, by
considering the relaxed notion of nonuniform CPAC learning.

</details>


### [96] [STAR-VAE: Latent Variable Transformers for Scalable and Controllable Molecular Generation](https://arxiv.org/abs/2511.02769)
*Bum Chul Kwon,Ben Shapira,Moshiko Raboh,Shreyans Sethi,Shruti Murarka,Joseph A Morrone,Jianying Hu,Parthasarathy Suryanarayanan*

Main category: cs.LG

TL;DR: STAR-VAE是一个基于Transformer的变分自编码器，使用SELFIES表示法保证分子语法有效性，在PubChem的7900万个类药分子上训练，支持条件生成和参数高效微调。


<details>
  <summary>Details</summary>
Motivation: 药物样分子的化学空间巨大，需要开发能够学习广泛化学分布、支持条件生成并实现快速分子生成的生成模型。

Method: 提出STAR-VAE框架：使用Transformer编码器和自回归Transformer解码器，基于SELFIES表示保证语法有效性，采用条件潜变量公式实现属性引导生成，并使用LoRA进行参数高效微调。

Result: 在GuacaMol和MOSES基准测试中达到或超过基线水平，潜空间分析显示平滑的语义结构表示；在Tartarus基准测试中，条件模型将对接分数分布向更强预测结合方向移动。

Conclusion: 现代化、规模适当的VAE与原则性条件化和参数高效微调相结合，在分子生成方面仍具有竞争力。

Abstract: The chemical space of drug-like molecules is vast, motivating the development
of generative models that must learn broad chemical distributions, enable
conditional generation by capturing structure-property representations, and
provide fast molecular generation. Meeting the objectives depends on modeling
choices, including the probabilistic modeling approach, the conditional
generative formulation, the architecture, and the molecular input
representation. To address the challenges, we present STAR-VAE
(Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder),
a scalable latent-variable framework with a Transformer encoder and an
autoregressive Transformer decoder. It is trained on 79 million drug-like
molecules from PubChem, using SELFIES to guarantee syntactic validity. The
latent-variable formulation enables conditional generation: a property
predictor supplies a conditioning signal that is applied consistently to the
latent prior, the inference network, and the decoder. Our contributions are:
(i) a Transformer-based latent-variable encoder-decoder model trained on
SELFIES representations; (ii) a principled conditional latent-variable
formulation for property-guided generation; and (iii) efficient finetuning with
low-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation
with limited property and activity data. On the GuacaMol and MOSES benchmarks,
our approach matches or exceeds baselines, and latent-space analyses reveal
smooth, semantically structured representations that support both unconditional
exploration and property-aware generation. On the Tartarus benchmarks, the
conditional model shifts docking-score distributions toward stronger predicted
binding. These results suggest that a modernized, scale-appropriate VAE remains
competitive for molecular generation when paired with principled conditioning
and parameter-efficient finetuning.

</details>


### [97] [TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models](https://arxiv.org/abs/2511.02802)
*Aditya Tanna,Pratinav Seth,Mohamed Bouadi,Utsav Avaiya,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: TabTune是一个统一库，通过单一接口标准化表格基础模型的完整工作流程，解决预处理、API、微调和评估标准化问题。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在结构化数据学习中日益重要，但由于异构预处理流程、碎片化API、不一致的微调程序以及缺乏部署导向指标的标准化评估，其采用仍然有限。

Method: TabTune提供对7个最先进模型的一致访问，支持多种适应策略，包括零样本推理、元学习、监督微调和参数高效微调。该框架自动化模型感知预处理，内部管理架构异构性，并集成性能、校准和公平性评估模块。

Result: TabTune实现了表格基础模型适应策略的一致基准测试，支持可扩展性和可重复性。

Conclusion: TabTune通过统一接口标准化表格基础模型的完整工作流程，解决了当前采用障碍，为表格基础模型的开发和应用提供了标准化工具。

Abstract: Tabular foundation models represent a growing paradigm in structured data
learning, extending the benefits of large-scale pretraining to tabular domains.
However, their adoption remains limited due to heterogeneous preprocessing
pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the
absence of standardized evaluation for deployment-oriented metrics such as
calibration and fairness. We present TabTune, a unified library that
standardizes the complete workflow for tabular foundation models through a
single interface. TabTune provides consistent access to seven state-of-the-art
models supporting multiple adaptation strategies, including zero-shot
inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient
fine-tuning (PEFT). The framework automates model-aware preprocessing, manages
architectural heterogeneity internally, and integrates evaluation modules for
performance, calibration, and fairness. Designed for extensibility and
reproducibility, TabTune enables consistent benchmarking of adaptation
strategies of tabular foundation models. The library is open source and
available at https://github.com/Lexsi-Labs/TabTune .

</details>


### [98] [Nesterov-Accelerated Robust Federated Learning Over Byzantine Adversaries](https://arxiv.org/abs/2511.02657)
*Lihan Xu,Yanjie Dong,Gang Wang,Runhao Zeng,Xiaoyi Fan,Xiping Hu*

Main category: cs.LG

TL;DR: 提出Byrd-NAFL算法，将Nesterov动量与拜占庭弹性聚合规则结合，在存在恶意攻击的联邦学习环境中实现快速安全的收敛。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在拜占庭攻击下的鲁棒性问题，同时提升通信效率和收敛速度。

Method: 将Nesterov动量集成到联邦学习过程中，结合拜占庭弹性聚合规则，在非凸平滑损失函数下实现快速收敛。

Result: 建立了有限时间收敛保证，实验验证了算法在收敛速度、准确性和对多种拜占庭攻击策略的弹性方面的优越性。

Conclusion: Byrd-NAFL算法能够有效应对拜占庭攻击，在保持通信效率的同时实现快速安全的联邦学习。

Abstract: We investigate robust federated learning, where a group of workers
collaboratively train a shared model under the orchestration of a central
server in the presence of Byzantine adversaries capable of arbitrary and
potentially malicious behaviors. To simultaneously enhance communication
efficiency and robustness against such adversaries, we propose a
Byzantine-resilient Nesterov-Accelerated Federated Learning (Byrd-NAFL)
algorithm. Byrd-NAFL seamlessly integrates Nesterov's momentum into the
federated learning process alongside Byzantine-resilient aggregation rules to
achieve fast and safeguarding convergence against gradient corruption. We
establish a finite-time convergence guarantee for Byrd-NAFL under non-convex
and smooth loss functions with relaxed assumption on the aggregated gradients.
Extensive numerical experiments validate the effectiveness of Byrd-NAFL and
demonstrate the superiority over existing benchmarks in terms of convergence
speed, accuracy, and resilience to diverse Byzantine attack strategies.

</details>


### [99] [Assessing win strength in MLB win prediction models](https://arxiv.org/abs/2511.02815)
*Morgan Allen,Paul Savala*

Main category: cs.LG

TL;DR: 该研究通过训练一套机器学习模型来预测MLB比赛胜率，并分析预测胜率与比分差异的关系，最后探讨了在跑分线投注中使用预测胜率作为决策机制的效果。


<details>
  <summary>Details</summary>
Motivation: 扩展先前关于预测棒球比赛获胜球队的研究，通过使用统一数据集训练全面的机器学习模型，并探索预测胜率与实际比分差异之间的关系。

Method: 使用统一数据集训练多种机器学习模型来预测比赛胜率，然后将预测胜率与比分差异（衡量获胜强度）相关联，并分析在跑分线投注中使用这些预测的效果。

Result: 最常见的机器学习模型确实显示出预测胜率与获胜强度之间的关系；在跑分线投注中使用预测胜率作为决策机制时，采用适当的投注策略可以获得正回报，但简单使用机器学习模型进行投注会导致显著损失。

Conclusion: 机器学习模型能够有效预测棒球比赛胜率并与获胜强度相关，但在实际投注应用中需要谨慎的策略设计，简单的模型直接应用会导致财务损失。

Abstract: In Major League Baseball, strategy and planning are major factors in
determining the outcome of a game. Previous studies have aided this by building
machine learning models for predicting the winning team of any given game. We
extend this work by training a comprehensive set of machine learning models
using a common dataset. In addition, we relate the win probabilities produced
by these models to win strength as measured by score differential. In doing so
we show that the most common machine learning models do indeed demonstrate a
relationship between predicted win probability and the strength of the win.
Finally, we analyze the results of using predicted win probabilities as a
decision making mechanism on run-line betting. We demonstrate positive returns
when utilizing appropriate betting strategies, and show that naive use of
machine learning models for betting lead to significant loses.

</details>


### [100] [Curriculum Design for Trajectory-Constrained Agent: Compressing Chain-of-Thought Tokens in LLMs](https://arxiv.org/abs/2511.02690)
*Georgios Tzannetos,Parameswaran Kamalaruban,Adish Singla*

Main category: cs.LG

TL;DR: 提出一种课程学习策略，在训练过程中逐步收紧约束条件，帮助智能体逐步掌握部署要求，从而在复杂约束环境下实现更高效的训练和部署。


<details>
  <summary>Details</summary>
Motivation: 在严格约束条件下（如有限资源预算或严格安全要求）训练智能体面临重大挑战，特别是当这些约束使任务变得复杂时。需要一种方法能够平滑过渡到具有挑战性的环境。

Method: 采用课程学习策略，从简化的约束版本开始训练，逐步引入完整的部署条件。该方法受到无约束强化学习中自定步学习技术的启发，在强化学习和大型语言模型智能体上进行了验证。

Result: 理论分析显示该课程策略相比基线方法能加速训练。在多种设置下的实证验证表明方法有效且通用，包括二叉树MDP、多任务导航领域和数学推理任务。在LLM上应用时，能够压缩输出思维链令牌，在消费级硬件上实现显著推理加速。

Conclusion: 课程设计在提高在复杂轨迹约束下操作的智能体的效率和性能方面具有潜力，特别适用于资源受限的部署场景。

Abstract: Training agents to operate under strict constraints during deployment, such
as limited resource budgets or stringent safety requirements, presents
significant challenges, especially when these constraints render the task
complex. In this work, we propose a curriculum learning strategy that gradually
tightens constraints during training, enabling the agent to incrementally
master the deployment requirements. Inspired by self-paced learning techniques
in unconstrained reinforcement learning (RL), our approach facilitates a
smoother transition to challenging environments by initially training on
simplified versions of the constraints and progressively introducing the full
deployment conditions. We provide a theoretical analysis using an RL agent in a
binary-tree Markov Decision Process (MDP) to demonstrate that our curriculum
strategy can accelerate training relative to a baseline approach that imposes
the trajectory constraints from the outset. Moreover, we empirically validate
the effectiveness and generality of our method across both RL and large
language model (LLM) agents in diverse settings, including a binary-tree MDP, a
multi-task navigation domain, and a math reasoning task with two benchmarks.
These results highlight the potential of curriculum design in enhancing the
efficiency and performance of agents operating under complex trajectory
constraints during deployment. Moreover, when applied to LLMs, our strategy
enables compression of output chain-of-thought tokens, achieving a substantial
inference speedup on consumer hardware, demonstrating its effectiveness for
resource-constrained deployment.

</details>


### [101] [Does Interpretability of Knowledge Tracing Models Support Teacher Decision Making?](https://arxiv.org/abs/2511.02718)
*Adia Khalid,Alina Deriyeva,Benjamin Paassen*

Main category: cs.LG

TL;DR: 该研究探讨了知识追踪模型的可解释性是否真正帮助教师做出教学决策。研究发现虽然可解释模型在模拟中能更快达到掌握水平，但实际教师使用时，不同模型在达到掌握所需任务数量上差异不大。


<details>
  <summary>Details</summary>
Motivation: 知识追踪模型通常被要求具有可解释性，但之前没有研究验证这种可解释性是否真正帮助教师做出教学决策。本研究旨在填补这一空白。

Method: 首先进行模拟研究，比较基于可解释和不可解释知识追踪模型的教学决策效果；然后让12名教师基于不同模型提供的信息做出教学决策，评估可用性和可信度。

Result: 模拟研究显示基于可解释模型的教学决策能更快达到掌握水平；但教师实际使用时，不同模型在达到掌握所需任务数量上差异不大，尽管教师对可解释模型的可用性和可信度评价更高。

Conclusion: 模型可解释性与教师决策之间的关系并不直接：教师不完全依赖知识追踪模型做决策，需要进一步研究学习者和教师如何实际理解和使用这些模型。

Abstract: Knowledge tracing (KT) models are a crucial basis for pedagogical
decision-making, namely which task to select next for a learner and when to
stop teaching a particular skill. Given the high stakes of pedagogical
decisions, KT models are typically required to be interpretable, in the sense
that they should implement an explicit model of human learning and provide
explicit estimates of learners' abilities. However, to our knowledge, no study
to date has investigated whether the interpretability of KT models actually
helps human teachers to make teaching decisions. We address this gap. First, we
perform a simulation study to show that, indeed, decisions based on
interpretable KT models achieve mastery faster compared to decisions based on a
non-interpretable model. Second, we repeat the study but ask $N=12$ human
teachers to make the teaching decisions based on the information provided by KT
models. As expected, teachers rate interpretable KT models higher in terms of
usability and trustworthiness. However, the number of tasks needed until
mastery hardly differs between KT models. This suggests that the relationship
between model interpretability and teacher decisions is not straightforward:
teachers do not solely rely on KT models to make decisions and further research
is needed to investigate how learners and teachers actually understand and use
KT models.

</details>


### [102] [Calibration improves detection of mislabeled examples](https://arxiv.org/abs/2511.02738)
*Ilies Chibane,Thomas George,Pierre Nodet,Vincent Lemaire*

Main category: cs.LG

TL;DR: 本文研究了校准基础模型对误标检测的影响，发现使用校准方法能提高误标实例检测的准确性和鲁棒性，为工业应用提供实用有效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现实应用中误标数据普遍存在，会削弱机器学习系统的性能。需要有效方法来检测误标实例并进行特殊处理（如过滤或重新标注）。

Method: 通过训练基础机器学习模型，然后对每个实例进行探测以获得标签可信度分数，研究校准该基础模型的影响。

Result: 实证结果表明，使用校准方法能够提高误标实例检测的准确性和鲁棒性。

Conclusion: 模型校准是提高误标检测性能的有效方法，为工业应用提供了实用的解决方案。

Abstract: Mislabeled data is a pervasive issue that undermines the performance of
machine learning systems in real-world applications. An effective approach to
mitigate this problem is to detect mislabeled instances and subject them to
special treatment, such as filtering or relabeling. Automatic mislabeling
detection methods typically rely on training a base machine learning model and
then probing it for each instance to obtain a trust score that each provided
label is genuine or incorrect. The properties of this base model are thus of
paramount importance. In this paper, we investigate the impact of calibrating
this model. Our empirical results show that using calibration methods improves
the accuracy and robustness of mislabeled instance detection, providing a
practical and effective solution for industrial applications.

</details>


### [103] [ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free Finetuning of Large Language Models](https://arxiv.org/abs/2511.02757)
*Lejs Deen Behric,Liang Zhang,Bingcong Li,Kiran Koshy Thekumparampil*

Main category: cs.LG

TL;DR: ConMeZO是一种新颖的零阶优化器，通过自适应方向采样加速收敛，在微调大语言模型时比MeZO快2倍，同时保持零阶方法的内存优势。


<details>
  <summary>Details</summary>
Motivation: 零阶优化（MeZO）在微调大语言模型时避免了反向传播的内存开销，但由于在高维参数空间中搜索下降方向，收敛速度较慢。

Method: ConMeZO通过将采样限制在以动量估计为中心的锥形区域内，而不是均匀随机采样方向，从而将搜索集中在真实梯度更可能存在的方向上。

Result: 理论证明ConMeZO达到与MeZO相同的最坏情况收敛率，实证显示在自然语言任务上微调LLMs时，ConMeZO比MeZO快2倍。

Conclusion: ConMeZO在保持零阶方法低内存占用的同时，显著加速了收敛速度，为大语言模型的微调提供了更高效的解决方案。

Abstract: Zeroth-order or derivative-free optimization (MeZO) is an attractive strategy
for finetuning large language models (LLMs) because it eliminates the memory
overhead of backpropagation. However, it converges slowly due to the inherent
curse of dimensionality when searching for descent directions in the
high-dimensional parameter space of billion-scale LLMs. We propose ConMeZO, a
novel zeroth-order optimizer that accelerates convergence by adaptive
directional sampling. Instead of drawing the direction uniformly at random,
ConMeZO restricts the sampling to a cone centered around a momentum estimate.
This concentrates the search in directions where the true gradient is more
likely to lie and thus reduces the effect of high dimensions. We prove that
ConMeZO achieves the same worst-case convergence rate as MeZO. Empirically,
when finetuning LLMs on natural language tasks, ConMeZO is up to 2X faster than
MeZO while retaining the low-memory footprint of zeroth-order methods.

</details>


### [104] [From Solo to Symphony: Orchestrating Multi-Agent Collaboration with Single-Agent Demos](https://arxiv.org/abs/2511.02762)
*Xun Wang,Zhuoran Li,Yanshan Lin,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: SoCo框架通过将单智能体知识迁移到协作学习中，显著提升了多智能体强化学习的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习从零开始训练效率低下，而现有的离线或可迁移方法仍依赖昂贵的多智能体数据。相比之下，单智能体经验在许多重要场景中更容易获取。

Method: SoCo框架首先从单智能体演示中预训练共享策略，然后通过策略融合机制（包含MoE-like门控选择器和动作编辑器）在多智能体训练期间适应协作。

Result: 在多种协作任务上的实验表明，SoCo显著提升了骨干算法的训练效率和性能。

Conclusion: 单智能体演示为多智能体数据提供了可扩展且有效的补充，使协作学习更加实用和广泛适用。

Abstract: Training a team of agents from scratch in multi-agent reinforcement learning
(MARL) is highly inefficient, much like asking beginners to play a symphony
together without first practicing solo. Existing methods, such as offline or
transferable MARL, can ease this burden, but they still rely on costly
multi-agent data, which often becomes the bottleneck. In contrast, solo
experiences are far easier to obtain in many important scenarios, e.g.,
collaborative coding, household cooperation, and search-and-rescue. To unlock
their potential, we propose Solo-to-Collaborative RL (SoCo), a framework that
transfers solo knowledge into cooperative learning. SoCo first pretrains a
shared solo policy from solo demonstrations, then adapts it for cooperation
during multi-agent training through a policy fusion mechanism that combines an
MoE-like gating selector and an action editor. Experiments across diverse
cooperative tasks show that SoCo significantly boosts the training efficiency
and performance of backbone algorithms. These results demonstrate that solo
demonstrations provide a scalable and effective complement to multi-agent data,
making cooperative learning more practical and broadly applicable.

</details>


### [105] [VecComp: Vector Computing via MIMO Digital Over-the-Air Computation](https://arxiv.org/abs/2511.02765)
*Saeed Razavikia,José Mairton Barros Da Silva Junior,Carlo Fischione*

Main category: cs.LG

TL;DR: VecComp是ChannelComp框架的扩展，通过结合多天线技术实现向量函数计算，解决了原框架只能计算标量函数且易受信道衰落影响的问题。


<details>
  <summary>Details</summary>
Motivation: 传统ChannelComp框架仅限于标量函数计算，而许多数据应用需要向量计算，且易受信道衰落影响。

Method: 将ChannelComp与多天线技术结合，开发VecComp框架，支持向量函数计算且计算复杂度随向量维度线性增长。

Result: 建立了VecComp在衰落信道条件下的均方误差非渐近上界，数值实验显示其在噪声和衰落多址信道上有效改善向量函数计算和衰落补偿。

Conclusion: VecComp为高维数据应用提供了计算高效且对信道损伤鲁棒的向量函数计算框架。

Abstract: Recently, the ChannelComp framework has proposed digital over-the-air
computation by designing digital modulations that enable the computation of
arbitrary functions. Unlike traditional analog over-the-air computation, which
is restricted to nomographic functions, ChannelComp enables a broader range of
computational tasks while maintaining compatibility with digital communication
systems. This framework is intended for applications that favor local
information processing over the mere acquisition of data. However, ChannelComp
is currently designed for scalar function computation, while numerous
data-centric applications necessitate vector-based computations, and it is
susceptible to channel fading. In this work, we introduce a generalization of
the ChannelComp framework, called VecComp, by integrating ChannelComp with
multiple-antenna technology. This generalization not only enables vector
function computation but also ensures scalability in the computational
complexity, which increases only linearly with the vector dimension. As such,
VecComp remains computationally efficient and robust against channel
impairments, making it suitable for high-dimensional, data-centric
applications. We establish a non-asymptotic upper bound on the mean squared
error of VecComp, affirming its computation efficiency under fading channel
conditions. Numerical experiments show the effectiveness of VecComp in
improving the computation of vector functions and fading compensation over
noisy and fading multiple-access channels.

</details>


### [106] [Enhancing Federated Learning Privacy with QUBO](https://arxiv.org/abs/2511.02785)
*Andras Ferenczi,Sutapa Samanta,Dagen Wang,Todd Hodges*

Main category: cs.LG

TL;DR: 该论文提出了一种基于量子计算启发的QUBO优化方法，通过选择最相关的客户端更新子集来大幅减少联邦学习中的隐私暴露风险。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中随着客户端更新被包含在聚合模型中的迭代次数增加，敏感数据暴露风险累积上升。攻击者可以发起成员推理攻击、属性推理攻击和模型反演攻击，从而推断客户端特定属性甚至重构输入数据。

Method: 使用量子计算启发的二次无约束二进制优化(QUBO)公式，在每轮训练中选择一小部分最相关的客户端更新子集，同时假设可信中央服务器和全局数据分布的验证/测试集。

Result: 在MNIST数据集上，300个客户端20轮训练中实现了95.2%的每轮隐私暴露减少和49%的累积隐私暴露减少，147个客户端的更新从未被使用，同时保持了全聚合的准确性甚至更好。在CINIC-10数据集上，30个客户端实现了82%的每轮隐私改进和33%的累积隐私改进。

Conclusion: 该方法能有效降低联邦学习中的隐私暴露风险，在小规模和复杂模型下也表现高效，在保持模型性能的同时显著减少了客户端的隐私暴露。

Abstract: Federated learning (FL) is a widely used method for training machine learning
(ML) models in a scalable way while preserving privacy (i.e., without
centralizing raw data). Prior research shows that the risk of exposing
sensitive data increases cumulatively as the number of iterations where a
client's updates are included in the aggregated model increase. Attackers can
launch membership inference attacks (MIA; deciding whether a sample or client
participated), property inference attacks (PIA; inferring attributes of a
client's data), and model inversion attacks (MI; reconstructing inputs),
thereby inferring client-specific attributes and, in some cases, reconstructing
inputs. In this paper, we mitigate risk by substantially reducing per client
exposure using a quantum computing-inspired quadratic unconstrained binary
optimization (QUBO) formulation that selects a small subset of client updates
most relevant for each training round. In this work, we focus on two threat
vectors: (i) information leakage by clients during training and (ii)
adversaries who can query or obtain the global model. We assume a trusted
central server and do not model server compromise. This method also assumes
that the server has access to a validation/test set with global data
distribution. Experiments on the MNIST dataset with 300 clients in 20 rounds
showed a 95.2% per-round and 49% cumulative privacy exposure reduction, with
147 clients' updates never being used during training while maintaining in
general the full-aggregation accuracy or even better. The method proved to be
efficient at lower scale and more complex model as well. A CINIC-10
dataset-based experiment with 30 clients resulted in 82% per-round privacy
improvement and 33% cumulative privacy.

</details>


### [107] [Fast, Private, and Protected: Safeguarding Data Privacy and Defending Against Model Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2511.02797)
*Nicolas Riccieri Gardin Assumpcao,Leandro Villas*

Main category: cs.LG

TL;DR: FPP是一种保护联邦学习训练安全的新方法，通过参与者评估、训练恢复和信誉机制来防御模型投毒攻击，同时保持数据隐私。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然保护数据隐私，但也难以防范攻击者破坏训练结果，需要一种既能保护隐私又能防御攻击的解决方案。

Method: FPP采用安全聚合保护数据隐私，通过参与者评估轮次、训练恢复机制和信誉机制来识别和限制恶意参与者的影响。

Result: 实验表明FPP具有快速收敛速度，即使在存在恶意参与者进行模型投毒攻击的情况下也能收敛。

Conclusion: FPP能够有效保护联邦学习训练过程，在保持数据隐私的同时防御恶意攻击，实现安全可靠的分布式训练。

Abstract: Federated Learning (FL) is a distributed training paradigm wherein
participants collaborate to build a global model while ensuring the privacy of
the involved data, which remains stored on participant devices. However,
proposals aiming to ensure such privacy also make it challenging to protect
against potential attackers seeking to compromise the training outcome. In this
context, we present Fast, Private, and Protected (FPP), a novel approach that
aims to safeguard federated training while enabling secure aggregation to
preserve data privacy. This is accomplished by evaluating rounds using
participants' assessments and enabling training recovery after an attack. FPP
also employs a reputation-based mechanism to mitigate the participation of
attackers. We created a dockerized environment to validate the performance of
FPP compared to other approaches in the literature (FedAvg, Power-of-Choice,
and aggregation via Trimmed Mean and Median). Our experiments demonstrate that
FPP achieves a rapid convergence rate and can converge even in the presence of
malicious participants performing model poisoning attacks.

</details>


### [108] [GeoCrossBench: Cross-Band Generalization for Remote Sensing](https://arxiv.org/abs/2511.02831)
*Hakob Tamazyan,Ani Vanyan,Alvard Barseghyan,Anna Khosrovyan,Evan Shelhamer,Hrant Khachatrian*

Main category: cs.LG

TL;DR: GeoCrossBench是一个新的遥感基准测试，用于评估模型在跨卫星泛化能力，包括同分布性能、无波段重叠卫星泛化以及额外波段泛化。提出了自监督ChiViT模型，在跨卫星泛化方面显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 随着遥感卫星数量和多样性的增加，大多数标注数据来自旧卫星，而基础模型规模扩大导致重新训练成本增加，因此模型对新卫星的泛化能力变得越来越重要。

Method: 扩展GeoBench基准测试，引入新的评估协议；开发自监督ChannelViT扩展模型ChiViT；测试现有最佳基础模型（DOFA、TerraFM）和通用模型（DINOv3）在不同场景下的性能。

Result: 1) 同分布设置下，遥感基础模型不优于通用模型DINOv3；2) 无波段重叠泛化时所有模型性能下降2-4倍，ChiViT显著优于DINOv3；3) 测试时提供额外波段所有模型性能下降5-25%；4) 仅微调最后一层线性层可获得相对一致的跨卫星性能。

Conclusion: 遥感模型的跨卫星泛化能力仍有很大提升空间，基准测试远未饱和。公开代码和数据集以鼓励开发更具未来适应性的遥感模型。

Abstract: The number and diversity of remote sensing satellites grows over time, while
the vast majority of labeled data comes from older satellites. As the
foundation models for Earth observation scale up, the cost of (re-)training to
support new satellites grows too, so the generalization capabilities of the
models towards new satellites become increasingly important. In this work we
introduce GeoCrossBench, an extension of the popular GeoBench benchmark with a
new evaluation protocol: it tests the in-distribution performance;
generalization to satellites with no band overlap; and generalization to
satellites with additional bands with respect to the training set. We also
develop a self-supervised extension of ChannelViT, ChiViT, to improve its
cross-satellite performance. First, we show that even the best foundation
models for remote sensing (DOFA, TerraFM) do not outperform general purpose
models like DINOv3 in the in-distribution setting. Second, when generalizing to
new satellites with no band overlap, all models suffer 2-4x drop in
performance, and ChiViT significantly outperforms the runner-up DINOv3. Third,
the performance of all tested models drops on average by 5-25\% when given
additional bands during test time. Finally, we show that fine-tuning just the
last linear layer of these models using oracle labels from all bands can get
relatively consistent performance across all satellites, highlighting that the
benchmark is far from being saturated. We publicly release the code and the
datasets to encourage the development of more future-proof remote sensing
models with stronger cross-satellite generalization.

</details>
